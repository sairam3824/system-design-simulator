module.exports=[70406,(e,t,r)=>{t.exports=e.x("next/dist/compiled/@opentelemetry/api",()=>require("next/dist/compiled/@opentelemetry/api"))},93695,(e,t,r)=>{t.exports=e.x("next/dist/shared/lib/no-fallback-error.external.js",()=>require("next/dist/shared/lib/no-fallback-error.external.js"))},18622,(e,t,r)=>{t.exports=e.x("next/dist/compiled/next-server/app-page-turbo.runtime.prod.js",()=>require("next/dist/compiled/next-server/app-page-turbo.runtime.prod.js"))},56704,(e,t,r)=>{t.exports=e.x("next/dist/server/app-render/work-async-storage.external.js",()=>require("next/dist/server/app-render/work-async-storage.external.js"))},32319,(e,t,r)=>{t.exports=e.x("next/dist/server/app-render/work-unit-async-storage.external.js",()=>require("next/dist/server/app-render/work-unit-async-storage.external.js"))},20635,(e,t,r)=>{t.exports=e.x("next/dist/server/app-render/action-async-storage.external.js",()=>require("next/dist/server/app-render/action-async-storage.external.js"))},24725,(e,t,r)=>{t.exports=e.x("next/dist/server/app-render/after-task-async-storage.external.js",()=>require("next/dist/server/app-render/after-task-async-storage.external.js"))},54799,(e,t,r)=>{t.exports=e.x("crypto",()=>require("crypto"))},50227,(e,t,r)=>{t.exports=e.x("node:path",()=>require("node:path"))},57764,(e,t,r)=>{t.exports=e.x("node:url",()=>require("node:url"))},91335,(e,t,r)=>{t.exports=e.x("@prisma/client-a42149e401336614/runtime/client",()=>require("@prisma/client-a42149e401336614/runtime/client"))},81074,e=>e.a(async(t,r)=>{try{let t=await e.y("@libsql/client-1fcf21e78c4be154");e.n(t),r()}catch(e){r(e)}},!0),52022,e=>{"use strict";var t=e.i(74227);async function r(e){if(!(0,t.isRedisAvailable)())return null;try{let r=(0,t.getRedisClient)();if(!r)return null;return await r.get(e)}catch(e){return console.error("Cache get error:",e),null}}async function a(e,r,n=t.CACHE_TTL.USER_ANALYTICS){if(!(0,t.isRedisAvailable)())return!1;try{let a=(0,t.getRedisClient)();if(!a)return!1;return await a.set(e,JSON.stringify(r),{ex:n}),!0}catch(e){return console.error("Cache set error:",e),!1}}async function n(e){if(!(0,t.isRedisAvailable)())return!1;try{let r=(0,t.getRedisClient)();if(!r)return!1;return await r.del(e),!0}catch(e){return console.error("Cache delete error:",e),!1}}async function o(e){if(!(0,t.isRedisAvailable)())return!1;try{let r=(0,t.getRedisClient)();if(!r)return!1;let a=0;do{let[t,n]=await r.scan(a,{match:e,count:100});a=parseInt(t,10),n.length>0&&await r.del(...n)}while(0!==a)return!0}catch(e){return console.error("Cache delete pattern error:",e),!1}}async function s(e,n,o=t.CACHE_TTL.USER_ANALYTICS){let i=await r(e);if(null!==i){if("string"==typeof i)try{return JSON.parse(i)}catch{}return i}let l=await n();return await a(e,l,o),l}async function i(e){await o(`analytics:user:${e}*`)}async function l(){await o("leaderboard:*")}e.s(["cacheDelete",()=>n,"cacheDeletePattern",()=>o,"cacheGet",()=>r,"cacheSet",()=>a,"getOrFetch",()=>s,"invalidateLeaderboard",()=>l,"invalidateUserAnalytics",()=>i])},15239,e=>{"use strict";let t=process.env.OLLAMA_BASE_URL||"http://localhost:11434",r=process.env.OLLAMA_MODEL||"llama3",a=process.env.OLLAMA_CODE_MODEL||"codellama:7b";function n(e){return"coding"===e?a:r}async function o(){try{let e=new AbortController,r=setTimeout(()=>e.abort(),5e3),a=await fetch(`${t}/api/tags`,{method:"GET",signal:e.signal});return clearTimeout(r),a.ok}catch{return!1}}async function s(e,r="general"){let a=n(r),o=new AbortController,i=setTimeout(()=>o.abort(),3e4);try{let r=await fetch(`${t}/api/chat`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:a,messages:e.messages,stream:!1,options:{temperature:e.temperature??.7,num_predict:e.maxTokens??800,top_p:e.topP??.9,stop:e.stop}}),signal:o.signal});if(clearTimeout(i),!r.ok){let e=await r.text();throw Error(`Ollama error: ${e}`)}let n=await r.json();return{content:n.message?.content||"",provider:"ollama",fallbackUsed:!1,model:a}}catch(e){throw clearTimeout(i),e}}async function i(e,r="general"){var a;let o,s,l,c=n(r),p=await fetch(`${t}/api/chat`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:c,messages:e.messages,stream:!0,options:{temperature:e.temperature??.7,num_predict:e.maxTokens??800,top_p:e.topP??.9,stop:e.stop}})});if(!p.ok){let e=await p.text();throw Error(`Ollama streaming error: ${e}`)}if(!p.body)throw Error("No response body from Ollama");return{stream:(a=p.body,o=new TextEncoder,s=new TextDecoder,l="",new ReadableStream({async start(e){let t=a.getReader();try{for(;;){let{done:r,value:a}=await t.read();if(r){e.close();break}let n=(l+=s.decode(a,{stream:!0})).split("\n");for(let t of(l=n.pop()||"",n))if(t.trim())try{let r=JSON.parse(t);if(r.message?.content){let t=`data: ${JSON.stringify({content:r.message.content})}

`;e.enqueue(o.encode(t))}r.done&&e.enqueue(o.encode("data: [DONE]\n\n"))}catch{}}}catch(t){e.error(t)}}})),provider:"ollama",fallbackUsed:!1,model:c}}e.i(48585);var l=e.i(26447);let c=process.env.OPENAI_MODEL||"gpt-4o-mini",p=null;function u(){if(!p){if(!process.env.OPENAI_API_KEY)throw Error("OpenAI API key not configured");p=new l.default({apiKey:process.env.OPENAI_API_KEY})}return p}async function m(e){let t=u(),r=await t.chat.completions.create({model:c,messages:e.messages.map(e=>({role:e.role,content:e.content})),temperature:e.temperature??.7,max_tokens:e.maxTokens??800,top_p:e.topP??.9,stop:e.stop});return{content:r.choices[0]?.message?.content||"",provider:"openai",fallbackUsed:!0,model:c}}async function d(e){var t;let r,a=u();return{stream:(t=await a.chat.completions.create({model:c,messages:e.messages.map(e=>({role:e.role,content:e.content})),temperature:e.temperature??.7,max_tokens:e.maxTokens??800,top_p:e.topP??.9,stop:e.stop,stream:!0}),r=new TextEncoder,new ReadableStream({async start(e){try{for await(let a of t){let t=a.choices[0]?.delta?.content;if(t){let a=`data: ${JSON.stringify({content:t})}

`;e.enqueue(r.encode(a))}a.choices[0]?.finish_reason&&e.enqueue(r.encode("data: [DONE]\n\n"))}e.close()}catch(t){e.error(t)}}})),provider:"openai",fallbackUsed:!0,model:c}}var f=e.i(52022),g=e.i(74227);async function h(){return{ollamaAvailable:await (0,f.getOrFetch)(g.CACHE_KEYS.OLLAMA_HEALTH,o,g.CACHE_TTL.OLLAMA_HEALTH),openaiConfigured:!!process.env.OPENAI_API_KEY}}async function y(e,t="general"){let r=await h();if(r.ollamaAvailable)try{console.log("[LLM] Attempting Ollama completion...");let r=await s(e,t);return console.log("[LLM] Ollama completion successful"),r}catch(e){console.warn("[LLM] Ollama failed, checking fallback:",e)}if(r.openaiConfigured){console.log("[LLM] Falling back to OpenAI...");let t=await m(e);return console.log("[LLM] OpenAI completion successful"),t}throw Error("No LLM provider available. Ollama is not running and OpenAI is not configured.")}async function _(e,t="general"){let r=await h();if(r.ollamaAvailable)try{console.log("[LLM] Attempting Ollama streaming...");let r=await i(e,t);return console.log("[LLM] Ollama streaming started"),r}catch(e){console.warn("[LLM] Ollama streaming failed, checking fallback:",e)}if(r.openaiConfigured){console.log("[LLM] Falling back to OpenAI streaming...");let t=await d(e);return console.log("[LLM] OpenAI streaming started"),t}throw Error("No LLM provider available. Ollama is not running and OpenAI is not configured.")}e.s(["complete",()=>y,"streamComplete",()=>_],15239)},3218,e=>{"use strict";e.i(15239),e.s([])},50760,e=>{"use strict";let t=process.env.OLLAMA_BASE_URL||"http://localhost:11434",r=process.env.OLLAMA_MODEL||"llama3:8b";async function a(){try{return(await fetch(`${t}/api/tags`,{method:"GET"})).ok}catch{return!1}}async function n(){try{let e=await fetch(`${t}/api/tags`);if(!e.ok)return[];let r=await e.json();return r.models?.map(e=>e.name)||[]}catch{return[]}}async function o(e){let{model:a=r,messages:n,temperature:o=.7}=e,s=await fetch(`${t}/api/chat`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:a,messages:n,stream:!1,options:{temperature:o,num_predict:e.max_tokens||800,top_p:e.top_p||.9,stop:e.stop}})});if(!s.ok){let e=await s.text();throw Error(`Ollama error: ${e}`)}return s.json()}process.env.OLLAMA_CODE_MODEL,e.s(["checkOllamaHealth",()=>a,"listModels",()=>n,"ollamaChat",()=>o])},16005,e=>{e.v(t=>Promise.all(["server/chunks/[externals]_node:buffer_00e2e67a._.js"].map(t=>e.l(t))).then(()=>t(51615)))},17059,e=>{e.v(t=>Promise.all(["server/chunks/[externals]_@prisma_client_runtime_query_compiler_fast_bg_sqlite_mjs_6659bef3._.js"].map(t=>e.l(t))).then(()=>t(45217)))},46021,e=>{e.v(t=>Promise.all(["server/chunks/2e4a4_@prisma_client_runtime_query_compiler_fast_bg_sqlite_wasm-base64_mjs_dda4a92d._.js"].map(t=>e.l(t))).then(()=>t(82256)))},22902,e=>{e.v(e=>Promise.resolve().then(()=>e(52022)))},33006,e=>{e.v(t=>Promise.all(["server/chunks/Documents_Git_system-design_system-design-simulator_src_lib_redis_ts_bc66bc9d._.js"].map(t=>e.l(t))).then(()=>t(31901)))}];

//# sourceMappingURL=%5Broot-of-the-server%5D__ee41c311._.js.map
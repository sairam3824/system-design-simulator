{"version":3,"sources":["../../../../../../../Documents/Git/system-design/system-design-simulator/src/lib/cache.ts","../../../../../../../Documents/Git/system-design/system-design-simulator/src/lib/llm/ollama-provider.ts","../../../../../../../Documents/Git/system-design/system-design-simulator/src/lib/llm/openai-provider.ts","../../../../../../../Documents/Git/system-design/system-design-simulator/src/lib/llm/client.ts","../../../../../../../Documents/Git/system-design/system-design-simulator/src/lib/llm/index.ts","../../../../../../../Documents/Git/system-design/system-design-simulator/src/lib/ollama.ts"],"sourcesContent":["/**\n * Cache Utilities\n *\n * Provides helpers for caching with Redis.\n * All operations gracefully fall back to direct execution if Redis is unavailable.\n */\n\nimport { getRedisClient, isRedisAvailable, CACHE_TTL } from \"./redis\";\n\n/**\n * Get a cached value by key\n */\nexport async function cacheGet<T>(key: string): Promise<T | null> {\n  if (!isRedisAvailable()) {\n    return null;\n  }\n\n  try {\n    const redis = getRedisClient();\n    if (!redis) return null;\n\n    const value = await redis.get(key);\n    return value as T | null;\n  } catch (error) {\n    console.error(\"Cache get error:\", error);\n    return null;\n  }\n}\n\n/**\n * Set a cached value with TTL\n */\nexport async function cacheSet<T>(\n  key: string,\n  value: T,\n  ttlSeconds: number = CACHE_TTL.USER_ANALYTICS\n): Promise<boolean> {\n  if (!isRedisAvailable()) {\n    return false;\n  }\n\n  try {\n    const redis = getRedisClient();\n    if (!redis) return false;\n\n    await redis.set(key, JSON.stringify(value), { ex: ttlSeconds });\n    return true;\n  } catch (error) {\n    console.error(\"Cache set error:\", error);\n    return false;\n  }\n}\n\n/**\n * Delete a cached value\n */\nexport async function cacheDelete(key: string): Promise<boolean> {\n  if (!isRedisAvailable()) {\n    return false;\n  }\n\n  try {\n    const redis = getRedisClient();\n    if (!redis) return false;\n\n    await redis.del(key);\n    return true;\n  } catch (error) {\n    console.error(\"Cache delete error:\", error);\n    return false;\n  }\n}\n\n/**\n * Delete multiple cached values by pattern\n */\nexport async function cacheDeletePattern(pattern: string): Promise<boolean> {\n  if (!isRedisAvailable()) {\n    return false;\n  }\n\n  try {\n    const redis = getRedisClient();\n    if (!redis) return false;\n\n    // Upstash Redis supports SCAN for pattern matching\n    let cursor = 0;\n    do {\n      const [newCursor, keys] = await redis.scan(cursor, {\n        match: pattern,\n        count: 100,\n      });\n      cursor = parseInt(newCursor as string, 10);\n\n      if (keys.length > 0) {\n        await redis.del(...keys);\n      }\n    } while (cursor !== 0);\n\n    return true;\n  } catch (error) {\n    console.error(\"Cache delete pattern error:\", error);\n    return false;\n  }\n}\n\n/**\n * Get or fetch pattern - returns cached value or fetches and caches it\n */\nexport async function getOrFetch<T>(\n  key: string,\n  fetcher: () => Promise<T>,\n  ttlSeconds: number = CACHE_TTL.USER_ANALYTICS\n): Promise<T> {\n  // Try to get from cache first\n  const cached = await cacheGet<T>(key);\n  if (cached !== null) {\n    // Parse if it's a string (JSON)\n    if (typeof cached === \"string\") {\n      try {\n        return JSON.parse(cached) as T;\n      } catch {\n        return cached as T;\n      }\n    }\n    return cached;\n  }\n\n  // Fetch fresh data\n  const fresh = await fetcher();\n\n  // Cache the result\n  await cacheSet(key, fresh, ttlSeconds);\n\n  return fresh;\n}\n\n/**\n * Invalidate user analytics cache\n */\nexport async function invalidateUserAnalytics(userId: string): Promise<void> {\n  await cacheDeletePattern(`analytics:user:${userId}*`);\n}\n\n/**\n * Invalidate leaderboard cache\n */\nexport async function invalidateLeaderboard(): Promise<void> {\n  await cacheDeletePattern(\"leaderboard:*\");\n}\n","/**\n * Ollama Provider\n *\n * Handles all Ollama-specific LLM interactions with timeout support.\n */\n\nimport type { LLMMessage, LLMCompletionOptions, LLMResponse, LLMStreamResponse } from \"./index\";\n\nconst OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || \"http://localhost:11434\";\nconst OLLAMA_MODEL = process.env.OLLAMA_MODEL || \"llama3\";\nconst OLLAMA_CODE_MODEL = process.env.OLLAMA_CODE_MODEL || \"codellama:7b\";\nconst OLLAMA_TIMEOUT = 30000; // 30 second timeout\n\nexport type OllamaModelType = \"general\" | \"coding\";\n\nfunction getModelForType(type: OllamaModelType): string {\n  return type === \"coding\" ? OLLAMA_CODE_MODEL : OLLAMA_MODEL;\n}\n\n/**\n * Check if Ollama is healthy\n */\nexport async function checkOllamaHealth(): Promise<boolean> {\n  try {\n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), 5000);\n\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {\n      method: \"GET\",\n      signal: controller.signal,\n    });\n\n    clearTimeout(timeout);\n    return response.ok;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Non-streaming completion with Ollama\n */\nexport async function ollamaComplete(\n  options: LLMCompletionOptions,\n  modelType: OllamaModelType = \"general\"\n): Promise<LLMResponse> {\n  const model = getModelForType(modelType);\n  const controller = new AbortController();\n  const timeout = setTimeout(() => controller.abort(), OLLAMA_TIMEOUT);\n\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        model,\n        messages: options.messages,\n        stream: false,\n        options: {\n          temperature: options.temperature ?? 0.7,\n          num_predict: options.maxTokens ?? 800,\n          top_p: options.topP ?? 0.9,\n          stop: options.stop,\n        },\n      }),\n      signal: controller.signal,\n    });\n\n    clearTimeout(timeout);\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Ollama error: ${error}`);\n    }\n\n    const data = await response.json();\n\n    return {\n      content: data.message?.content || \"\",\n      provider: \"ollama\",\n      fallbackUsed: false,\n      model,\n    };\n  } catch (error) {\n    clearTimeout(timeout);\n    throw error;\n  }\n}\n\n/**\n * Streaming completion with Ollama\n */\nexport async function ollamaStreamComplete(\n  options: LLMCompletionOptions,\n  modelType: OllamaModelType = \"general\"\n): Promise<LLMStreamResponse> {\n  const model = getModelForType(modelType);\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages: options.messages,\n      stream: true,\n      options: {\n        temperature: options.temperature ?? 0.7,\n        num_predict: options.maxTokens ?? 800,\n        top_p: options.topP ?? 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama streaming error: ${error}`);\n  }\n\n  if (!response.body) {\n    throw new Error(\"No response body from Ollama\");\n  }\n\n  // Transform Ollama stream to SSE format\n  const stream = transformOllamaStream(response.body);\n\n  return {\n    stream,\n    provider: \"ollama\",\n    fallbackUsed: false,\n    model,\n  };\n}\n\n/**\n * Transform Ollama stream to SSE format\n */\nfunction transformOllamaStream(\n  ollamaStream: ReadableStream<Uint8Array>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n  let buffer = \"\";\n\n  return new ReadableStream({\n    async start(controller) {\n      const reader = ollamaStream.getReader();\n\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n\n          if (done) {\n            controller.close();\n            break;\n          }\n\n          buffer += decoder.decode(value, { stream: true });\n\n          // Process complete JSON lines\n          const lines = buffer.split(\"\\n\");\n          buffer = lines.pop() || \"\";\n\n          for (const line of lines) {\n            if (line.trim()) {\n              try {\n                const json = JSON.parse(line);\n                if (json.message?.content) {\n                  const sseData = `data: ${JSON.stringify({ content: json.message.content })}\\n\\n`;\n                  controller.enqueue(encoder.encode(sseData));\n                }\n                if (json.done) {\n                  controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n                }\n              } catch {\n                // Skip invalid JSON lines\n              }\n            }\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n","/**\n * OpenAI Provider\n *\n * Handles all OpenAI-specific LLM interactions as fallback.\n */\n\nimport OpenAI from \"openai\";\nimport type { LLMCompletionOptions, LLMResponse, LLMStreamResponse } from \"./index\";\n\nconst OPENAI_MODEL = process.env.OPENAI_MODEL || \"gpt-4o-mini\";\n\nlet openaiClient: OpenAI | null = null;\n\nfunction getOpenAIClient(): OpenAI {\n  if (!openaiClient) {\n    if (!process.env.OPENAI_API_KEY) {\n      throw new Error(\"OpenAI API key not configured\");\n    }\n    openaiClient = new OpenAI({\n      apiKey: process.env.OPENAI_API_KEY,\n    });\n  }\n  return openaiClient;\n}\n\n/**\n * Check if OpenAI is configured\n */\nexport function isOpenAIConfigured(): boolean {\n  return !!process.env.OPENAI_API_KEY;\n}\n\n/**\n * Non-streaming completion with OpenAI\n */\nexport async function openaiComplete(\n  options: LLMCompletionOptions\n): Promise<LLMResponse> {\n  const client = getOpenAIClient();\n\n  const response = await client.chat.completions.create({\n    model: OPENAI_MODEL,\n    messages: options.messages.map((m) => ({\n      role: m.role,\n      content: m.content,\n    })),\n    temperature: options.temperature ?? 0.7,\n    max_tokens: options.maxTokens ?? 800,\n    top_p: options.topP ?? 0.9,\n    stop: options.stop,\n  });\n\n  return {\n    content: response.choices[0]?.message?.content || \"\",\n    provider: \"openai\",\n    fallbackUsed: true,\n    model: OPENAI_MODEL,\n  };\n}\n\n/**\n * Streaming completion with OpenAI\n */\nexport async function openaiStreamComplete(\n  options: LLMCompletionOptions\n): Promise<LLMStreamResponse> {\n  const client = getOpenAIClient();\n\n  const response = await client.chat.completions.create({\n    model: OPENAI_MODEL,\n    messages: options.messages.map((m) => ({\n      role: m.role,\n      content: m.content,\n    })),\n    temperature: options.temperature ?? 0.7,\n    max_tokens: options.maxTokens ?? 800,\n    top_p: options.topP ?? 0.9,\n    stop: options.stop,\n    stream: true,\n  });\n\n  // Transform OpenAI stream to our SSE format\n  const stream = transformOpenAIStream(response);\n\n  return {\n    stream,\n    provider: \"openai\",\n    fallbackUsed: true,\n    model: OPENAI_MODEL,\n  };\n}\n\n/**\n * Transform OpenAI stream to SSE format matching our expected format\n */\nfunction transformOpenAIStream(\n  openaiStream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n\n  return new ReadableStream({\n    async start(controller) {\n      try {\n        for await (const chunk of openaiStream) {\n          const content = chunk.choices[0]?.delta?.content;\n          if (content) {\n            const sseData = `data: ${JSON.stringify({ content })}\\n\\n`;\n            controller.enqueue(encoder.encode(sseData));\n          }\n\n          // Check if this is the final chunk\n          if (chunk.choices[0]?.finish_reason) {\n            controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n          }\n        }\n        controller.close();\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n","/**\n * Unified LLM Client\n *\n * Provides a unified interface for LLM operations with automatic\n * fallback from Ollama to OpenAI when Ollama is unavailable.\n */\n\nimport type {\n  LLMCompletionOptions,\n  LLMResponse,\n  LLMStreamResponse,\n  ModelType,\n} from \"./index\";\nimport {\n  checkOllamaHealth,\n  ollamaComplete,\n  ollamaStreamComplete,\n  type OllamaModelType,\n} from \"./ollama-provider\";\nimport {\n  isOpenAIConfigured,\n  openaiComplete,\n  openaiStreamComplete,\n} from \"./openai-provider\";\nimport { getOrFetch } from \"@/lib/cache\";\nimport { CACHE_KEYS, CACHE_TTL } from \"@/lib/redis\";\n\n/**\n * Check LLM health with caching\n */\nexport async function checkLLMHealth(): Promise<{\n  ollamaAvailable: boolean;\n  openaiConfigured: boolean;\n}> {\n  const ollamaAvailable = await getOrFetch(\n    CACHE_KEYS.OLLAMA_HEALTH,\n    checkOllamaHealth,\n    CACHE_TTL.OLLAMA_HEALTH\n  );\n\n  return {\n    ollamaAvailable,\n    openaiConfigured: isOpenAIConfigured(),\n  };\n}\n\n/**\n * Non-streaming completion with automatic fallback\n */\nexport async function complete(\n  options: LLMCompletionOptions,\n  modelType: ModelType = \"general\"\n): Promise<LLMResponse> {\n  // Check Ollama health (cached)\n  const health = await checkLLMHealth();\n\n  // Try Ollama first if available\n  if (health.ollamaAvailable) {\n    try {\n      console.log(\"[LLM] Attempting Ollama completion...\");\n      const response = await ollamaComplete(\n        options,\n        modelType as OllamaModelType\n      );\n      console.log(\"[LLM] Ollama completion successful\");\n      return response;\n    } catch (error) {\n      console.warn(\"[LLM] Ollama failed, checking fallback:\", error);\n    }\n  }\n\n  // Fallback to OpenAI\n  if (health.openaiConfigured) {\n    console.log(\"[LLM] Falling back to OpenAI...\");\n    const response = await openaiComplete(options);\n    console.log(\"[LLM] OpenAI completion successful\");\n    return response;\n  }\n\n  throw new Error(\n    \"No LLM provider available. Ollama is not running and OpenAI is not configured.\"\n  );\n}\n\n/**\n * Streaming completion with automatic fallback\n */\nexport async function streamComplete(\n  options: LLMCompletionOptions,\n  modelType: ModelType = \"general\"\n): Promise<LLMStreamResponse> {\n  // Check Ollama health (cached)\n  const health = await checkLLMHealth();\n\n  // Try Ollama first if available\n  if (health.ollamaAvailable) {\n    try {\n      console.log(\"[LLM] Attempting Ollama streaming...\");\n      const response = await ollamaStreamComplete(\n        options,\n        modelType as OllamaModelType\n      );\n      console.log(\"[LLM] Ollama streaming started\");\n      return response;\n    } catch (error) {\n      console.warn(\"[LLM] Ollama streaming failed, checking fallback:\", error);\n    }\n  }\n\n  // Fallback to OpenAI\n  if (health.openaiConfigured) {\n    console.log(\"[LLM] Falling back to OpenAI streaming...\");\n    const response = await openaiStreamComplete(options);\n    console.log(\"[LLM] OpenAI streaming started\");\n    return response;\n  }\n\n  throw new Error(\n    \"No LLM provider available. Ollama is not running and OpenAI is not configured.\"\n  );\n}\n","/**\n * LLM Types and Interfaces\n *\n * Unified type definitions for LLM interactions.\n */\n\nexport interface LLMMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\nexport interface LLMCompletionOptions {\n  messages: LLMMessage[];\n  temperature?: number;\n  maxTokens?: number;\n  topP?: number;\n  stop?: string[];\n  stream?: boolean;\n}\n\nexport interface LLMResponse {\n  content: string;\n  provider: \"ollama\" | \"openai\";\n  fallbackUsed: boolean;\n  model: string;\n}\n\nexport interface LLMStreamResponse {\n  stream: ReadableStream<Uint8Array>;\n  provider: \"ollama\" | \"openai\";\n  fallbackUsed: boolean;\n  model: string;\n}\n\nexport type ModelType = \"general\" | \"coding\";\n\n// Re-export client functions\nexport { complete, streamComplete, checkLLMHealth } from \"./client\";\n","/**\n * Ollama Client for Local LLM Inference\n *\n * Uses Ollama running locally for faster interview interactions.\n *\n * Models:\n * - llama3 (8B): Default for system design interviews\n * - codellama:7b: Specialized for coding challenges\n *\n * Make sure Ollama is running: ollama serve\n * Pull the models:\n *   ollama pull llama3\n *   ollama pull codellama:7b\n */\n\nconst OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || \"http://localhost:11434\";\nconst OLLAMA_MODEL = process.env.OLLAMA_MODEL || \"llama3:8b\";\nconst OLLAMA_CODE_MODEL = process.env.OLLAMA_CODE_MODEL || \"codellama:7b\";\n\n// Model types for different use cases\nexport type ModelType = \"general\" | \"coding\";\n\n// Get the appropriate model based on use case\nexport function getModelForType(type: ModelType): string {\n  switch (type) {\n    case \"coding\":\n      return OLLAMA_CODE_MODEL;\n    case \"general\":\n    default:\n      return OLLAMA_MODEL;\n  }\n}\n\nexport interface OllamaMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\nexport interface OllamaCompletionOptions {\n  model?: string;\n  messages: OllamaMessage[];\n  stream?: boolean;\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  stop?: string[];\n}\n\nexport interface OllamaResponse {\n  model: string;\n  created_at: string;\n  message: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  eval_count?: number;\n}\n\n/**\n * Check if Ollama is running and available\n */\nexport async function checkOllamaHealth(): Promise<boolean> {\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {\n      method: \"GET\",\n    });\n    return response.ok;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Check Ollama health with Redis caching\n */\nexport async function checkOllamaHealthCached(): Promise<boolean> {\n  const { getOrFetch } = await import(\"@/lib/cache\");\n  const { CACHE_KEYS, CACHE_TTL } = await import(\"@/lib/redis\");\n\n  return getOrFetch(\n    CACHE_KEYS.OLLAMA_HEALTH,\n    checkOllamaHealth,\n    CACHE_TTL.OLLAMA_HEALTH\n  );\n}\n\n/**\n * List available models\n */\nexport async function listModels(): Promise<string[]> {\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`);\n    if (!response.ok) return [];\n    const data = await response.json();\n    return data.models?.map((m: { name: string }) => m.name) || [];\n  } catch {\n    return [];\n  }\n}\n\n/**\n * Non-streaming chat completion\n */\nexport async function ollamaChat(\n  options: OllamaCompletionOptions\n): Promise<OllamaResponse> {\n  const { model = OLLAMA_MODEL, messages, temperature = 0.7 } = options;\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages,\n      stream: false,\n      options: {\n        temperature,\n        num_predict: options.max_tokens || 800,\n        top_p: options.top_p || 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama error: ${error}`);\n  }\n\n  return response.json();\n}\n\n/**\n * Streaming chat completion - returns a ReadableStream\n */\nexport async function ollamaChatStream(\n  options: OllamaCompletionOptions\n): Promise<ReadableStream<Uint8Array>> {\n  const { model = OLLAMA_MODEL, messages, temperature = 0.7 } = options;\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages,\n      stream: true,\n      options: {\n        temperature,\n        num_predict: options.max_tokens || 800,\n        top_p: options.top_p || 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama streaming error: ${error}`);\n  }\n\n  if (!response.body) {\n    throw new Error(\"No response body from Ollama\");\n  }\n\n  return response.body;\n}\n\n/**\n * Transform Ollama stream to SSE format for client consumption\n */\nexport function createOllamaSSEStream(\n  ollamaStream: ReadableStream<Uint8Array>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n\n  let buffer = \"\";\n\n  return new ReadableStream({\n    async start(controller) {\n      const reader = ollamaStream.getReader();\n\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n\n          if (done) {\n            controller.close();\n            break;\n          }\n\n          buffer += decoder.decode(value, { stream: true });\n\n          // Process complete JSON lines\n          const lines = buffer.split(\"\\n\");\n          buffer = lines.pop() || \"\";\n\n          for (const line of lines) {\n            if (line.trim()) {\n              try {\n                const json = JSON.parse(line);\n                if (json.message?.content) {\n                  // Send as SSE format\n                  const sseData = `data: ${JSON.stringify({ content: json.message.content })}\\n\\n`;\n                  controller.enqueue(encoder.encode(sseData));\n                }\n                if (json.done) {\n                  controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n                }\n              } catch {\n                // Skip invalid JSON lines\n              }\n            }\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n\n/**\n * Simple wrapper for interview chat - handles both streaming and non-streaming\n */\nexport const ollama = {\n  chat: {\n    completions: {\n      create: async (options: {\n        model?: string;\n        messages: OllamaMessage[];\n        temperature?: number;\n        max_tokens?: number;\n        stream?: boolean;\n      }) => {\n        if (options.stream) {\n          const stream = await ollamaChatStream({\n            model: options.model || OLLAMA_MODEL,\n            messages: options.messages,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: true,\n          });\n          return { body: createOllamaSSEStream(stream) };\n        } else {\n          const response = await ollamaChat({\n            model: options.model || OLLAMA_MODEL,\n            messages: options.messages,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: false,\n          });\n          return {\n            choices: [\n              {\n                message: {\n                  role: response.message.role,\n                  content: response.message.content,\n                },\n              },\n            ],\n          };\n        }\n      },\n    },\n  },\n};\n\n/**\n * CodeLlama-specific chat for coding challenges\n * Uses codellama:7b model optimized for code generation and analysis\n */\nexport async function codeLlamaChat(\n  options: Omit<OllamaCompletionOptions, \"model\">\n): Promise<OllamaResponse> {\n  return ollamaChat({\n    ...options,\n    model: OLLAMA_CODE_MODEL,\n  });\n}\n\n/**\n * Check if CodeLlama model is available\n */\nexport async function checkCodeLlamaAvailable(): Promise<boolean> {\n  try {\n    const models = await listModels();\n    return models.some(\n      (m) => m.includes(\"codellama\") || m.includes(\"code-llama\")\n    );\n  } catch {\n    return false;\n  }\n}\n\nexport default ollama;\n"],"names":[],"mappings":"omDAOA,IAAA,EAAA,EAAA,CAAA,CAAA,OAKO,eAAe,EAAY,CAAW,EAC3C,GAAI,CAAC,CAAA,EAAA,EAAA,gBAAA,AAAgB,IACnB,CADuB,MAChB,KAGT,GAAI,CACF,IAAM,EAAQ,CAAA,EAAA,EAAA,cAAA,AAAc,IAC5B,GAAI,CAAC,EAAO,OAAO,KAGnB,OADc,AACP,MADa,EAAM,GAAG,CAAC,EAEhC,CAAE,MAAO,EAAO,CAEd,OADA,QAAQ,KAAK,CAAC,mBAAoB,GAC3B,IACT,CACF,CAKO,eAAe,EACpB,CAAW,CACX,CAAQ,CACR,EAAqB,EAAA,SAAS,CAAC,cAAc,EAE7C,GAAI,CAAC,CAAA,EAAA,EAAA,gBAAA,AAAgB,IACnB,CADuB,MAChB,EAGT,GAAI,CACF,IAAM,EAAQ,CAAA,EAAA,EAAA,cAAA,AAAc,IAC5B,GAAI,CAAC,EAAO,OAAO,EAGnB,OADA,MAAM,EAAM,GAAG,CAAC,EAAK,KAAK,SAAS,CAAC,GAAQ,CAAE,GAAI,CAAW,IACtD,CACT,CAAE,MAAO,EAAO,CAEd,OADA,QAAQ,KAAK,CAAC,mBAAoB,IAC3B,CACT,CACF,CAKO,eAAe,EAAY,CAAW,EAC3C,GAAI,CAAC,CAAA,EAAA,EAAA,gBAAA,AAAgB,IACnB,CADuB,MAChB,EAGT,GAAI,CACF,IAAM,EAAQ,CAAA,EAAA,EAAA,cAAc,AAAd,IACd,GAAI,CAAC,EAAO,MAAO,GAGnB,OADA,MAAM,EAAM,GAAG,CAAC,IACT,CACT,CAAE,MAAO,EAAO,CAEd,OADA,QAAQ,KAAK,CAAC,sBAAuB,IAC9B,CACT,CACF,CAKO,eAAe,EAAmB,CAAe,EACtD,GAAI,CAAC,CAAA,EAAA,EAAA,gBAAA,AAAgB,IACnB,CADuB,MAChB,EAGT,GAAI,CACF,IAAM,EAAQ,CAAA,EAAA,EAAA,cAAA,AAAc,IAC5B,GAAI,CAAC,EAAO,OAAO,EAGnB,IAAI,EAAS,EACb,EAAG,CACD,GAAM,CAAC,EAAW,EAAK,CAAG,MAAM,EAAM,IAAI,CAAC,EAAQ,CACjD,MAAO,EACP,MAAO,GACT,GACA,EAAS,SAAS,EAAqB,IAEnC,EAAK,MAAM,CAAG,GAAG,AACnB,MAAM,EAAM,GAAG,IAAI,EAEvB,OAAoB,IAAX,EAAc,AAEvB,OAAO,CACT,CAAE,MAAO,EAAO,CAEd,OADA,QAAQ,KAAK,CAAC,8BAA+B,IACtC,CACT,CACF,CAKO,eAAe,EACpB,CAAW,CACX,CAAyB,CACzB,EAAqB,EAAA,SAAS,CAAC,cAAc,EAG7C,IAAM,EAAS,MAAM,EAAY,GACjC,GAAe,OAAX,EAAiB,CAEnB,GAAsB,UAAlB,AAA4B,OAArB,EACT,GAAI,CACF,OAAO,KAAK,KAAK,CAAC,EACpB,CAAE,KAAM,CAER,CAEF,OAAO,CACT,CAGA,IAAM,EAAQ,MAAM,IAKpB,OAFA,MAAM,EAAS,EAAK,EAAO,GAEpB,CACT,CAKO,eAAe,EAAwB,CAAc,EAC1D,MAAM,EAAmB,CAAC,eAAe,EAAE,EAAO,CAAC,CAAC,CACtD,CAKO,eAAe,IACpB,MAAM,EAAmB,gBAC3B,kMC7IA,IAAM,EAAkB,QAAQ,GAAG,CAAC,eAAe,EAAI,yBACjD,EAAe,QAAQ,GAAG,CAAC,YAAY,EAAI,SAC3C,EAAoB,QAAQ,GAAG,CAAC,iBAAiB,EAAI,eAK3D,SAAS,EAAgB,CAAqB,EAC5C,MAAO,AAAS,aAAW,EAAoB,CACjD,CAKO,eAAe,IACpB,GAAI,CACF,IAAM,EAAa,IAAI,gBACjB,EAAU,WAAW,IAAM,EAAW,KAAK,GAAI,KAE/C,EAAW,MAAM,MAAM,CAAA,EAAG,EAAgB,SAAS,CAAC,CAAE,CAC1D,OAAQ,MACR,OAAQ,EAAW,MAAM,AAC3B,GAGA,OADA,aAAa,GACN,EAAS,EAAE,AACpB,CAAE,KAAM,CACN,MAAO,EACT,CACF,CAKO,eAAe,EACpB,CAA6B,CAC7B,EAA6B,SAAS,EAEtC,IAAM,EAAQ,EAAgB,GACxB,EAAa,IAAI,gBACjB,EAAU,WAAW,IAAM,EAAW,KAAK,GArC5B,CAqCgC,IAErD,EAvC4B,CAuCxB,CACF,IAAM,EAAW,MAAM,MAAM,AAxCiB,CAwCjB,EAAG,EAAgB,SAAS,CAAC,CAAE,CAC1D,OAAQ,OACR,QAAS,CACP,eAAgB,kBAClB,EACA,KAAM,KAAK,SAAS,CAAC,OACnB,EACA,SAAU,EAAQ,QAAQ,CAC1B,QAAQ,EACR,QAAS,CACP,YAAa,EAAQ,WAAW,EAAI,GACpC,YAAa,EAAQ,SAAS,EAAI,IAClC,MAAO,EAAQ,IAAI,EAAI,GACvB,KAAM,EAAQ,IAAI,AACpB,CACF,GACA,OAAQ,EAAW,MAAM,AAC3B,GAIA,GAFA,aAAa,GAET,CAAC,EAAS,EAAE,CAAE,CAChB,IAAM,EAAQ,MAAM,EAAS,IAAI,EACjC,OAAM,AAAI,MAAM,CAAC,cAAc,EAAE,EAAA,CAAO,CAC1C,CAEA,IAAM,EAAO,MAAM,EAAS,IAAI,GAEhC,MAAO,CACL,QAAS,EAAK,OAAO,EAAE,SAAW,GAClC,SAAU,SACV,aAAc,SACd,CACF,CACF,CAAE,MAAO,EAAO,CAEd,MADA,aAAa,GACP,CACR,CACF,CAKO,eAAe,EACpB,CAA6B,CAC7B,EAA6B,SAAS,MA8CtC,EA5CA,MA+CM,EACF,EAJoC,AA5ClC,EAAQ,EAAgB,GAExB,EAAW,MAAM,MAAM,CAAA,EAAG,EAAgB,SAAS,CAAC,CAAE,CAC1D,OAAQ,OACR,QAAS,CACP,eAAgB,kBAClB,EACA,KAAM,KAAK,SAAS,CAAC,OACnB,EACA,SAAU,EAAQ,QAAQ,CAC1B,QAAQ,EACR,QAAS,CACP,YAAa,EAAQ,WAAW,EAAI,GACpC,YAAa,EAAQ,SAAS,EAAI,IAClC,MAAO,EAAQ,IAAI,EAAI,GACvB,KAAM,EAAQ,IAAI,AACpB,CACF,EACF,GAEA,GAAI,CAAC,EAAS,EAAE,CAAE,CAChB,IAAM,EAAQ,MAAM,EAAS,IAAI,EACjC,OAAM,AAAI,MAAM,CAAC,wBAAwB,EAAE,EAAA,CAAO,CACpD,CAEA,GAAI,CAAC,EAAS,IAAI,CAChB,CADkB,KACZ,AAAI,MAAM,gCAMlB,MAAO,CACL,MAAA,IAHmC,EAAS,IAAI,CAgB5C,EAAU,IAAI,cACJ,IAAI,cACP,GAEN,IAAI,eAAe,CACxB,MAAM,MAAM,CAAU,EACpB,IAAM,EAAS,EAAa,SAAS,GAErC,GAAI,CACF,MAAO,CAAM,CACX,GAAM,MAAE,CAAI,OAAE,CAAK,CAAE,CAAG,MAAM,EAAO,IAAI,GAEzC,GAAI,EAAM,CACR,EAAW,KAAK,GAChB,KACF,CAKA,IAAM,EAAQ,CAHd,GAAU,EAAQ,MAAM,CAAC,EAAO,CAAE,QAAQ,CAAK,EAAA,EAG1B,KAAK,CAAC,MAG3B,IAAK,IAAM,KAFX,EAAS,EAAM,GAAG,IAAM,GAEL,GACjB,GADwB,AACpB,EAAK,IAAI,GACX,CADe,EACX,CACF,IAAM,EAAO,KAAK,KAAK,CAAC,GACxB,GAAI,EAAK,OAAO,EAAE,QAAS,CACzB,IAAM,EAAU,CAAC,MAAM,EAAE,KAAK,SAAS,CAAC,CAAE,QAAS,EAAK,OAAO,CAAC,OAAO,AAAC,GAAG;AAAA;AAAI,CAAC,CAChF,EAAW,OAAO,CAAC,EAAQ,MAAM,CAAC,GACpC,CACI,EAAK,IAAI,EAAE,AACb,EAAW,OAAO,CAAC,EAAQ,MAAM,CAAC,oBAEtC,CAAE,KAAM,CAER,CAGN,CACF,CAAE,MAAO,EAAO,CACd,EAAW,KAAK,CAAC,EACnB,CACF,CACF,IAxDE,SAAU,SACV,cAAc,QACd,CACF,CACF,CClIA,EAAA,CAAA,CAAA,OAAA,IAAA,EAAA,EAAA,CAAA,CAAA,OAGA,IAAM,EAAe,QAAQ,GAAG,CAAC,YAAY,EAAI,cAE7C,EAA8B,KAElC,SAAS,IACP,GAAI,CAAC,EAAc,CACjB,GAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,CAC7B,CAD+B,KACzB,AAAI,MAAM,iCAElB,EAAe,IAAI,EAAA,OAAM,CAAC,CACxB,OAAQ,QAAQ,GAAG,CAAC,cACtB,AADoC,EAEtC,CACA,OAAO,CACT,CAYO,eAAe,EACpB,CAA6B,EAE7B,IAAM,EAAS,IAET,EAAW,MAAM,EAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC,CACpD,MAAO,EACP,SAAU,EAAQ,QAAQ,CAAC,GAAG,CAAE,AAAD,IAAO,AAAC,CACrC,KAAM,EAAE,IAAI,CACZ,QAAS,EAAE,OAAO,CACpB,CAAC,EACD,YAAa,EAAQ,WAAW,EAAI,GACpC,WAAY,EAAQ,SAAS,EAAI,IACjC,MAAO,EAAQ,IAAI,EAAI,GACvB,KAAM,EAAQ,IAAI,AACpB,GAEA,MAAO,CACL,QAAS,EAAS,OAAO,CAAC,EAAE,EAAE,SAAS,SAAW,GAClD,SAAU,SACV,aAAc,GACd,MAAO,CACT,CACF,CAKO,eAAe,EACpB,CAA6B,MAgC7B,EA9BA,MAAM,EAAS,EA8ByD,EAZxE,MAAO,CACL,MAAA,IAjBe,MAAM,EAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC,CACpD,MAAO,EACP,SAAU,EAAQ,QAAQ,CAAC,GAAG,CAAC,AAAC,IAAM,AAAC,CACrC,KAAM,EAAE,IAAI,CACZ,QAAS,EAAE,OAAO,CACpB,CAAC,EACD,YAAa,EAAQ,WAAW,EAAI,GACpC,WAAY,EAAQ,SAAS,EAAI,IACjC,MAAO,EAAQ,IAAI,EAAI,GACvB,KAAM,EAAQ,IAAI,CAClB,QAAQ,CACV,GAmBM,EAAU,IAAI,YAEb,IAAI,eAAe,CACxB,MAAM,MAAM,CAAU,EACpB,GAAI,CACF,UAAW,IAAM,KAAS,EAAc,CACtC,IAAM,EAAU,EAAM,OAAO,CAAC,EAAE,EAAE,OAAO,QACzC,GAAI,EAAS,CACX,IAAM,EAAU,CAAC,MAAM,EAAE,KAAK,SAAS,CAAC,SAAE,CAAQ,GAAG;AAAA;AAAI,CAAC,CAC1D,EAAW,OAAO,CAAC,EAAQ,MAAM,CAAC,GACpC,CAGI,EAAM,OAAO,CAAC,EAAE,EAAE,eAAe,AACnC,EAAW,OAAO,CAAC,EAAQ,MAAM,CAAC,oBAEtC,CACA,EAAW,KAAK,EAClB,CAAE,MAAO,EAAO,CACd,EAAW,KAAK,CAAC,EACnB,CACF,CACF,IAlCE,SAAU,SACV,aAAc,GACd,MAAO,CACT,CACF,CClEA,IAAA,EAAA,EAAA,CAAA,CAAA,OACA,EAAA,EAAA,CAAA,CAAA,OAKO,eAAe,IAUpB,MAAO,CACL,gBAPsB,MAAM,CAAA,EAAA,EAAA,UAAA,AAAU,EACtC,EAAA,UAAU,CAAC,aAAa,CACxB,EACA,EAAA,SAAS,CAAC,aAAa,EAKvB,iBDbK,CAAC,ACaY,CDbX,QAAQ,GAAG,CAAC,cCcrB,ADdmC,CCerC,CAKO,eAAe,EACpB,CAA6B,CAC7B,EAAuB,SAAS,EAGhC,IAAM,EAAS,MAAM,IAGrB,GAAI,EAAO,eAAe,CACxB,CAD0B,EACtB,CACF,QAAQ,GAAG,CAAC,yCACZ,IAAM,EAAW,MAAM,EACrB,EACA,GAGF,OADA,QAAQ,GAAG,CAAC,sCACL,CACT,CAAE,MAAO,EAAO,CACd,QAAQ,IAAI,CAAC,0CAA2C,EAC1D,CAIF,GAAI,EAAO,gBAAgB,CAAE,CAC3B,QAAQ,GAAG,CAAC,mCACZ,IAAM,EAAW,MAAM,EAAe,GAEtC,OADA,QAAQ,GAAG,CAAC,sCACL,CACT,CAEA,MAAM,AAAI,MACR,iFAEJ,CAKO,eAAe,EACpB,CAA6B,CAC7B,EAAuB,SAAS,EAGhC,IAAM,EAAS,MAAM,IAGrB,GAAI,EAAO,eAAe,CACxB,CAD0B,EACtB,CACF,QAAQ,GAAG,CAAC,wCACZ,IAAM,EAAW,MAAM,EACrB,EACA,GAGF,OADA,QAAQ,GAAG,CAAC,kCACL,CACT,CAAE,MAAO,EAAO,CACd,QAAQ,IAAI,CAAC,oDAAqD,EACpE,CAIF,GAAI,EAAO,gBAAgB,CAAE,CAC3B,QAAQ,GAAG,CAAC,6CACZ,IAAM,EAAW,MAAM,EAAqB,GAE5C,OADA,QAAQ,GAAG,CAAC,kCACL,CACT,CAEA,MAAU,AAAJ,MACJ,iFAEJ,6ECnFA,EAAA,CAAA,CAAA,uCCtBA,IAAM,EAAkB,QAAQ,GAAG,CAAC,eAAe,EAAI,yBACjD,EAAe,QAAQ,GAAG,CAAC,YAAY,EAAI,YAiD1C,eAAe,IACpB,GAAI,CAIF,MAAO,CAHU,MAAM,MAAM,CAAA,EAAG,EAAgB,SAAS,CAAC,CAAE,CAC1D,OAAQ,KACV,EAAA,EACgB,EAAE,AACpB,CAAE,KAAM,CACN,OAAO,CACT,CACF,CAmBO,eAAe,IACpB,GAAI,CACF,IAAM,EAAW,MAAM,MAAM,CAAA,EAAG,EAAgB,SAAS,CAAC,EAC1D,GAAI,CAAC,EAAS,EAAE,CAAE,MAAO,EAAE,CAC3B,IAAM,EAAO,MAAM,EAAS,IAAI,GAChC,OAAO,EAAK,MAAM,EAAE,IAAI,AAAC,GAAwB,EAAE,IAAI,GAAK,EAAE,AAChE,CAAE,KAAM,CACN,MAAO,EAAE,AACX,CACF,CAKO,eAAe,EACpB,CAAgC,EAEhC,GAAM,OAAE,EAAQ,CAAY,CAAE,UAAQ,aAAE,EAAc,EAAG,CAAE,CAAG,EAExD,EAAW,MAAM,MAAM,CAAA,EAAG,EAAgB,SAAS,CAAC,CAAE,CAC1D,OAAQ,OACR,QAAS,CACP,eAAgB,kBAClB,EACA,KAAM,KAAK,SAAS,CAAC,OACnB,WACA,EACA,QAAQ,EACR,QAAS,aACP,EACA,YAAa,EAAQ,UAAU,EAAI,IACnC,MAAO,EAAQ,KAAK,EAAI,GACxB,KAAM,EAAQ,IAAI,AACpB,CACF,EACF,GAEA,GAAI,CAAC,EAAS,EAAE,CAAE,CAChB,IAAM,EAAQ,MAAM,EAAS,IAAI,EACjC,OAAM,AAAI,MAAM,CAAC,cAAc,EAAE,EAAA,CAAO,CAC1C,CAEA,OAAO,EAAS,IAAI,EACtB,CAvH0B,QAAQ,GAAG,CAAC,iBAAiB,IAAI"}
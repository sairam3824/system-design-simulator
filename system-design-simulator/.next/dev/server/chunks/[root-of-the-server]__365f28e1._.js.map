{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 46, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/ollama.ts"],"sourcesContent":["/**\n * Ollama Client for Local LLM Inference\n *\n * Uses Ollama running locally for faster interview interactions.\n * Default model: llama3 (8B)\n *\n * Make sure Ollama is running: ollama serve\n * Pull the model: ollama pull llama3\n */\n\nconst OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || \"http://localhost:11434\";\nconst OLLAMA_MODEL = process.env.OLLAMA_MODEL || \"llama3\";\n\nexport interface OllamaMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\nexport interface OllamaCompletionOptions {\n  model?: string;\n  messages: OllamaMessage[];\n  stream?: boolean;\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  stop?: string[];\n}\n\nexport interface OllamaResponse {\n  model: string;\n  created_at: string;\n  message: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  eval_count?: number;\n}\n\n/**\n * Check if Ollama is running and available\n */\nexport async function checkOllamaHealth(): Promise<boolean> {\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {\n      method: \"GET\",\n    });\n    return response.ok;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * List available models\n */\nexport async function listModels(): Promise<string[]> {\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`);\n    if (!response.ok) return [];\n    const data = await response.json();\n    return data.models?.map((m: { name: string }) => m.name) || [];\n  } catch {\n    return [];\n  }\n}\n\n/**\n * Non-streaming chat completion\n */\nexport async function ollamaChat(\n  options: OllamaCompletionOptions\n): Promise<OllamaResponse> {\n  const { model = OLLAMA_MODEL, messages, temperature = 0.7 } = options;\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages,\n      stream: false,\n      options: {\n        temperature,\n        num_predict: options.max_tokens || 800,\n        top_p: options.top_p || 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama error: ${error}`);\n  }\n\n  return response.json();\n}\n\n/**\n * Streaming chat completion - returns a ReadableStream\n */\nexport async function ollamaChatStream(\n  options: OllamaCompletionOptions\n): Promise<ReadableStream<Uint8Array>> {\n  const { model = OLLAMA_MODEL, messages, temperature = 0.7 } = options;\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages,\n      stream: true,\n      options: {\n        temperature,\n        num_predict: options.max_tokens || 800,\n        top_p: options.top_p || 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama streaming error: ${error}`);\n  }\n\n  if (!response.body) {\n    throw new Error(\"No response body from Ollama\");\n  }\n\n  return response.body;\n}\n\n/**\n * Transform Ollama stream to SSE format for client consumption\n */\nexport function createOllamaSSEStream(\n  ollamaStream: ReadableStream<Uint8Array>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n\n  let buffer = \"\";\n\n  return new ReadableStream({\n    async start(controller) {\n      const reader = ollamaStream.getReader();\n\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n\n          if (done) {\n            controller.close();\n            break;\n          }\n\n          buffer += decoder.decode(value, { stream: true });\n\n          // Process complete JSON lines\n          const lines = buffer.split(\"\\n\");\n          buffer = lines.pop() || \"\";\n\n          for (const line of lines) {\n            if (line.trim()) {\n              try {\n                const json = JSON.parse(line);\n                if (json.message?.content) {\n                  // Send as SSE format\n                  const sseData = `data: ${JSON.stringify({ content: json.message.content })}\\n\\n`;\n                  controller.enqueue(encoder.encode(sseData));\n                }\n                if (json.done) {\n                  controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n                }\n              } catch {\n                // Skip invalid JSON lines\n              }\n            }\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n\n/**\n * Simple wrapper for interview chat - handles both streaming and non-streaming\n */\nexport const ollama = {\n  chat: {\n    completions: {\n      create: async (options: {\n        model?: string;\n        messages: OllamaMessage[];\n        temperature?: number;\n        max_tokens?: number;\n        stream?: boolean;\n      }) => {\n        if (options.stream) {\n          const stream = await ollamaChatStream({\n            model: options.model || OLLAMA_MODEL,\n            messages: options.messages,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: true,\n          });\n          return { body: createOllamaSSEStream(stream) };\n        } else {\n          const response = await ollamaChat({\n            model: options.model || OLLAMA_MODEL,\n            messages: options.messages,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: false,\n          });\n          return {\n            choices: [\n              {\n                message: {\n                  role: response.message.role,\n                  content: response.message.content,\n                },\n              },\n            ],\n          };\n        }\n      },\n    },\n  },\n};\n\nexport default ollama;\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AAAA;;;;;;;;CAQC,GAED,MAAM,kBAAkB,QAAQ,GAAG,CAAC,eAAe,IAAI;AACvD,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AAkC1C,eAAe;IACpB,IAAI;QACF,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;YAC1D,QAAQ;QACV;QACA,OAAO,SAAS,EAAE;IACpB,EAAE,OAAM;QACN,OAAO;IACT;AACF;AAKO,eAAe;IACpB,IAAI;QACF,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC;QAC1D,IAAI,CAAC,SAAS,EAAE,EAAE,OAAO,EAAE;QAC3B,MAAM,OAAO,MAAM,SAAS,IAAI;QAChC,OAAO,KAAK,MAAM,EAAE,IAAI,CAAC,IAAwB,EAAE,IAAI,KAAK,EAAE;IAChE,EAAE,OAAM;QACN,OAAO,EAAE;IACX;AACF;AAKO,eAAe,WACpB,OAAgC;IAEhC,MAAM,EAAE,QAAQ,YAAY,EAAE,QAAQ,EAAE,cAAc,GAAG,EAAE,GAAG;IAE9D,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;QAC1D,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;QACA,MAAM,KAAK,SAAS,CAAC;YACnB;YACA;YACA,QAAQ;YACR,SAAS;gBACP;gBACA,aAAa,QAAQ,UAAU,IAAI;gBACnC,OAAO,QAAQ,KAAK,IAAI;gBACxB,MAAM,QAAQ,IAAI;YACpB;QACF;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;QACjC,MAAM,IAAI,MAAM,CAAC,cAAc,EAAE,OAAO;IAC1C;IAEA,OAAO,SAAS,IAAI;AACtB;AAKO,eAAe,iBACpB,OAAgC;IAEhC,MAAM,EAAE,QAAQ,YAAY,EAAE,QAAQ,EAAE,cAAc,GAAG,EAAE,GAAG;IAE9D,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;QAC1D,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;QACA,MAAM,KAAK,SAAS,CAAC;YACnB;YACA;YACA,QAAQ;YACR,SAAS;gBACP;gBACA,aAAa,QAAQ,UAAU,IAAI;gBACnC,OAAO,QAAQ,KAAK,IAAI;gBACxB,MAAM,QAAQ,IAAI;YACpB;QACF;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;QACjC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,OAAO;IACpD;IAEA,IAAI,CAAC,SAAS,IAAI,EAAE;QAClB,MAAM,IAAI,MAAM;IAClB;IAEA,OAAO,SAAS,IAAI;AACtB;AAKO,SAAS,sBACd,YAAwC;IAExC,MAAM,UAAU,IAAI;IACpB,MAAM,UAAU,IAAI;IAEpB,IAAI,SAAS;IAEb,OAAO,IAAI,eAAe;QACxB,MAAM,OAAM,UAAU;YACpB,MAAM,SAAS,aAAa,SAAS;YAErC,IAAI;gBACF,MAAO,KAAM;oBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;oBAEzC,IAAI,MAAM;wBACR,WAAW,KAAK;wBAChB;oBACF;oBAEA,UAAU,QAAQ,MAAM,CAAC,OAAO;wBAAE,QAAQ;oBAAK;oBAE/C,8BAA8B;oBAC9B,MAAM,QAAQ,OAAO,KAAK,CAAC;oBAC3B,SAAS,MAAM,GAAG,MAAM;oBAExB,KAAK,MAAM,QAAQ,MAAO;wBACxB,IAAI,KAAK,IAAI,IAAI;4BACf,IAAI;gCACF,MAAM,OAAO,KAAK,KAAK,CAAC;gCACxB,IAAI,KAAK,OAAO,EAAE,SAAS;oCACzB,qBAAqB;oCACrB,MAAM,UAAU,CAAC,MAAM,EAAE,KAAK,SAAS,CAAC;wCAAE,SAAS,KAAK,OAAO,CAAC,OAAO;oCAAC,GAAG,IAAI,CAAC;oCAChF,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;gCACpC;gCACA,IAAI,KAAK,IAAI,EAAE;oCACb,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;gCACpC;4BACF,EAAE,OAAM;4BACN,0BAA0B;4BAC5B;wBACF;oBACF;gBACF;YACF,EAAE,OAAO,OAAO;gBACd,WAAW,KAAK,CAAC;YACnB;QACF;IACF;AACF;AAKO,MAAM,SAAS;IACpB,MAAM;QACJ,aAAa;YACX,QAAQ,OAAO;gBAOb,IAAI,QAAQ,MAAM,EAAE;oBAClB,MAAM,SAAS,MAAM,iBAAiB;wBACpC,OAAO,QAAQ,KAAK,IAAI;wBACxB,UAAU,QAAQ,QAAQ;wBAC1B,aAAa,QAAQ,WAAW;wBAChC,YAAY,QAAQ,UAAU;wBAC9B,QAAQ;oBACV;oBACA,OAAO;wBAAE,MAAM,sBAAsB;oBAAQ;gBAC/C,OAAO;oBACL,MAAM,WAAW,MAAM,WAAW;wBAChC,OAAO,QAAQ,KAAK,IAAI;wBACxB,UAAU,QAAQ,QAAQ;wBAC1B,aAAa,QAAQ,WAAW;wBAChC,YAAY,QAAQ,UAAU;wBAC9B,QAAQ;oBACV;oBACA,OAAO;wBACL,SAAS;4BACP;gCACE,SAAS;oCACP,MAAM,SAAS,OAAO,CAAC,IAAI;oCAC3B,SAAS,SAAS,OAAO,CAAC,OAAO;gCACnC;4BACF;yBACD;oBACH;gBACF;YACF;QACF;IACF;AACF;uCAEe"}},
    {"offset": {"line": 234, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/app/api/ollama/status/route.ts"],"sourcesContent":["/**\n * Ollama Status Check API\n *\n * GET /api/ollama/status\n * Returns the status of Ollama and available models\n */\n\nimport { NextResponse } from \"next/server\";\nimport { checkOllamaHealth, listModels } from \"@/lib/ollama\";\n\nexport async function GET() {\n  try {\n    const isHealthy = await checkOllamaHealth();\n\n    if (!isHealthy) {\n      return NextResponse.json({\n        status: \"offline\",\n        message: \"Ollama is not running. Start it with: ollama serve\",\n        models: [],\n      });\n    }\n\n    const models = await listModels();\n    const hasLlama3 = models.some(\n      (m) => m.includes(\"llama3\") || m.includes(\"llama3:8b\")\n    );\n\n    return NextResponse.json({\n      status: \"online\",\n      message: hasLlama3\n        ? \"Ollama is running with llama3\"\n        : \"Ollama is running but llama3 not found. Run: ollama pull llama3\",\n      models,\n      recommendedModel: \"llama3\",\n      hasRecommendedModel: hasLlama3,\n    });\n  } catch (error) {\n    console.error(\"Ollama status check error:\", error);\n    return NextResponse.json(\n      {\n        status: \"error\",\n        message: \"Failed to check Ollama status\",\n        models: [],\n      },\n      { status: 500 }\n    );\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;;;;;CAKC,GAED;AACA;;;AAEO,eAAe;IACpB,IAAI;QACF,MAAM,YAAY,MAAM,IAAA,oNAAiB;QAEzC,IAAI,CAAC,WAAW;YACd,OAAO,yNAAY,CAAC,IAAI,CAAC;gBACvB,QAAQ;gBACR,SAAS;gBACT,QAAQ,EAAE;YACZ;QACF;QAEA,MAAM,SAAS,MAAM,IAAA,6MAAU;QAC/B,MAAM,YAAY,OAAO,IAAI,CAC3B,CAAC,IAAM,EAAE,QAAQ,CAAC,aAAa,EAAE,QAAQ,CAAC;QAG5C,OAAO,yNAAY,CAAC,IAAI,CAAC;YACvB,QAAQ;YACR,SAAS,YACL,kCACA;YACJ;YACA,kBAAkB;YAClB,qBAAqB;QACvB;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,8BAA8B;QAC5C,OAAO,yNAAY,CAAC,IAAI,CACtB;YACE,QAAQ;YACR,SAAS;YACT,QAAQ,EAAE;QACZ,GACA;YAAE,QAAQ;QAAI;IAElB;AACF"}}]
}
{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 70, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/generated/prisma/internal/class.ts"],"sourcesContent":["\n/* !!! This is code generated by Prisma. Do not edit directly. !!! */\n/* eslint-disable */\n// biome-ignore-all lint: generated file\n// @ts-nocheck \n/*\n * WARNING: This is an internal file that is subject to change!\n *\n * ðŸ›‘ Under no circumstances should you import this file directly! ðŸ›‘\n *\n * Please import the `PrismaClient` class from the `client.ts` file instead.\n */\n\nimport * as runtime from \"@prisma/client/runtime/client\"\nimport type * as Prisma from \"./prismaNamespace\"\n\n\nconst config: runtime.GetPrismaClientConfig = {\n  \"previewFeatures\": [],\n  \"clientVersion\": \"7.3.0\",\n  \"engineVersion\": \"9d6ad21cbbceab97458517b147a6a09ff43aa735\",\n  \"activeProvider\": \"sqlite\",\n  \"inlineSchema\": \"// Prisma schema for AI System Design Simulator\\ngenerator client {\\n  provider = \\\"prisma-client\\\"\\n  output   = \\\"../src/generated/prisma\\\"\\n}\\n\\ndatasource db {\\n  provider = \\\"sqlite\\\"\\n}\\n\\nmodel User {\\n  id        String   @id @default(cuid())\\n  email     String   @unique\\n  password  String\\n  name      String?\\n  avatar    String?\\n  createdAt DateTime @default(now())\\n  updatedAt DateTime @updatedAt\\n\\n  profile          Profile?\\n  resumes          Resume[]\\n  interviews       Interview[]\\n  analytics        UserAnalytics?\\n  codingChallenges CodingChallenge[]\\n  codingTests      CodingTest[]\\n}\\n\\nmodel Profile {\\n  id     String @id @default(cuid())\\n  userId String @unique\\n  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\\n\\n  bio             String?\\n  yearsExperience Int?\\n  skills          String  @default(\\\"[]\\\") // JSON array stored as string\\n  targetCompanies String  @default(\\\"[]\\\") // JSON array stored as string\\n  targetRole      String?\\n\\n  createdAt DateTime @default(now())\\n  updatedAt DateTime @updatedAt\\n}\\n\\nmodel Resume {\\n  id     String @id @default(cuid())\\n  userId String\\n  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\\n\\n  fileName String\\n  content  String // Full text content of resume\\n  analysis String? // JSON object with AI analysis\\n  filePath String? // Local path to the stored resume file\\n\\n  // ATS Score Data\\n  atsScore       Int? // Total ATS score (0-100)\\n  atsBreakdown   String? // JSON: { contactInfo, structure, experience, keywords, impact }\\n  atsFeedback    String? // JSON array of feedback strings\\n  keywords       String? // JSON array of extracted keywords\\n  softSkills     String? // JSON array of soft skills\\n  predictedRoles String? // JSON array of predicted job roles\\n\\n  uploadedAt DateTime @default(now())\\n  updatedAt  DateTime @updatedAt\\n}\\n\\nmodel Interview {\\n  id     String @id @default(cuid())\\n  userId String\\n  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\\n\\n  topic      String // e.g., \\\"Design Twitter\\\", \\\"Design Uber\\\"\\n  difficulty String @default(\\\"medium\\\") // easy, medium, hard\\n  status     String @default(\\\"pending\\\") // pending, in_progress, completed\\n\\n  startedAt DateTime?\\n  endedAt   DateTime?\\n  createdAt DateTime  @default(now())\\n\\n  // Time tracking for phases\\n  phaseDurations   String? // JSON: { requirements: 480, highLevel: 720 } in seconds\\n  phaseTransitions String? // JSON: [{ phase: \\\"requirements\\\", startedAt: ISO, endedAt: ISO }]\\n\\n  messages Message[]\\n  score    Score?\\n\\n  @@index([userId, status, endedAt])\\n}\\n\\nmodel Message {\\n  id          String    @id @default(cuid())\\n  interviewId String\\n  interview   Interview @relation(fields: [interviewId], references: [id], onDelete: Cascade)\\n\\n  role    String // user, assistant, system\\n  content String\\n\\n  timestamp DateTime @default(now())\\n}\\n\\nmodel Score {\\n  id          String    @id @default(cuid())\\n  interviewId String    @unique\\n  interview   Interview @relation(fields: [interviewId], references: [id], onDelete: Cascade)\\n\\n  // 6 Industry-Standard FAANG Evaluation Dimensions (1-4 scale)\\n  requirementsClarification Int @default(0)\\n  highLevelDesign           Int @default(0)\\n  detailedDesign            Int @default(0)\\n  scalability               Int @default(0)\\n  tradeoffs                 Int @default(0)\\n  communication             Int @default(0)\\n\\n  // Calculated fields\\n  overallScore Float   @default(0) // Weighted average\\n  passStatus   Boolean @default(false)\\n\\n  // Detailed feedback as JSON\\n  feedback String @default(\\\"{}\\\") // JSON with per-dimension comments\\n\\n  createdAt DateTime @default(now())\\n  updatedAt DateTime @updatedAt\\n}\\n\\n// Cached analytics for user performance tracking\\nmodel UserAnalytics {\\n  id     String @id @default(cuid())\\n  userId String @unique\\n  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\\n\\n  // Cached aggregates (updated after each interview)\\n  totalInterviews     Int    @default(0)\\n  completedInterviews Int    @default(0)\\n  avgOverallScore     Float?\\n  passRate            Float?\\n\\n  // Weak/strong areas tracking\\n  weakDimensions   String @default(\\\"[]\\\") // JSON array: [\\\"scalability\\\", \\\"tradeoffs\\\"]\\n  strongDimensions String @default(\\\"[]\\\") // JSON array: [\\\"communication\\\", \\\"requirements\\\"]\\n\\n  // Topic performance\\n  topicStats String @default(\\\"{}\\\") // JSON: { \\\"Design Twitter\\\": { count: 5, avgScore: 2.8 } }\\n\\n  // Trend data\\n  scoreTrend       String   @default(\\\"stable\\\") // \\\"improving\\\" | \\\"stable\\\" | \\\"declining\\\"\\n  lastCalculatedAt DateTime @default(now())\\n\\n  createdAt DateTime @default(now())\\n  updatedAt DateTime @updatedAt\\n}\\n\\n// ==================== CODING CHALLENGE MODELS ====================\\n\\n// Predefined problem bank\\nmodel CodingProblem {\\n  id                 String            @id @default(cuid())\\n  title              String\\n  description        String\\n  difficulty         String // easy, medium, hard\\n  category           String // arrays, strings, dp, graphs, trees, etc.\\n  companies          String            @default(\\\"[]\\\") // JSON array of company names\\n  constraints        String            @default(\\\"[]\\\") // JSON array\\n  examples           String            @default(\\\"[]\\\") // JSON [{input, output, explanation}]\\n  testCases          String            @default(\\\"[]\\\") // JSON [{input, expectedOutput, isHidden}]\\n  solutionApproaches String            @default(\\\"[]\\\") // JSON array\\n  starterCode        String            @default(\\\"{}\\\") // JSON {python: \\\"...\\\", java: \\\"...\\\"}\\n  timeLimit          Int               @default(45) // minutes\\n  createdAt          DateTime          @default(now())\\n  updatedAt          DateTime          @updatedAt\\n  challenges         CodingChallenge[]\\n\\n  @@index([difficulty, category])\\n}\\n\\n// User's coding challenge instance\\nmodel CodingChallenge {\\n  id           String                @id @default(cuid())\\n  userId       String\\n  user         User                  @relation(fields: [userId], references: [id], onDelete: Cascade)\\n  problemId    String?\\n  problem      CodingProblem?        @relation(fields: [problemId], references: [id])\\n  testId       String?\\n  test         CodingTest?           @relation(fields: [testId], references: [id], onDelete: Cascade)\\n  title        String\\n  description  String\\n  difficulty   String\\n  category     String\\n  company      String?\\n  language     String // python, java, javascript, cpp, c, csharp, go\\n  visibleTests String                @default(\\\"[]\\\")\\n  hiddenTests  String                @default(\\\"[]\\\")\\n  timeLimit    Int                   @default(45)\\n  status       String                @default(\\\"pending\\\") // pending, in_progress, completed\\n  startedAt    DateTime?\\n  endedAt      DateTime?\\n  starterCode  String?\\n  finalCode    String?\\n  createdAt    DateTime              @default(now())\\n  updatedAt    DateTime              @updatedAt\\n  submissions  ChallengeSubmission[]\\n  score        ChallengeScore?\\n\\n  @@index([userId, status])\\n}\\n\\n// Code submission for evaluation\\nmodel ChallengeSubmission {\\n  id           String          @id @default(cuid())\\n  challengeId  String\\n  challenge    CodingChallenge @relation(fields: [challengeId], references: [id], onDelete: Cascade)\\n  code         String\\n  language     String\\n  submittedAt  DateTime        @default(now())\\n  evaluation   String?\\n  testResults  String?\\n  isValid      Boolean         @default(false)\\n  errorMessage String?\\n\\n  @@index([challengeId])\\n}\\n\\n// Final score for coding challenge\\nmodel ChallengeScore {\\n  id                 String          @id @default(cuid())\\n  challengeId        String          @unique\\n  challenge          CodingChallenge @relation(fields: [challengeId], references: [id], onDelete: Cascade)\\n  correctness        Float\\n  efficiency         Float\\n  codeQuality        Float\\n  edgeCases          Float\\n  overallScore       Float\\n  passStatus         Boolean\\n  feedback           String\\n  suggestedApproach  String?\\n  complexityAnalysis String?\\n  createdAt          DateTime        @default(now())\\n}\\n\\n// ==================== CODING TEST MODELS ====================\\n\\nmodel CodingTest {\\n  id         String            @id @default(cuid())\\n  userId     String\\n  user       User              @relation(fields: [userId], references: [id], onDelete: Cascade)\\n  challenges CodingChallenge[]\\n  status     String            @default(\\\"pending\\\") // pending, in_progress, completed\\n  startedAt  DateTime?\\n  endedAt    DateTime?\\n  timeLimit  Int // Total minutes for all questions\\n  createdAt  DateTime          @default(now())\\n  updatedAt  DateTime          @updatedAt\\n}\\n\",\n  \"runtimeDataModel\": {\n    \"models\": {},\n    \"enums\": {},\n    \"types\": {}\n  }\n}\n\nconfig.runtimeDataModel = JSON.parse(\"{\\\"models\\\":{\\\"User\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"email\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"password\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"name\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"avatar\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"profile\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Profile\\\",\\\"relationName\\\":\\\"ProfileToUser\\\"},{\\\"name\\\":\\\"resumes\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Resume\\\",\\\"relationName\\\":\\\"ResumeToUser\\\"},{\\\"name\\\":\\\"interviews\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Interview\\\",\\\"relationName\\\":\\\"InterviewToUser\\\"},{\\\"name\\\":\\\"analytics\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"UserAnalytics\\\",\\\"relationName\\\":\\\"UserToUserAnalytics\\\"},{\\\"name\\\":\\\"codingChallenges\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingChallenge\\\",\\\"relationName\\\":\\\"CodingChallengeToUser\\\"},{\\\"name\\\":\\\"codingTests\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingTest\\\",\\\"relationName\\\":\\\"CodingTestToUser\\\"}],\\\"dbName\\\":null},\\\"Profile\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"userId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"user\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"User\\\",\\\"relationName\\\":\\\"ProfileToUser\\\"},{\\\"name\\\":\\\"bio\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"yearsExperience\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"skills\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"targetCompanies\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"targetRole\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null},\\\"Resume\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"userId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"user\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"User\\\",\\\"relationName\\\":\\\"ResumeToUser\\\"},{\\\"name\\\":\\\"fileName\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"content\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"analysis\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"filePath\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"atsScore\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"atsBreakdown\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"atsFeedback\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"keywords\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"softSkills\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"predictedRoles\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"uploadedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null},\\\"Interview\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"userId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"user\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"User\\\",\\\"relationName\\\":\\\"InterviewToUser\\\"},{\\\"name\\\":\\\"topic\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"difficulty\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"status\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"startedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"endedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"phaseDurations\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"phaseTransitions\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"messages\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Message\\\",\\\"relationName\\\":\\\"InterviewToMessage\\\"},{\\\"name\\\":\\\"score\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Score\\\",\\\"relationName\\\":\\\"InterviewToScore\\\"}],\\\"dbName\\\":null},\\\"Message\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"interviewId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"interview\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Interview\\\",\\\"relationName\\\":\\\"InterviewToMessage\\\"},{\\\"name\\\":\\\"role\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"content\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"timestamp\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null},\\\"Score\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"interviewId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"interview\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"Interview\\\",\\\"relationName\\\":\\\"InterviewToScore\\\"},{\\\"name\\\":\\\"requirementsClarification\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"highLevelDesign\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"detailedDesign\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"scalability\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"tradeoffs\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"communication\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"overallScore\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"passStatus\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Boolean\\\"},{\\\"name\\\":\\\"feedback\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null},\\\"UserAnalytics\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"userId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"user\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"User\\\",\\\"relationName\\\":\\\"UserToUserAnalytics\\\"},{\\\"name\\\":\\\"totalInterviews\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"completedInterviews\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"avgOverallScore\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"passRate\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"weakDimensions\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"strongDimensions\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"topicStats\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"scoreTrend\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"lastCalculatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null},\\\"CodingProblem\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"title\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"description\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"difficulty\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"category\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"companies\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"constraints\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"examples\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"testCases\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"solutionApproaches\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"starterCode\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"timeLimit\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"challenges\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingChallenge\\\",\\\"relationName\\\":\\\"CodingChallengeToCodingProblem\\\"}],\\\"dbName\\\":null},\\\"CodingChallenge\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"userId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"user\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"User\\\",\\\"relationName\\\":\\\"CodingChallengeToUser\\\"},{\\\"name\\\":\\\"problemId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"problem\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingProblem\\\",\\\"relationName\\\":\\\"CodingChallengeToCodingProblem\\\"},{\\\"name\\\":\\\"testId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"test\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingTest\\\",\\\"relationName\\\":\\\"CodingChallengeToCodingTest\\\"},{\\\"name\\\":\\\"title\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"description\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"difficulty\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"category\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"company\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"language\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"visibleTests\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"hiddenTests\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"timeLimit\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"status\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"startedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"endedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"starterCode\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"finalCode\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"submissions\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"ChallengeSubmission\\\",\\\"relationName\\\":\\\"ChallengeSubmissionToCodingChallenge\\\"},{\\\"name\\\":\\\"score\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"ChallengeScore\\\",\\\"relationName\\\":\\\"ChallengeScoreToCodingChallenge\\\"}],\\\"dbName\\\":null},\\\"ChallengeSubmission\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"challengeId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"challenge\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingChallenge\\\",\\\"relationName\\\":\\\"ChallengeSubmissionToCodingChallenge\\\"},{\\\"name\\\":\\\"code\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"language\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"submittedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"evaluation\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"testResults\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"isValid\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Boolean\\\"},{\\\"name\\\":\\\"errorMessage\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"}],\\\"dbName\\\":null},\\\"ChallengeScore\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"challengeId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"challenge\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingChallenge\\\",\\\"relationName\\\":\\\"ChallengeScoreToCodingChallenge\\\"},{\\\"name\\\":\\\"correctness\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"efficiency\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"codeQuality\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"edgeCases\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"overallScore\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Float\\\"},{\\\"name\\\":\\\"passStatus\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Boolean\\\"},{\\\"name\\\":\\\"feedback\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"suggestedApproach\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"complexityAnalysis\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null},\\\"CodingTest\\\":{\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"userId\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"user\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"User\\\",\\\"relationName\\\":\\\"CodingTestToUser\\\"},{\\\"name\\\":\\\"challenges\\\",\\\"kind\\\":\\\"object\\\",\\\"type\\\":\\\"CodingChallenge\\\",\\\"relationName\\\":\\\"CodingChallengeToCodingTest\\\"},{\\\"name\\\":\\\"status\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"String\\\"},{\\\"name\\\":\\\"startedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"endedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"timeLimit\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"Int\\\"},{\\\"name\\\":\\\"createdAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"},{\\\"name\\\":\\\"updatedAt\\\",\\\"kind\\\":\\\"scalar\\\",\\\"type\\\":\\\"DateTime\\\"}],\\\"dbName\\\":null}},\\\"enums\\\":{},\\\"types\\\":{}}\")\n\nasync function decodeBase64AsWasm(wasmBase64: string): Promise<WebAssembly.Module> {\n  const { Buffer } = await import('node:buffer')\n  const wasmArray = Buffer.from(wasmBase64, 'base64')\n  return new WebAssembly.Module(wasmArray)\n}\n\nconfig.compilerWasm = {\n  getRuntime: async () => await import(\"@prisma/client/runtime/query_compiler_fast_bg.sqlite.mjs\"),\n\n  getQueryCompilerWasmModule: async () => {\n    const { wasm } = await import(\"@prisma/client/runtime/query_compiler_fast_bg.sqlite.wasm-base64.mjs\")\n    return await decodeBase64AsWasm(wasm)\n  },\n\n  importName: \"./query_compiler_fast_bg.js\"\n}\n\n\n\nexport type LogOptions<ClientOptions extends Prisma.PrismaClientOptions> =\n  'log' extends keyof ClientOptions ? ClientOptions['log'] extends Array<Prisma.LogLevel | Prisma.LogDefinition> ? Prisma.GetEvents<ClientOptions['log']> : never : never\n\nexport interface PrismaClientConstructor {\n    /**\n   * ## Prisma Client\n   * \n   * Type-safe database client for TypeScript\n   * @example\n   * ```\n   * const prisma = new PrismaClient()\n   * // Fetch zero or more Users\n   * const users = await prisma.user.findMany()\n   * ```\n   * \n   * Read more in our [docs](https://pris.ly/d/client).\n   */\n\n  new <\n    Options extends Prisma.PrismaClientOptions = Prisma.PrismaClientOptions,\n    LogOpts extends LogOptions<Options> = LogOptions<Options>,\n    OmitOpts extends Prisma.PrismaClientOptions['omit'] = Options extends { omit: infer U } ? U : Prisma.PrismaClientOptions['omit'],\n    ExtArgs extends runtime.Types.Extensions.InternalArgs = runtime.Types.Extensions.DefaultArgs\n  >(options: Prisma.Subset<Options, Prisma.PrismaClientOptions> ): PrismaClient<LogOpts, OmitOpts, ExtArgs>\n}\n\n/**\n * ## Prisma Client\n * \n * Type-safe database client for TypeScript\n * @example\n * ```\n * const prisma = new PrismaClient()\n * // Fetch zero or more Users\n * const users = await prisma.user.findMany()\n * ```\n * \n * Read more in our [docs](https://pris.ly/d/client).\n */\n\nexport interface PrismaClient<\n  in LogOpts extends Prisma.LogLevel = never,\n  in out OmitOpts extends Prisma.PrismaClientOptions['omit'] = undefined,\n  in out ExtArgs extends runtime.Types.Extensions.InternalArgs = runtime.Types.Extensions.DefaultArgs\n> {\n  [K: symbol]: { types: Prisma.TypeMap<ExtArgs>['other'] }\n\n  $on<V extends LogOpts>(eventType: V, callback: (event: V extends 'query' ? Prisma.QueryEvent : Prisma.LogEvent) => void): PrismaClient;\n\n  /**\n   * Connect with the database\n   */\n  $connect(): runtime.Types.Utils.JsPromise<void>;\n\n  /**\n   * Disconnect from the database\n   */\n  $disconnect(): runtime.Types.Utils.JsPromise<void>;\n\n/**\n   * Executes a prepared raw query and returns the number of affected rows.\n   * @example\n   * ```\n   * const result = await prisma.$executeRaw`UPDATE User SET cool = ${true} WHERE email = ${'user@email.com'};`\n   * ```\n   *\n   * Read more in our [docs](https://pris.ly/d/raw-queries).\n   */\n  $executeRaw<T = unknown>(query: TemplateStringsArray | Prisma.Sql, ...values: any[]): Prisma.PrismaPromise<number>;\n\n  /**\n   * Executes a raw query and returns the number of affected rows.\n   * Susceptible to SQL injections, see documentation.\n   * @example\n   * ```\n   * const result = await prisma.$executeRawUnsafe('UPDATE User SET cool = $1 WHERE email = $2 ;', true, 'user@email.com')\n   * ```\n   *\n   * Read more in our [docs](https://pris.ly/d/raw-queries).\n   */\n  $executeRawUnsafe<T = unknown>(query: string, ...values: any[]): Prisma.PrismaPromise<number>;\n\n  /**\n   * Performs a prepared raw query and returns the `SELECT` data.\n   * @example\n   * ```\n   * const result = await prisma.$queryRaw`SELECT * FROM User WHERE id = ${1} OR email = ${'user@email.com'};`\n   * ```\n   *\n   * Read more in our [docs](https://pris.ly/d/raw-queries).\n   */\n  $queryRaw<T = unknown>(query: TemplateStringsArray | Prisma.Sql, ...values: any[]): Prisma.PrismaPromise<T>;\n\n  /**\n   * Performs a raw query and returns the `SELECT` data.\n   * Susceptible to SQL injections, see documentation.\n   * @example\n   * ```\n   * const result = await prisma.$queryRawUnsafe('SELECT * FROM User WHERE id = $1 OR email = $2;', 1, 'user@email.com')\n   * ```\n   *\n   * Read more in our [docs](https://pris.ly/d/raw-queries).\n   */\n  $queryRawUnsafe<T = unknown>(query: string, ...values: any[]): Prisma.PrismaPromise<T>;\n\n\n  /**\n   * Allows the running of a sequence of read/write operations that are guaranteed to either succeed or fail as a whole.\n   * @example\n   * ```\n   * const [george, bob, alice] = await prisma.$transaction([\n   *   prisma.user.create({ data: { name: 'George' } }),\n   *   prisma.user.create({ data: { name: 'Bob' } }),\n   *   prisma.user.create({ data: { name: 'Alice' } }),\n   * ])\n   * ```\n   * \n   * Read more in our [docs](https://www.prisma.io/docs/concepts/components/prisma-client/transactions).\n   */\n  $transaction<P extends Prisma.PrismaPromise<any>[]>(arg: [...P], options?: { isolationLevel?: Prisma.TransactionIsolationLevel }): runtime.Types.Utils.JsPromise<runtime.Types.Utils.UnwrapTuple<P>>\n\n  $transaction<R>(fn: (prisma: Omit<PrismaClient, runtime.ITXClientDenyList>) => runtime.Types.Utils.JsPromise<R>, options?: { maxWait?: number, timeout?: number, isolationLevel?: Prisma.TransactionIsolationLevel }): runtime.Types.Utils.JsPromise<R>\n\n  $extends: runtime.Types.Extensions.ExtendsHook<\"extends\", Prisma.TypeMapCb<OmitOpts>, ExtArgs, runtime.Types.Utils.Call<Prisma.TypeMapCb<OmitOpts>, {\n    extArgs: ExtArgs\n  }>>\n\n      /**\n   * `prisma.user`: Exposes CRUD operations for the **User** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more Users\n    * const users = await prisma.user.findMany()\n    * ```\n    */\n  get user(): Prisma.UserDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.profile`: Exposes CRUD operations for the **Profile** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more Profiles\n    * const profiles = await prisma.profile.findMany()\n    * ```\n    */\n  get profile(): Prisma.ProfileDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.resume`: Exposes CRUD operations for the **Resume** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more Resumes\n    * const resumes = await prisma.resume.findMany()\n    * ```\n    */\n  get resume(): Prisma.ResumeDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.interview`: Exposes CRUD operations for the **Interview** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more Interviews\n    * const interviews = await prisma.interview.findMany()\n    * ```\n    */\n  get interview(): Prisma.InterviewDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.message`: Exposes CRUD operations for the **Message** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more Messages\n    * const messages = await prisma.message.findMany()\n    * ```\n    */\n  get message(): Prisma.MessageDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.score`: Exposes CRUD operations for the **Score** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more Scores\n    * const scores = await prisma.score.findMany()\n    * ```\n    */\n  get score(): Prisma.ScoreDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.userAnalytics`: Exposes CRUD operations for the **UserAnalytics** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more UserAnalytics\n    * const userAnalytics = await prisma.userAnalytics.findMany()\n    * ```\n    */\n  get userAnalytics(): Prisma.UserAnalyticsDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.codingProblem`: Exposes CRUD operations for the **CodingProblem** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more CodingProblems\n    * const codingProblems = await prisma.codingProblem.findMany()\n    * ```\n    */\n  get codingProblem(): Prisma.CodingProblemDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.codingChallenge`: Exposes CRUD operations for the **CodingChallenge** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more CodingChallenges\n    * const codingChallenges = await prisma.codingChallenge.findMany()\n    * ```\n    */\n  get codingChallenge(): Prisma.CodingChallengeDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.challengeSubmission`: Exposes CRUD operations for the **ChallengeSubmission** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more ChallengeSubmissions\n    * const challengeSubmissions = await prisma.challengeSubmission.findMany()\n    * ```\n    */\n  get challengeSubmission(): Prisma.ChallengeSubmissionDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.challengeScore`: Exposes CRUD operations for the **ChallengeScore** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more ChallengeScores\n    * const challengeScores = await prisma.challengeScore.findMany()\n    * ```\n    */\n  get challengeScore(): Prisma.ChallengeScoreDelegate<ExtArgs, { omit: OmitOpts }>;\n\n  /**\n   * `prisma.codingTest`: Exposes CRUD operations for the **CodingTest** model.\n    * Example usage:\n    * ```ts\n    * // Fetch zero or more CodingTests\n    * const codingTests = await prisma.codingTest.findMany()\n    * ```\n    */\n  get codingTest(): Prisma.CodingTestDelegate<ExtArgs, { omit: OmitOpts }>;\n}\n\nexport function getPrismaClientClass(): PrismaClientConstructor {\n  return runtime.getPrismaClient(config) as unknown as PrismaClientConstructor\n}\n"],"names":[],"mappings":";;;;AACA,mEAAmE,GACnE,kBAAkB,GAClB,wCAAwC;AACxC,eAAe;AACf;;;;;;CAMC,GAED;;AAIA,MAAM,SAAwC;IAC5C,mBAAmB,EAAE;IACrB,iBAAiB;IACjB,iBAAiB;IACjB,kBAAkB;IAClB,gBAAgB;IAChB,oBAAoB;QAClB,UAAU,CAAC;QACX,SAAS,CAAC;QACV,SAAS,CAAC;IACZ;AACF;AAEA,OAAO,gBAAgB,GAAG,KAAK,KAAK,CAAC;AAErC,eAAe,mBAAmB,UAAkB;IAClD,MAAM,EAAE,MAAM,EAAE,GAAG;IACnB,MAAM,YAAY,OAAO,IAAI,CAAC,YAAY;IAC1C,OAAO,IAAI,YAAY,MAAM,CAAC;AAChC;AAEA,OAAO,YAAY,GAAG;IACpB,YAAY,UAAY;IAExB,4BAA4B;QAC1B,MAAM,EAAE,IAAI,EAAE,GAAG;QACjB,OAAO,MAAM,mBAAmB;IAClC;IAEA,YAAY;AACd;AA4PO,SAAS;IACd,OAAO,4TAAuB,CAAC;AACjC"}},
    {"offset": {"line": 117, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/generated/prisma/internal/prismaNamespace.ts"],"sourcesContent":["\n/* !!! This is code generated by Prisma. Do not edit directly. !!! */\n/* eslint-disable */\n// biome-ignore-all lint: generated file\n// @ts-nocheck \n/*\n * WARNING: This is an internal file that is subject to change!\n *\n * ðŸ›‘ Under no circumstances should you import this file directly! ðŸ›‘\n *\n * All exports from this file are wrapped under a `Prisma` namespace object in the client.ts file.\n * While this enables partial backward compatibility, it is not part of the stable public API.\n *\n * If you are looking for your Models, Enums, and Input Types, please import them from the respective\n * model files in the `model` directory!\n */\n\nimport * as runtime from \"@prisma/client/runtime/client\"\nimport type * as Prisma from \"../models\"\nimport { type PrismaClient } from \"./class\"\n\nexport type * from '../models'\n\nexport type DMMF = typeof runtime.DMMF\n\nexport type PrismaPromise<T> = runtime.Types.Public.PrismaPromise<T>\n\n/**\n * Prisma Errors\n */\n\nexport const PrismaClientKnownRequestError = runtime.PrismaClientKnownRequestError\nexport type PrismaClientKnownRequestError = runtime.PrismaClientKnownRequestError\n\nexport const PrismaClientUnknownRequestError = runtime.PrismaClientUnknownRequestError\nexport type PrismaClientUnknownRequestError = runtime.PrismaClientUnknownRequestError\n\nexport const PrismaClientRustPanicError = runtime.PrismaClientRustPanicError\nexport type PrismaClientRustPanicError = runtime.PrismaClientRustPanicError\n\nexport const PrismaClientInitializationError = runtime.PrismaClientInitializationError\nexport type PrismaClientInitializationError = runtime.PrismaClientInitializationError\n\nexport const PrismaClientValidationError = runtime.PrismaClientValidationError\nexport type PrismaClientValidationError = runtime.PrismaClientValidationError\n\n/**\n * Re-export of sql-template-tag\n */\nexport const sql = runtime.sqltag\nexport const empty = runtime.empty\nexport const join = runtime.join\nexport const raw = runtime.raw\nexport const Sql = runtime.Sql\nexport type Sql = runtime.Sql\n\n\n\n/**\n * Decimal.js\n */\nexport const Decimal = runtime.Decimal\nexport type Decimal = runtime.Decimal\n\nexport type DecimalJsLike = runtime.DecimalJsLike\n\n/**\n* Extensions\n*/\nexport type Extension = runtime.Types.Extensions.UserArgs\nexport const getExtensionContext = runtime.Extensions.getExtensionContext\nexport type Args<T, F extends runtime.Operation> = runtime.Types.Public.Args<T, F>\nexport type Payload<T, F extends runtime.Operation = never> = runtime.Types.Public.Payload<T, F>\nexport type Result<T, A, F extends runtime.Operation> = runtime.Types.Public.Result<T, A, F>\nexport type Exact<A, W> = runtime.Types.Public.Exact<A, W>\n\nexport type PrismaVersion = {\n  client: string\n  engine: string\n}\n\n/**\n * Prisma Client JS version: 7.3.0\n * Query Engine version: 9d6ad21cbbceab97458517b147a6a09ff43aa735\n */\nexport const prismaVersion: PrismaVersion = {\n  client: \"7.3.0\",\n  engine: \"9d6ad21cbbceab97458517b147a6a09ff43aa735\"\n}\n\n/**\n * Utility Types\n */\n\nexport type Bytes = runtime.Bytes\nexport type JsonObject = runtime.JsonObject\nexport type JsonArray = runtime.JsonArray\nexport type JsonValue = runtime.JsonValue\nexport type InputJsonObject = runtime.InputJsonObject\nexport type InputJsonArray = runtime.InputJsonArray\nexport type InputJsonValue = runtime.InputJsonValue\n\n\nexport const NullTypes = {\n  DbNull: runtime.NullTypes.DbNull as (new (secret: never) => typeof runtime.DbNull),\n  JsonNull: runtime.NullTypes.JsonNull as (new (secret: never) => typeof runtime.JsonNull),\n  AnyNull: runtime.NullTypes.AnyNull as (new (secret: never) => typeof runtime.AnyNull),\n}\n/**\n * Helper for filtering JSON entries that have `null` on the database (empty on the db)\n *\n * @see https://www.prisma.io/docs/concepts/components/prisma-client/working-with-fields/working-with-json-fields#filtering-on-a-json-field\n */\nexport const DbNull = runtime.DbNull\n\n/**\n * Helper for filtering JSON entries that have JSON `null` values (not empty on the db)\n *\n * @see https://www.prisma.io/docs/concepts/components/prisma-client/working-with-fields/working-with-json-fields#filtering-on-a-json-field\n */\nexport const JsonNull = runtime.JsonNull\n\n/**\n * Helper for filtering JSON entries that are `Prisma.DbNull` or `Prisma.JsonNull`\n *\n * @see https://www.prisma.io/docs/concepts/components/prisma-client/working-with-fields/working-with-json-fields#filtering-on-a-json-field\n */\nexport const AnyNull = runtime.AnyNull\n\n\ntype SelectAndInclude = {\n  select: any\n  include: any\n}\n\ntype SelectAndOmit = {\n  select: any\n  omit: any\n}\n\n/**\n * From T, pick a set of properties whose keys are in the union K\n */\ntype Prisma__Pick<T, K extends keyof T> = {\n    [P in K]: T[P];\n};\n\nexport type Enumerable<T> = T | Array<T>;\n\n/**\n * Subset\n * @desc From `T` pick properties that exist in `U`. Simple version of Intersection\n */\nexport type Subset<T, U> = {\n  [key in keyof T]: key extends keyof U ? T[key] : never;\n};\n\n/**\n * SelectSubset\n * @desc From `T` pick properties that exist in `U`. Simple version of Intersection.\n * Additionally, it validates, if both select and include are present. If the case, it errors.\n */\nexport type SelectSubset<T, U> = {\n  [key in keyof T]: key extends keyof U ? T[key] : never\n} &\n  (T extends SelectAndInclude\n    ? 'Please either choose `select` or `include`.'\n    : T extends SelectAndOmit\n      ? 'Please either choose `select` or `omit`.'\n      : {})\n\n/**\n * Subset + Intersection\n * @desc From `T` pick properties that exist in `U` and intersect `K`\n */\nexport type SubsetIntersection<T, U, K> = {\n  [key in keyof T]: key extends keyof U ? T[key] : never\n} &\n  K\n\ntype Without<T, U> = { [P in Exclude<keyof T, keyof U>]?: never };\n\n/**\n * XOR is needed to have a real mutually exclusive union type\n * https://stackoverflow.com/questions/42123407/does-typescript-support-mutually-exclusive-types\n */\nexport type XOR<T, U> =\n  T extends object ?\n  U extends object ?\n    (Without<T, U> & U) | (Without<U, T> & T)\n  : U : T\n\n\n/**\n * Is T a Record?\n */\ntype IsObject<T extends any> = T extends Array<any>\n? False\n: T extends Date\n? False\n: T extends Uint8Array\n? False\n: T extends BigInt\n? False\n: T extends object\n? True\n: False\n\n\n/**\n * If it's T[], return T\n */\nexport type UnEnumerate<T extends unknown> = T extends Array<infer U> ? U : T\n\n/**\n * From ts-toolbelt\n */\n\ntype __Either<O extends object, K extends Key> = Omit<O, K> &\n  {\n    // Merge all but K\n    [P in K]: Prisma__Pick<O, P & keyof O> // With K possibilities\n  }[K]\n\ntype EitherStrict<O extends object, K extends Key> = Strict<__Either<O, K>>\n\ntype EitherLoose<O extends object, K extends Key> = ComputeRaw<__Either<O, K>>\n\ntype _Either<\n  O extends object,\n  K extends Key,\n  strict extends Boolean\n> = {\n  1: EitherStrict<O, K>\n  0: EitherLoose<O, K>\n}[strict]\n\nexport type Either<\n  O extends object,\n  K extends Key,\n  strict extends Boolean = 1\n> = O extends unknown ? _Either<O, K, strict> : never\n\nexport type Union = any\n\nexport type PatchUndefined<O extends object, O1 extends object> = {\n  [K in keyof O]: O[K] extends undefined ? At<O1, K> : O[K]\n} & {}\n\n/** Helper Types for \"Merge\" **/\nexport type IntersectOf<U extends Union> = (\n  U extends unknown ? (k: U) => void : never\n) extends (k: infer I) => void\n  ? I\n  : never\n\nexport type Overwrite<O extends object, O1 extends object> = {\n    [K in keyof O]: K extends keyof O1 ? O1[K] : O[K];\n} & {};\n\ntype _Merge<U extends object> = IntersectOf<Overwrite<U, {\n    [K in keyof U]-?: At<U, K>;\n}>>;\n\ntype Key = string | number | symbol;\ntype AtStrict<O extends object, K extends Key> = O[K & keyof O];\ntype AtLoose<O extends object, K extends Key> = O extends unknown ? AtStrict<O, K> : never;\nexport type At<O extends object, K extends Key, strict extends Boolean = 1> = {\n    1: AtStrict<O, K>;\n    0: AtLoose<O, K>;\n}[strict];\n\nexport type ComputeRaw<A extends any> = A extends Function ? A : {\n  [K in keyof A]: A[K];\n} & {};\n\nexport type OptionalFlat<O> = {\n  [K in keyof O]?: O[K];\n} & {};\n\ntype _Record<K extends keyof any, T> = {\n  [P in K]: T;\n};\n\n// cause typescript not to expand types and preserve names\ntype NoExpand<T> = T extends unknown ? T : never;\n\n// this type assumes the passed object is entirely optional\nexport type AtLeast<O extends object, K extends string> = NoExpand<\n  O extends unknown\n  ? | (K extends keyof O ? { [P in K]: O[P] } & O : O)\n    | {[P in keyof O as P extends K ? P : never]-?: O[P]} & O\n  : never>;\n\ntype _Strict<U, _U = U> = U extends unknown ? U & OptionalFlat<_Record<Exclude<Keys<_U>, keyof U>, never>> : never;\n\nexport type Strict<U extends object> = ComputeRaw<_Strict<U>>;\n/** End Helper Types for \"Merge\" **/\n\nexport type Merge<U extends object> = ComputeRaw<_Merge<Strict<U>>>;\n\nexport type Boolean = True | False\n\nexport type True = 1\n\nexport type False = 0\n\nexport type Not<B extends Boolean> = {\n  0: 1\n  1: 0\n}[B]\n\nexport type Extends<A1 extends any, A2 extends any> = [A1] extends [never]\n  ? 0 // anything `never` is false\n  : A1 extends A2\n  ? 1\n  : 0\n\nexport type Has<U extends Union, U1 extends Union> = Not<\n  Extends<Exclude<U1, U>, U1>\n>\n\nexport type Or<B1 extends Boolean, B2 extends Boolean> = {\n  0: {\n    0: 0\n    1: 1\n  }\n  1: {\n    0: 1\n    1: 1\n  }\n}[B1][B2]\n\nexport type Keys<U extends Union> = U extends unknown ? keyof U : never\n\nexport type GetScalarType<T, O> = O extends object ? {\n  [P in keyof T]: P extends keyof O\n    ? O[P]\n    : never\n} : never\n\ntype FieldPaths<\n  T,\n  U = Omit<T, '_avg' | '_sum' | '_count' | '_min' | '_max'>\n> = IsObject<T> extends True ? U : T\n\nexport type GetHavingFields<T> = {\n  [K in keyof T]: Or<\n    Or<Extends<'OR', K>, Extends<'AND', K>>,\n    Extends<'NOT', K>\n  > extends True\n    ? // infer is only needed to not hit TS limit\n      // based on the brilliant idea of Pierre-Antoine Mills\n      // https://github.com/microsoft/TypeScript/issues/30188#issuecomment-478938437\n      T[K] extends infer TK\n      ? GetHavingFields<UnEnumerate<TK> extends object ? Merge<UnEnumerate<TK>> : never>\n      : never\n    : {} extends FieldPaths<T[K]>\n    ? never\n    : K\n}[keyof T]\n\n/**\n * Convert tuple to union\n */\ntype _TupleToUnion<T> = T extends (infer E)[] ? E : never\ntype TupleToUnion<K extends readonly any[]> = _TupleToUnion<K>\nexport type MaybeTupleToUnion<T> = T extends any[] ? TupleToUnion<T> : T\n\n/**\n * Like `Pick`, but additionally can also accept an array of keys\n */\nexport type PickEnumerable<T, K extends Enumerable<keyof T> | keyof T> = Prisma__Pick<T, MaybeTupleToUnion<K>>\n\n/**\n * Exclude all keys with underscores\n */\nexport type ExcludeUnderscoreKeys<T extends string> = T extends `_${string}` ? never : T\n\n\nexport type FieldRef<Model, FieldType> = runtime.FieldRef<Model, FieldType>\n\ntype FieldRefInputType<Model, FieldType> = Model extends never ? never : FieldRef<Model, FieldType>\n\n\nexport const ModelName = {\n  User: 'User',\n  Profile: 'Profile',\n  Resume: 'Resume',\n  Interview: 'Interview',\n  Message: 'Message',\n  Score: 'Score',\n  UserAnalytics: 'UserAnalytics',\n  CodingProblem: 'CodingProblem',\n  CodingChallenge: 'CodingChallenge',\n  ChallengeSubmission: 'ChallengeSubmission',\n  ChallengeScore: 'ChallengeScore',\n  CodingTest: 'CodingTest'\n} as const\n\nexport type ModelName = (typeof ModelName)[keyof typeof ModelName]\n\n\n\nexport interface TypeMapCb<GlobalOmitOptions = {}> extends runtime.Types.Utils.Fn<{extArgs: runtime.Types.Extensions.InternalArgs }, runtime.Types.Utils.Record<string, any>> {\n  returns: TypeMap<this['params']['extArgs'], GlobalOmitOptions>\n}\n\nexport type TypeMap<ExtArgs extends runtime.Types.Extensions.InternalArgs = runtime.Types.Extensions.DefaultArgs, GlobalOmitOptions = {}> = {\n  globalOmitOptions: {\n    omit: GlobalOmitOptions\n  }\n  meta: {\n    modelProps: \"user\" | \"profile\" | \"resume\" | \"interview\" | \"message\" | \"score\" | \"userAnalytics\" | \"codingProblem\" | \"codingChallenge\" | \"challengeSubmission\" | \"challengeScore\" | \"codingTest\"\n    txIsolationLevel: TransactionIsolationLevel\n  }\n  model: {\n    User: {\n      payload: Prisma.$UserPayload<ExtArgs>\n      fields: Prisma.UserFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.UserFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.UserFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>\n        }\n        findFirst: {\n          args: Prisma.UserFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.UserFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>\n        }\n        findMany: {\n          args: Prisma.UserFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>[]\n        }\n        create: {\n          args: Prisma.UserCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>\n        }\n        createMany: {\n          args: Prisma.UserCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.UserCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>[]\n        }\n        delete: {\n          args: Prisma.UserDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>\n        }\n        update: {\n          args: Prisma.UserUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>\n        }\n        deleteMany: {\n          args: Prisma.UserDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.UserUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.UserUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>[]\n        }\n        upsert: {\n          args: Prisma.UserUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserPayload>\n        }\n        aggregate: {\n          args: Prisma.UserAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateUser>\n        }\n        groupBy: {\n          args: Prisma.UserGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.UserGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.UserCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.UserCountAggregateOutputType> | number\n        }\n      }\n    }\n    Profile: {\n      payload: Prisma.$ProfilePayload<ExtArgs>\n      fields: Prisma.ProfileFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.ProfileFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.ProfileFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>\n        }\n        findFirst: {\n          args: Prisma.ProfileFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.ProfileFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>\n        }\n        findMany: {\n          args: Prisma.ProfileFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>[]\n        }\n        create: {\n          args: Prisma.ProfileCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>\n        }\n        createMany: {\n          args: Prisma.ProfileCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.ProfileCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>[]\n        }\n        delete: {\n          args: Prisma.ProfileDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>\n        }\n        update: {\n          args: Prisma.ProfileUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>\n        }\n        deleteMany: {\n          args: Prisma.ProfileDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.ProfileUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.ProfileUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>[]\n        }\n        upsert: {\n          args: Prisma.ProfileUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ProfilePayload>\n        }\n        aggregate: {\n          args: Prisma.ProfileAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateProfile>\n        }\n        groupBy: {\n          args: Prisma.ProfileGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ProfileGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.ProfileCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ProfileCountAggregateOutputType> | number\n        }\n      }\n    }\n    Resume: {\n      payload: Prisma.$ResumePayload<ExtArgs>\n      fields: Prisma.ResumeFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.ResumeFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.ResumeFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>\n        }\n        findFirst: {\n          args: Prisma.ResumeFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.ResumeFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>\n        }\n        findMany: {\n          args: Prisma.ResumeFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>[]\n        }\n        create: {\n          args: Prisma.ResumeCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>\n        }\n        createMany: {\n          args: Prisma.ResumeCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.ResumeCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>[]\n        }\n        delete: {\n          args: Prisma.ResumeDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>\n        }\n        update: {\n          args: Prisma.ResumeUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>\n        }\n        deleteMany: {\n          args: Prisma.ResumeDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.ResumeUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.ResumeUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>[]\n        }\n        upsert: {\n          args: Prisma.ResumeUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ResumePayload>\n        }\n        aggregate: {\n          args: Prisma.ResumeAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateResume>\n        }\n        groupBy: {\n          args: Prisma.ResumeGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ResumeGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.ResumeCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ResumeCountAggregateOutputType> | number\n        }\n      }\n    }\n    Interview: {\n      payload: Prisma.$InterviewPayload<ExtArgs>\n      fields: Prisma.InterviewFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.InterviewFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.InterviewFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>\n        }\n        findFirst: {\n          args: Prisma.InterviewFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.InterviewFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>\n        }\n        findMany: {\n          args: Prisma.InterviewFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>[]\n        }\n        create: {\n          args: Prisma.InterviewCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>\n        }\n        createMany: {\n          args: Prisma.InterviewCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.InterviewCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>[]\n        }\n        delete: {\n          args: Prisma.InterviewDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>\n        }\n        update: {\n          args: Prisma.InterviewUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>\n        }\n        deleteMany: {\n          args: Prisma.InterviewDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.InterviewUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.InterviewUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>[]\n        }\n        upsert: {\n          args: Prisma.InterviewUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$InterviewPayload>\n        }\n        aggregate: {\n          args: Prisma.InterviewAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateInterview>\n        }\n        groupBy: {\n          args: Prisma.InterviewGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.InterviewGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.InterviewCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.InterviewCountAggregateOutputType> | number\n        }\n      }\n    }\n    Message: {\n      payload: Prisma.$MessagePayload<ExtArgs>\n      fields: Prisma.MessageFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.MessageFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.MessageFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>\n        }\n        findFirst: {\n          args: Prisma.MessageFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.MessageFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>\n        }\n        findMany: {\n          args: Prisma.MessageFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>[]\n        }\n        create: {\n          args: Prisma.MessageCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>\n        }\n        createMany: {\n          args: Prisma.MessageCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.MessageCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>[]\n        }\n        delete: {\n          args: Prisma.MessageDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>\n        }\n        update: {\n          args: Prisma.MessageUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>\n        }\n        deleteMany: {\n          args: Prisma.MessageDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.MessageUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.MessageUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>[]\n        }\n        upsert: {\n          args: Prisma.MessageUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$MessagePayload>\n        }\n        aggregate: {\n          args: Prisma.MessageAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateMessage>\n        }\n        groupBy: {\n          args: Prisma.MessageGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.MessageGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.MessageCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.MessageCountAggregateOutputType> | number\n        }\n      }\n    }\n    Score: {\n      payload: Prisma.$ScorePayload<ExtArgs>\n      fields: Prisma.ScoreFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.ScoreFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.ScoreFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>\n        }\n        findFirst: {\n          args: Prisma.ScoreFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.ScoreFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>\n        }\n        findMany: {\n          args: Prisma.ScoreFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>[]\n        }\n        create: {\n          args: Prisma.ScoreCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>\n        }\n        createMany: {\n          args: Prisma.ScoreCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.ScoreCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>[]\n        }\n        delete: {\n          args: Prisma.ScoreDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>\n        }\n        update: {\n          args: Prisma.ScoreUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>\n        }\n        deleteMany: {\n          args: Prisma.ScoreDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.ScoreUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.ScoreUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>[]\n        }\n        upsert: {\n          args: Prisma.ScoreUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ScorePayload>\n        }\n        aggregate: {\n          args: Prisma.ScoreAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateScore>\n        }\n        groupBy: {\n          args: Prisma.ScoreGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ScoreGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.ScoreCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ScoreCountAggregateOutputType> | number\n        }\n      }\n    }\n    UserAnalytics: {\n      payload: Prisma.$UserAnalyticsPayload<ExtArgs>\n      fields: Prisma.UserAnalyticsFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.UserAnalyticsFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.UserAnalyticsFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>\n        }\n        findFirst: {\n          args: Prisma.UserAnalyticsFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.UserAnalyticsFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>\n        }\n        findMany: {\n          args: Prisma.UserAnalyticsFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>[]\n        }\n        create: {\n          args: Prisma.UserAnalyticsCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>\n        }\n        createMany: {\n          args: Prisma.UserAnalyticsCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.UserAnalyticsCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>[]\n        }\n        delete: {\n          args: Prisma.UserAnalyticsDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>\n        }\n        update: {\n          args: Prisma.UserAnalyticsUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>\n        }\n        deleteMany: {\n          args: Prisma.UserAnalyticsDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.UserAnalyticsUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.UserAnalyticsUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>[]\n        }\n        upsert: {\n          args: Prisma.UserAnalyticsUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$UserAnalyticsPayload>\n        }\n        aggregate: {\n          args: Prisma.UserAnalyticsAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateUserAnalytics>\n        }\n        groupBy: {\n          args: Prisma.UserAnalyticsGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.UserAnalyticsGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.UserAnalyticsCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.UserAnalyticsCountAggregateOutputType> | number\n        }\n      }\n    }\n    CodingProblem: {\n      payload: Prisma.$CodingProblemPayload<ExtArgs>\n      fields: Prisma.CodingProblemFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.CodingProblemFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.CodingProblemFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>\n        }\n        findFirst: {\n          args: Prisma.CodingProblemFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.CodingProblemFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>\n        }\n        findMany: {\n          args: Prisma.CodingProblemFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>[]\n        }\n        create: {\n          args: Prisma.CodingProblemCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>\n        }\n        createMany: {\n          args: Prisma.CodingProblemCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.CodingProblemCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>[]\n        }\n        delete: {\n          args: Prisma.CodingProblemDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>\n        }\n        update: {\n          args: Prisma.CodingProblemUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>\n        }\n        deleteMany: {\n          args: Prisma.CodingProblemDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.CodingProblemUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.CodingProblemUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>[]\n        }\n        upsert: {\n          args: Prisma.CodingProblemUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingProblemPayload>\n        }\n        aggregate: {\n          args: Prisma.CodingProblemAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateCodingProblem>\n        }\n        groupBy: {\n          args: Prisma.CodingProblemGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.CodingProblemGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.CodingProblemCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.CodingProblemCountAggregateOutputType> | number\n        }\n      }\n    }\n    CodingChallenge: {\n      payload: Prisma.$CodingChallengePayload<ExtArgs>\n      fields: Prisma.CodingChallengeFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.CodingChallengeFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.CodingChallengeFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>\n        }\n        findFirst: {\n          args: Prisma.CodingChallengeFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.CodingChallengeFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>\n        }\n        findMany: {\n          args: Prisma.CodingChallengeFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>[]\n        }\n        create: {\n          args: Prisma.CodingChallengeCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>\n        }\n        createMany: {\n          args: Prisma.CodingChallengeCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.CodingChallengeCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>[]\n        }\n        delete: {\n          args: Prisma.CodingChallengeDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>\n        }\n        update: {\n          args: Prisma.CodingChallengeUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>\n        }\n        deleteMany: {\n          args: Prisma.CodingChallengeDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.CodingChallengeUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.CodingChallengeUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>[]\n        }\n        upsert: {\n          args: Prisma.CodingChallengeUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingChallengePayload>\n        }\n        aggregate: {\n          args: Prisma.CodingChallengeAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateCodingChallenge>\n        }\n        groupBy: {\n          args: Prisma.CodingChallengeGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.CodingChallengeGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.CodingChallengeCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.CodingChallengeCountAggregateOutputType> | number\n        }\n      }\n    }\n    ChallengeSubmission: {\n      payload: Prisma.$ChallengeSubmissionPayload<ExtArgs>\n      fields: Prisma.ChallengeSubmissionFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.ChallengeSubmissionFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.ChallengeSubmissionFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>\n        }\n        findFirst: {\n          args: Prisma.ChallengeSubmissionFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.ChallengeSubmissionFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>\n        }\n        findMany: {\n          args: Prisma.ChallengeSubmissionFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>[]\n        }\n        create: {\n          args: Prisma.ChallengeSubmissionCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>\n        }\n        createMany: {\n          args: Prisma.ChallengeSubmissionCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.ChallengeSubmissionCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>[]\n        }\n        delete: {\n          args: Prisma.ChallengeSubmissionDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>\n        }\n        update: {\n          args: Prisma.ChallengeSubmissionUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>\n        }\n        deleteMany: {\n          args: Prisma.ChallengeSubmissionDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.ChallengeSubmissionUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.ChallengeSubmissionUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>[]\n        }\n        upsert: {\n          args: Prisma.ChallengeSubmissionUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeSubmissionPayload>\n        }\n        aggregate: {\n          args: Prisma.ChallengeSubmissionAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateChallengeSubmission>\n        }\n        groupBy: {\n          args: Prisma.ChallengeSubmissionGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ChallengeSubmissionGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.ChallengeSubmissionCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ChallengeSubmissionCountAggregateOutputType> | number\n        }\n      }\n    }\n    ChallengeScore: {\n      payload: Prisma.$ChallengeScorePayload<ExtArgs>\n      fields: Prisma.ChallengeScoreFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.ChallengeScoreFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.ChallengeScoreFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>\n        }\n        findFirst: {\n          args: Prisma.ChallengeScoreFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.ChallengeScoreFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>\n        }\n        findMany: {\n          args: Prisma.ChallengeScoreFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>[]\n        }\n        create: {\n          args: Prisma.ChallengeScoreCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>\n        }\n        createMany: {\n          args: Prisma.ChallengeScoreCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.ChallengeScoreCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>[]\n        }\n        delete: {\n          args: Prisma.ChallengeScoreDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>\n        }\n        update: {\n          args: Prisma.ChallengeScoreUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>\n        }\n        deleteMany: {\n          args: Prisma.ChallengeScoreDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.ChallengeScoreUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.ChallengeScoreUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>[]\n        }\n        upsert: {\n          args: Prisma.ChallengeScoreUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$ChallengeScorePayload>\n        }\n        aggregate: {\n          args: Prisma.ChallengeScoreAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateChallengeScore>\n        }\n        groupBy: {\n          args: Prisma.ChallengeScoreGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ChallengeScoreGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.ChallengeScoreCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.ChallengeScoreCountAggregateOutputType> | number\n        }\n      }\n    }\n    CodingTest: {\n      payload: Prisma.$CodingTestPayload<ExtArgs>\n      fields: Prisma.CodingTestFieldRefs\n      operations: {\n        findUnique: {\n          args: Prisma.CodingTestFindUniqueArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload> | null\n        }\n        findUniqueOrThrow: {\n          args: Prisma.CodingTestFindUniqueOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>\n        }\n        findFirst: {\n          args: Prisma.CodingTestFindFirstArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload> | null\n        }\n        findFirstOrThrow: {\n          args: Prisma.CodingTestFindFirstOrThrowArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>\n        }\n        findMany: {\n          args: Prisma.CodingTestFindManyArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>[]\n        }\n        create: {\n          args: Prisma.CodingTestCreateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>\n        }\n        createMany: {\n          args: Prisma.CodingTestCreateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        createManyAndReturn: {\n          args: Prisma.CodingTestCreateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>[]\n        }\n        delete: {\n          args: Prisma.CodingTestDeleteArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>\n        }\n        update: {\n          args: Prisma.CodingTestUpdateArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>\n        }\n        deleteMany: {\n          args: Prisma.CodingTestDeleteManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateMany: {\n          args: Prisma.CodingTestUpdateManyArgs<ExtArgs>\n          result: BatchPayload\n        }\n        updateManyAndReturn: {\n          args: Prisma.CodingTestUpdateManyAndReturnArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>[]\n        }\n        upsert: {\n          args: Prisma.CodingTestUpsertArgs<ExtArgs>\n          result: runtime.Types.Utils.PayloadToResult<Prisma.$CodingTestPayload>\n        }\n        aggregate: {\n          args: Prisma.CodingTestAggregateArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.AggregateCodingTest>\n        }\n        groupBy: {\n          args: Prisma.CodingTestGroupByArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.CodingTestGroupByOutputType>[]\n        }\n        count: {\n          args: Prisma.CodingTestCountArgs<ExtArgs>\n          result: runtime.Types.Utils.Optional<Prisma.CodingTestCountAggregateOutputType> | number\n        }\n      }\n    }\n  }\n} & {\n  other: {\n    payload: any\n    operations: {\n      $executeRaw: {\n        args: [query: TemplateStringsArray | Sql, ...values: any[]],\n        result: any\n      }\n      $executeRawUnsafe: {\n        args: [query: string, ...values: any[]],\n        result: any\n      }\n      $queryRaw: {\n        args: [query: TemplateStringsArray | Sql, ...values: any[]],\n        result: any\n      }\n      $queryRawUnsafe: {\n        args: [query: string, ...values: any[]],\n        result: any\n      }\n    }\n  }\n}\n\n/**\n * Enums\n */\n\nexport const TransactionIsolationLevel = runtime.makeStrictEnum({\n  Serializable: 'Serializable'\n} as const)\n\nexport type TransactionIsolationLevel = (typeof TransactionIsolationLevel)[keyof typeof TransactionIsolationLevel]\n\n\nexport const UserScalarFieldEnum = {\n  id: 'id',\n  email: 'email',\n  password: 'password',\n  name: 'name',\n  avatar: 'avatar',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type UserScalarFieldEnum = (typeof UserScalarFieldEnum)[keyof typeof UserScalarFieldEnum]\n\n\nexport const ProfileScalarFieldEnum = {\n  id: 'id',\n  userId: 'userId',\n  bio: 'bio',\n  yearsExperience: 'yearsExperience',\n  skills: 'skills',\n  targetCompanies: 'targetCompanies',\n  targetRole: 'targetRole',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type ProfileScalarFieldEnum = (typeof ProfileScalarFieldEnum)[keyof typeof ProfileScalarFieldEnum]\n\n\nexport const ResumeScalarFieldEnum = {\n  id: 'id',\n  userId: 'userId',\n  fileName: 'fileName',\n  content: 'content',\n  analysis: 'analysis',\n  filePath: 'filePath',\n  atsScore: 'atsScore',\n  atsBreakdown: 'atsBreakdown',\n  atsFeedback: 'atsFeedback',\n  keywords: 'keywords',\n  softSkills: 'softSkills',\n  predictedRoles: 'predictedRoles',\n  uploadedAt: 'uploadedAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type ResumeScalarFieldEnum = (typeof ResumeScalarFieldEnum)[keyof typeof ResumeScalarFieldEnum]\n\n\nexport const InterviewScalarFieldEnum = {\n  id: 'id',\n  userId: 'userId',\n  topic: 'topic',\n  difficulty: 'difficulty',\n  status: 'status',\n  startedAt: 'startedAt',\n  endedAt: 'endedAt',\n  createdAt: 'createdAt',\n  phaseDurations: 'phaseDurations',\n  phaseTransitions: 'phaseTransitions'\n} as const\n\nexport type InterviewScalarFieldEnum = (typeof InterviewScalarFieldEnum)[keyof typeof InterviewScalarFieldEnum]\n\n\nexport const MessageScalarFieldEnum = {\n  id: 'id',\n  interviewId: 'interviewId',\n  role: 'role',\n  content: 'content',\n  timestamp: 'timestamp'\n} as const\n\nexport type MessageScalarFieldEnum = (typeof MessageScalarFieldEnum)[keyof typeof MessageScalarFieldEnum]\n\n\nexport const ScoreScalarFieldEnum = {\n  id: 'id',\n  interviewId: 'interviewId',\n  requirementsClarification: 'requirementsClarification',\n  highLevelDesign: 'highLevelDesign',\n  detailedDesign: 'detailedDesign',\n  scalability: 'scalability',\n  tradeoffs: 'tradeoffs',\n  communication: 'communication',\n  overallScore: 'overallScore',\n  passStatus: 'passStatus',\n  feedback: 'feedback',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type ScoreScalarFieldEnum = (typeof ScoreScalarFieldEnum)[keyof typeof ScoreScalarFieldEnum]\n\n\nexport const UserAnalyticsScalarFieldEnum = {\n  id: 'id',\n  userId: 'userId',\n  totalInterviews: 'totalInterviews',\n  completedInterviews: 'completedInterviews',\n  avgOverallScore: 'avgOverallScore',\n  passRate: 'passRate',\n  weakDimensions: 'weakDimensions',\n  strongDimensions: 'strongDimensions',\n  topicStats: 'topicStats',\n  scoreTrend: 'scoreTrend',\n  lastCalculatedAt: 'lastCalculatedAt',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type UserAnalyticsScalarFieldEnum = (typeof UserAnalyticsScalarFieldEnum)[keyof typeof UserAnalyticsScalarFieldEnum]\n\n\nexport const CodingProblemScalarFieldEnum = {\n  id: 'id',\n  title: 'title',\n  description: 'description',\n  difficulty: 'difficulty',\n  category: 'category',\n  companies: 'companies',\n  constraints: 'constraints',\n  examples: 'examples',\n  testCases: 'testCases',\n  solutionApproaches: 'solutionApproaches',\n  starterCode: 'starterCode',\n  timeLimit: 'timeLimit',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type CodingProblemScalarFieldEnum = (typeof CodingProblemScalarFieldEnum)[keyof typeof CodingProblemScalarFieldEnum]\n\n\nexport const CodingChallengeScalarFieldEnum = {\n  id: 'id',\n  userId: 'userId',\n  problemId: 'problemId',\n  testId: 'testId',\n  title: 'title',\n  description: 'description',\n  difficulty: 'difficulty',\n  category: 'category',\n  company: 'company',\n  language: 'language',\n  visibleTests: 'visibleTests',\n  hiddenTests: 'hiddenTests',\n  timeLimit: 'timeLimit',\n  status: 'status',\n  startedAt: 'startedAt',\n  endedAt: 'endedAt',\n  starterCode: 'starterCode',\n  finalCode: 'finalCode',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type CodingChallengeScalarFieldEnum = (typeof CodingChallengeScalarFieldEnum)[keyof typeof CodingChallengeScalarFieldEnum]\n\n\nexport const ChallengeSubmissionScalarFieldEnum = {\n  id: 'id',\n  challengeId: 'challengeId',\n  code: 'code',\n  language: 'language',\n  submittedAt: 'submittedAt',\n  evaluation: 'evaluation',\n  testResults: 'testResults',\n  isValid: 'isValid',\n  errorMessage: 'errorMessage'\n} as const\n\nexport type ChallengeSubmissionScalarFieldEnum = (typeof ChallengeSubmissionScalarFieldEnum)[keyof typeof ChallengeSubmissionScalarFieldEnum]\n\n\nexport const ChallengeScoreScalarFieldEnum = {\n  id: 'id',\n  challengeId: 'challengeId',\n  correctness: 'correctness',\n  efficiency: 'efficiency',\n  codeQuality: 'codeQuality',\n  edgeCases: 'edgeCases',\n  overallScore: 'overallScore',\n  passStatus: 'passStatus',\n  feedback: 'feedback',\n  suggestedApproach: 'suggestedApproach',\n  complexityAnalysis: 'complexityAnalysis',\n  createdAt: 'createdAt'\n} as const\n\nexport type ChallengeScoreScalarFieldEnum = (typeof ChallengeScoreScalarFieldEnum)[keyof typeof ChallengeScoreScalarFieldEnum]\n\n\nexport const CodingTestScalarFieldEnum = {\n  id: 'id',\n  userId: 'userId',\n  status: 'status',\n  startedAt: 'startedAt',\n  endedAt: 'endedAt',\n  timeLimit: 'timeLimit',\n  createdAt: 'createdAt',\n  updatedAt: 'updatedAt'\n} as const\n\nexport type CodingTestScalarFieldEnum = (typeof CodingTestScalarFieldEnum)[keyof typeof CodingTestScalarFieldEnum]\n\n\nexport const SortOrder = {\n  asc: 'asc',\n  desc: 'desc'\n} as const\n\nexport type SortOrder = (typeof SortOrder)[keyof typeof SortOrder]\n\n\nexport const NullsOrder = {\n  first: 'first',\n  last: 'last'\n} as const\n\nexport type NullsOrder = (typeof NullsOrder)[keyof typeof NullsOrder]\n\n\n\n/**\n * Field references\n */\n\n\n/**\n * Reference to a field of type 'String'\n */\nexport type StringFieldRefInput<$PrismaModel> = FieldRefInputType<$PrismaModel, 'String'>\n    \n\n\n/**\n * Reference to a field of type 'DateTime'\n */\nexport type DateTimeFieldRefInput<$PrismaModel> = FieldRefInputType<$PrismaModel, 'DateTime'>\n    \n\n\n/**\n * Reference to a field of type 'Int'\n */\nexport type IntFieldRefInput<$PrismaModel> = FieldRefInputType<$PrismaModel, 'Int'>\n    \n\n\n/**\n * Reference to a field of type 'Float'\n */\nexport type FloatFieldRefInput<$PrismaModel> = FieldRefInputType<$PrismaModel, 'Float'>\n    \n\n\n/**\n * Reference to a field of type 'Boolean'\n */\nexport type BooleanFieldRefInput<$PrismaModel> = FieldRefInputType<$PrismaModel, 'Boolean'>\n    \n\n/**\n * Batch Payload for updateMany & deleteMany & createMany\n */\nexport type BatchPayload = {\n  count: number\n}\n\nexport const defineExtension = runtime.Extensions.defineExtension as unknown as runtime.Types.Extensions.ExtendsHook<\"define\", TypeMapCb, runtime.Types.Extensions.DefaultArgs>\nexport type DefaultPrismaClient = PrismaClient\nexport type ErrorFormat = 'pretty' | 'colorless' | 'minimal'\nexport type PrismaClientOptions = ({\n  /**\n   * Instance of a Driver Adapter, e.g., like one provided by `@prisma/adapter-pg`.\n   */\n  adapter: runtime.SqlDriverAdapterFactory\n  accelerateUrl?: never\n} | {\n  /**\n   * Prisma Accelerate URL allowing the client to connect through Accelerate instead of a direct database.\n   */\n  accelerateUrl: string\n  adapter?: never\n}) & {\n  /**\n   * @default \"colorless\"\n   */\n  errorFormat?: ErrorFormat\n  /**\n   * @example\n   * ```\n   * // Shorthand for `emit: 'stdout'`\n   * log: ['query', 'info', 'warn', 'error']\n   * \n   * // Emit as events only\n   * log: [\n   *   { emit: 'event', level: 'query' },\n   *   { emit: 'event', level: 'info' },\n   *   { emit: 'event', level: 'warn' }\n   *   { emit: 'event', level: 'error' }\n   * ]\n   * \n   * / Emit as events and log to stdout\n   * og: [\n   *  { emit: 'stdout', level: 'query' },\n   *  { emit: 'stdout', level: 'info' },\n   *  { emit: 'stdout', level: 'warn' }\n   *  { emit: 'stdout', level: 'error' }\n   * \n   * ```\n   * Read more in our [docs](https://pris.ly/d/logging).\n   */\n  log?: (LogLevel | LogDefinition)[]\n  /**\n   * The default values for transactionOptions\n   * maxWait ?= 2000\n   * timeout ?= 5000\n   */\n  transactionOptions?: {\n    maxWait?: number\n    timeout?: number\n    isolationLevel?: TransactionIsolationLevel\n  }\n  /**\n   * Global configuration for omitting model fields by default.\n   * \n   * @example\n   * ```\n   * const prisma = new PrismaClient({\n   *   omit: {\n   *     user: {\n   *       password: true\n   *     }\n   *   }\n   * })\n   * ```\n   */\n  omit?: GlobalOmitConfig\n  /**\n   * SQL commenter plugins that add metadata to SQL queries as comments.\n   * Comments follow the sqlcommenter format: https://google.github.io/sqlcommenter/\n   * \n   * @example\n   * ```\n   * const prisma = new PrismaClient({\n   *   adapter,\n   *   comments: [\n   *     traceContext(),\n   *     queryInsights(),\n   *   ],\n   * })\n   * ```\n   */\n  comments?: runtime.SqlCommenterPlugin[]\n}\nexport type GlobalOmitConfig = {\n  user?: Prisma.UserOmit\n  profile?: Prisma.ProfileOmit\n  resume?: Prisma.ResumeOmit\n  interview?: Prisma.InterviewOmit\n  message?: Prisma.MessageOmit\n  score?: Prisma.ScoreOmit\n  userAnalytics?: Prisma.UserAnalyticsOmit\n  codingProblem?: Prisma.CodingProblemOmit\n  codingChallenge?: Prisma.CodingChallengeOmit\n  challengeSubmission?: Prisma.ChallengeSubmissionOmit\n  challengeScore?: Prisma.ChallengeScoreOmit\n  codingTest?: Prisma.CodingTestOmit\n}\n\n/* Types for Logging */\nexport type LogLevel = 'info' | 'query' | 'warn' | 'error'\nexport type LogDefinition = {\n  level: LogLevel\n  emit: 'stdout' | 'event'\n}\n\nexport type CheckIsLogLevel<T> = T extends LogLevel ? T : never;\n\nexport type GetLogType<T> = CheckIsLogLevel<\n  T extends LogDefinition ? T['level'] : T\n>;\n\nexport type GetEvents<T extends any[]> = T extends Array<LogLevel | LogDefinition>\n  ? GetLogType<T[number]>\n  : never;\n\nexport type QueryEvent = {\n  timestamp: Date\n  query: string\n  params: string\n  duration: number\n  target: string\n}\n\nexport type LogEvent = {\n  timestamp: Date\n  message: string\n  target: string\n}\n/* End Types for Logging */\n\n\nexport type PrismaAction =\n  | 'findUnique'\n  | 'findUniqueOrThrow'\n  | 'findMany'\n  | 'findFirst'\n  | 'findFirstOrThrow'\n  | 'create'\n  | 'createMany'\n  | 'createManyAndReturn'\n  | 'update'\n  | 'updateMany'\n  | 'updateManyAndReturn'\n  | 'upsert'\n  | 'delete'\n  | 'deleteMany'\n  | 'executeRaw'\n  | 'queryRaw'\n  | 'aggregate'\n  | 'count'\n  | 'runCommandRaw'\n  | 'findRaw'\n  | 'groupBy'\n\n/**\n * `PrismaClient` proxy available in interactive transactions.\n */\nexport type TransactionClient = Omit<DefaultPrismaClient, runtime.ITXClientDenyList>\n\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AACA,mEAAmE,GACnE,kBAAkB,GAClB,wCAAwC;AACxC,eAAe;AACf;;;;;;;;;;CAUC,GAED;;AAcO,MAAM,gCAAgC,0UAAqC;AAG3E,MAAM,kCAAkC,4UAAuC;AAG/E,MAAM,6BAA6B,uUAAkC;AAGrE,MAAM,kCAAkC,4UAAuC;AAG/E,MAAM,8BAA8B,wUAAmC;AAMvE,MAAM,MAAM,mTAAc;AAC1B,MAAM,QAAQ,kTAAa;AAC3B,MAAM,OAAO,iTAAY;AACzB,MAAM,MAAM,gTAAW;AACvB,MAAM,MAAM,gTAAW;AAQvB,MAAM,UAAU,oTAAe;AAS/B,MAAM,sBAAsB,uTAAkB,CAAC,mBAAmB;AAelE,MAAM,gBAA+B;IAC1C,QAAQ;IACR,QAAQ;AACV;AAeO,MAAM,YAAY;IACvB,QAAQ,sTAAiB,CAAC,MAAM;IAChC,UAAU,sTAAiB,CAAC,QAAQ;IACpC,SAAS,sTAAiB,CAAC,OAAO;AACpC;AAMO,MAAM,SAAS,mTAAc;AAO7B,MAAM,WAAW,qTAAgB;AAOjC,MAAM,UAAU,oTAAe;AAkQ/B,MAAM,YAAY;IACvB,MAAM;IACN,SAAS;IACT,QAAQ;IACR,WAAW;IACX,SAAS;IACT,OAAO;IACP,eAAe;IACf,eAAe;IACf,iBAAiB;IACjB,qBAAqB;IACrB,gBAAgB;IAChB,YAAY;AACd;AAw6BO,MAAM,4BAA4B,2TAAsB,CAAC;IAC9D,cAAc;AAChB;AAKO,MAAM,sBAAsB;IACjC,IAAI;IACJ,OAAO;IACP,UAAU;IACV,MAAM;IACN,QAAQ;IACR,WAAW;IACX,WAAW;AACb;AAKO,MAAM,yBAAyB;IACpC,IAAI;IACJ,QAAQ;IACR,KAAK;IACL,iBAAiB;IACjB,QAAQ;IACR,iBAAiB;IACjB,YAAY;IACZ,WAAW;IACX,WAAW;AACb;AAKO,MAAM,wBAAwB;IACnC,IAAI;IACJ,QAAQ;IACR,UAAU;IACV,SAAS;IACT,UAAU;IACV,UAAU;IACV,UAAU;IACV,cAAc;IACd,aAAa;IACb,UAAU;IACV,YAAY;IACZ,gBAAgB;IAChB,YAAY;IACZ,WAAW;AACb;AAKO,MAAM,2BAA2B;IACtC,IAAI;IACJ,QAAQ;IACR,OAAO;IACP,YAAY;IACZ,QAAQ;IACR,WAAW;IACX,SAAS;IACT,WAAW;IACX,gBAAgB;IAChB,kBAAkB;AACpB;AAKO,MAAM,yBAAyB;IACpC,IAAI;IACJ,aAAa;IACb,MAAM;IACN,SAAS;IACT,WAAW;AACb;AAKO,MAAM,uBAAuB;IAClC,IAAI;IACJ,aAAa;IACb,2BAA2B;IAC3B,iBAAiB;IACjB,gBAAgB;IAChB,aAAa;IACb,WAAW;IACX,eAAe;IACf,cAAc;IACd,YAAY;IACZ,UAAU;IACV,WAAW;IACX,WAAW;AACb;AAKO,MAAM,+BAA+B;IAC1C,IAAI;IACJ,QAAQ;IACR,iBAAiB;IACjB,qBAAqB;IACrB,iBAAiB;IACjB,UAAU;IACV,gBAAgB;IAChB,kBAAkB;IAClB,YAAY;IACZ,YAAY;IACZ,kBAAkB;IAClB,WAAW;IACX,WAAW;AACb;AAKO,MAAM,+BAA+B;IAC1C,IAAI;IACJ,OAAO;IACP,aAAa;IACb,YAAY;IACZ,UAAU;IACV,WAAW;IACX,aAAa;IACb,UAAU;IACV,WAAW;IACX,oBAAoB;IACpB,aAAa;IACb,WAAW;IACX,WAAW;IACX,WAAW;AACb;AAKO,MAAM,iCAAiC;IAC5C,IAAI;IACJ,QAAQ;IACR,WAAW;IACX,QAAQ;IACR,OAAO;IACP,aAAa;IACb,YAAY;IACZ,UAAU;IACV,SAAS;IACT,UAAU;IACV,cAAc;IACd,aAAa;IACb,WAAW;IACX,QAAQ;IACR,WAAW;IACX,SAAS;IACT,aAAa;IACb,WAAW;IACX,WAAW;IACX,WAAW;AACb;AAKO,MAAM,qCAAqC;IAChD,IAAI;IACJ,aAAa;IACb,MAAM;IACN,UAAU;IACV,aAAa;IACb,YAAY;IACZ,aAAa;IACb,SAAS;IACT,cAAc;AAChB;AAKO,MAAM,gCAAgC;IAC3C,IAAI;IACJ,aAAa;IACb,aAAa;IACb,YAAY;IACZ,aAAa;IACb,WAAW;IACX,cAAc;IACd,YAAY;IACZ,UAAU;IACV,mBAAmB;IACnB,oBAAoB;IACpB,WAAW;AACb;AAKO,MAAM,4BAA4B;IACvC,IAAI;IACJ,QAAQ;IACR,QAAQ;IACR,WAAW;IACX,SAAS;IACT,WAAW;IACX,WAAW;IACX,WAAW;AACb;AAKO,MAAM,YAAY;IACvB,KAAK;IACL,MAAM;AACR;AAKO,MAAM,aAAa;IACxB,OAAO;IACP,MAAM;AACR;AAoDO,MAAM,kBAAkB,uTAAkB,CAAC,eAAe"}},
    {"offset": {"line": 413, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/generated/prisma/enums.ts"],"sourcesContent":["\n/* !!! This is code generated by Prisma. Do not edit directly. !!! */\n/* eslint-disable */\n// biome-ignore-all lint: generated file\n// @ts-nocheck \n/*\n* This file exports all enum related types from the schema.\n*\n* ðŸŸ¢ You can import this file directly.\n*/\n\n\n\n// This file is empty because there are no enums in the schema.\nexport {}\n"],"names":[],"mappings":"AACA,mEAAmE,GACnE,kBAAkB,GAClB,wCAAwC;AACxC,eAAe;AACf;;;;AAIA,GAIA,+DAA+D"}},
    {"offset": {"line": 426, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/generated/prisma/client.ts"],"sourcesContent":["\n/* !!! This is code generated by Prisma. Do not edit directly. !!! */\n/* eslint-disable */\n// biome-ignore-all lint: generated file\n// @ts-nocheck \n/*\n * This file should be your main import to use Prisma. Through it you get access to all the models, enums, and input types.\n * If you're looking for something you can import in the client-side of your application, please refer to the `browser.ts` file instead.\n *\n * ðŸŸ¢ You can import this file directly.\n */\n\nimport * as process from 'node:process'\nimport * as path from 'node:path'\nimport { fileURLToPath } from 'node:url'\nglobalThis['__dirname'] = path.dirname(fileURLToPath(import.meta.url))\n\nimport * as runtime from \"@prisma/client/runtime/client\"\nimport * as $Enums from \"./enums\"\nimport * as $Class from \"./internal/class\"\nimport * as Prisma from \"./internal/prismaNamespace\"\n\nexport * as $Enums from './enums'\nexport * from \"./enums\"\n/**\n * ## Prisma Client\n * \n * Type-safe database client for TypeScript\n * @example\n * ```\n * const prisma = new PrismaClient()\n * // Fetch zero or more Users\n * const users = await prisma.user.findMany()\n * ```\n * \n * Read more in our [docs](https://pris.ly/d/client).\n */\nexport const PrismaClient = $Class.getPrismaClientClass()\nexport type PrismaClient<LogOpts extends Prisma.LogLevel = never, OmitOpts extends Prisma.PrismaClientOptions[\"omit\"] = Prisma.PrismaClientOptions[\"omit\"], ExtArgs extends runtime.Types.Extensions.InternalArgs = runtime.Types.Extensions.DefaultArgs> = $Class.PrismaClient<LogOpts, OmitOpts, ExtArgs>\nexport { Prisma }\n\n/**\n * Model User\n * \n */\nexport type User = Prisma.UserModel\n/**\n * Model Profile\n * \n */\nexport type Profile = Prisma.ProfileModel\n/**\n * Model Resume\n * \n */\nexport type Resume = Prisma.ResumeModel\n/**\n * Model Interview\n * \n */\nexport type Interview = Prisma.InterviewModel\n/**\n * Model Message\n * \n */\nexport type Message = Prisma.MessageModel\n/**\n * Model Score\n * \n */\nexport type Score = Prisma.ScoreModel\n/**\n * Model UserAnalytics\n * \n */\nexport type UserAnalytics = Prisma.UserAnalyticsModel\n/**\n * Model CodingProblem\n * \n */\nexport type CodingProblem = Prisma.CodingProblemModel\n/**\n * Model CodingChallenge\n * \n */\nexport type CodingChallenge = Prisma.CodingChallengeModel\n/**\n * Model ChallengeSubmission\n * \n */\nexport type ChallengeSubmission = Prisma.ChallengeSubmissionModel\n/**\n * Model ChallengeScore\n * \n */\nexport type ChallengeScore = Prisma.ChallengeScoreModel\n/**\n * Model CodingTest\n * \n */\nexport type CodingTest = Prisma.CodingTestModel\n"],"names":[],"mappings":"AACA,mEAAmE,GACnE,kBAAkB,GAClB,wCAAwC;AACxC,eAAe;AACf;;;;;CAKC;;;;AAGD;AACA;AAKA;AACA;AAEA;;;;;;;;AAPA,UAAU,CAAC,YAAY,GAAG,4HAAY,CAAC,IAAA,gIAAa,EAAC,8BAAY,GAAG;;;;;AAsB7D,MAAM,eAAe,kPAA2B"}},
    {"offset": {"line": 462, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/prisma.ts"],"sourcesContent":["import { PrismaClient } from \"../generated/prisma/client\";\nimport { PrismaLibSql } from \"@prisma/adapter-libsql\";\n\nconst adapter = new PrismaLibSql({\n  url: process.env.DATABASE_URL || \"file:./dev.db\",\n});\n\nconst globalForPrisma = globalThis as unknown as {\n  prisma: PrismaClient | undefined;\n};\n\nexport const prisma = globalForPrisma.prisma ?? new PrismaClient({ adapter });\n\nif (process.env.NODE_ENV !== \"production\") globalForPrisma.prisma = prisma;\n\nexport default prisma;\n"],"names":[],"mappings":";;;;;;AAAA;AACA;;;;;;;AAEA,MAAM,UAAU,IAAI,kQAAY,CAAC;IAC/B,KAAK,QAAQ,GAAG,CAAC,YAAY,IAAI;AACnC;AAEA,MAAM,kBAAkB;AAIjB,MAAM,SAAS,gBAAgB,MAAM,IAAI,IAAI,+OAAY,CAAC;IAAE;AAAQ;AAE3E,wCAA2C,gBAAgB,MAAM,GAAG;uCAErD"}},
    {"offset": {"line": 492, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/auth.ts"],"sourcesContent":["import NextAuth from \"next-auth\";\nimport Credentials from \"next-auth/providers/credentials\";\nimport bcrypt from \"bcryptjs\";\nimport { prisma } from \"./prisma\";\n\nexport const { handlers, signIn, signOut, auth } = NextAuth({\n  providers: [\n    Credentials({\n      name: \"credentials\",\n      credentials: {\n        email: { label: \"Email\", type: \"email\" },\n        password: { label: \"Password\", type: \"password\" },\n      },\n      async authorize(credentials) {\n        if (!credentials?.email || !credentials?.password) {\n          return null;\n        }\n\n        const email = credentials.email as string;\n        const password = credentials.password as string;\n\n        const user = await prisma.user.findUnique({\n          where: { email },\n        });\n\n        if (!user) {\n          return null;\n        }\n\n        const isPasswordValid = await bcrypt.compare(password, user.password);\n\n        if (!isPasswordValid) {\n          return null;\n        }\n\n        return {\n          id: user.id,\n          email: user.email,\n          name: user.name,\n          image: user.avatar,\n        };\n      },\n    }),\n  ],\n  callbacks: {\n    async jwt({ token, user }) {\n      if (user) {\n        token.id = user.id;\n      }\n      return token;\n    },\n    async session({ session, token }) {\n      if (session.user) {\n        session.user.id = token.id as string;\n      }\n      return session;\n    },\n  },\n  pages: {\n    signIn: \"/login\",\n  },\n  session: {\n    strategy: \"jwt\",\n  },\n});\n"],"names":[],"mappings":";;;;;;;;;;AAAA;AACA;AAAA;AACA;AACA;;;;;;;;;AAEO,MAAM,EAAE,QAAQ,EAAE,MAAM,EAAE,OAAO,EAAE,IAAI,EAAE,GAAG,IAAA,2OAAQ,EAAC;IAC1D,WAAW;QACT,IAAA,gPAAW,EAAC;YACV,MAAM;YACN,aAAa;gBACX,OAAO;oBAAE,OAAO;oBAAS,MAAM;gBAAQ;gBACvC,UAAU;oBAAE,OAAO;oBAAY,MAAM;gBAAW;YAClD;YACA,MAAM,WAAU,WAAW;gBACzB,IAAI,CAAC,aAAa,SAAS,CAAC,aAAa,UAAU;oBACjD,OAAO;gBACT;gBAEA,MAAM,QAAQ,YAAY,KAAK;gBAC/B,MAAM,WAAW,YAAY,QAAQ;gBAErC,MAAM,OAAO,MAAM,yMAAM,CAAC,IAAI,CAAC,UAAU,CAAC;oBACxC,OAAO;wBAAE;oBAAM;gBACjB;gBAEA,IAAI,CAAC,MAAM;oBACT,OAAO;gBACT;gBAEA,MAAM,kBAAkB,MAAM,uNAAM,CAAC,OAAO,CAAC,UAAU,KAAK,QAAQ;gBAEpE,IAAI,CAAC,iBAAiB;oBACpB,OAAO;gBACT;gBAEA,OAAO;oBACL,IAAI,KAAK,EAAE;oBACX,OAAO,KAAK,KAAK;oBACjB,MAAM,KAAK,IAAI;oBACf,OAAO,KAAK,MAAM;gBACpB;YACF;QACF;KACD;IACD,WAAW;QACT,MAAM,KAAI,EAAE,KAAK,EAAE,IAAI,EAAE;YACvB,IAAI,MAAM;gBACR,MAAM,EAAE,GAAG,KAAK,EAAE;YACpB;YACA,OAAO;QACT;QACA,MAAM,SAAQ,EAAE,OAAO,EAAE,KAAK,EAAE;YAC9B,IAAI,QAAQ,IAAI,EAAE;gBAChB,QAAQ,IAAI,CAAC,EAAE,GAAG,MAAM,EAAE;YAC5B;YACA,OAAO;QACT;IACF;IACA,OAAO;QACL,QAAQ;IACV;IACA,SAAS;QACP,UAAU;IACZ;AACF"}},
    {"offset": {"line": 582, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/llm/ollama-provider.ts"],"sourcesContent":["/**\n * Ollama Provider\n *\n * Handles all Ollama-specific LLM interactions with timeout support.\n */\n\nimport type { LLMMessage, LLMCompletionOptions, LLMResponse, LLMStreamResponse } from \"./index\";\n\nconst OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || \"http://localhost:11434\";\nconst OLLAMA_MODEL = process.env.OLLAMA_MODEL || \"llama3\";\nconst OLLAMA_CODE_MODEL = process.env.OLLAMA_CODE_MODEL || \"codellama:7b\";\nconst OLLAMA_TIMEOUT = 30000; // 30 second timeout\n\nexport type OllamaModelType = \"general\" | \"coding\";\n\nfunction getModelForType(type: OllamaModelType): string {\n  return type === \"coding\" ? OLLAMA_CODE_MODEL : OLLAMA_MODEL;\n}\n\n/**\n * Check if Ollama is healthy\n */\nexport async function checkOllamaHealth(): Promise<boolean> {\n  try {\n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), 5000);\n\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {\n      method: \"GET\",\n      signal: controller.signal,\n    });\n\n    clearTimeout(timeout);\n    return response.ok;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Non-streaming completion with Ollama\n */\nexport async function ollamaComplete(\n  options: LLMCompletionOptions,\n  modelType: OllamaModelType = \"general\"\n): Promise<LLMResponse> {\n  const model = getModelForType(modelType);\n  const controller = new AbortController();\n  const timeout = setTimeout(() => controller.abort(), OLLAMA_TIMEOUT);\n\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        model,\n        messages: options.messages,\n        stream: false,\n        options: {\n          temperature: options.temperature ?? 0.7,\n          num_predict: options.maxTokens ?? 800,\n          top_p: options.topP ?? 0.9,\n          stop: options.stop,\n        },\n      }),\n      signal: controller.signal,\n    });\n\n    clearTimeout(timeout);\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Ollama error: ${error}`);\n    }\n\n    const data = await response.json();\n\n    return {\n      content: data.message?.content || \"\",\n      provider: \"ollama\",\n      fallbackUsed: false,\n      model,\n    };\n  } catch (error) {\n    clearTimeout(timeout);\n    throw error;\n  }\n}\n\n/**\n * Streaming completion with Ollama\n */\nexport async function ollamaStreamComplete(\n  options: LLMCompletionOptions,\n  modelType: OllamaModelType = \"general\"\n): Promise<LLMStreamResponse> {\n  const model = getModelForType(modelType);\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages: options.messages,\n      stream: true,\n      options: {\n        temperature: options.temperature ?? 0.7,\n        num_predict: options.maxTokens ?? 800,\n        top_p: options.topP ?? 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama streaming error: ${error}`);\n  }\n\n  if (!response.body) {\n    throw new Error(\"No response body from Ollama\");\n  }\n\n  // Transform Ollama stream to SSE format\n  const stream = transformOllamaStream(response.body);\n\n  return {\n    stream,\n    provider: \"ollama\",\n    fallbackUsed: false,\n    model,\n  };\n}\n\n/**\n * Transform Ollama stream to SSE format\n */\nfunction transformOllamaStream(\n  ollamaStream: ReadableStream<Uint8Array>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n  let buffer = \"\";\n\n  return new ReadableStream({\n    async start(controller) {\n      const reader = ollamaStream.getReader();\n\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n\n          if (done) {\n            controller.close();\n            break;\n          }\n\n          buffer += decoder.decode(value, { stream: true });\n\n          // Process complete JSON lines\n          const lines = buffer.split(\"\\n\");\n          buffer = lines.pop() || \"\";\n\n          for (const line of lines) {\n            if (line.trim()) {\n              try {\n                const json = JSON.parse(line);\n                if (json.message?.content) {\n                  const sseData = `data: ${JSON.stringify({ content: json.message.content })}\\n\\n`;\n                  controller.enqueue(encoder.encode(sseData));\n                }\n                if (json.done) {\n                  controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n                }\n              } catch {\n                // Skip invalid JSON lines\n              }\n            }\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n"],"names":[],"mappings":"AAAA;;;;CAIC;;;;;;;;AAID,MAAM,kBAAkB,QAAQ,GAAG,CAAC,eAAe,IAAI;AACvD,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AACjD,MAAM,oBAAoB,QAAQ,GAAG,CAAC,iBAAiB,IAAI;AAC3D,MAAM,iBAAiB,OAAO,oBAAoB;AAIlD,SAAS,gBAAgB,IAAqB;IAC5C,OAAO,SAAS,WAAW,oBAAoB;AACjD;AAKO,eAAe;IACpB,IAAI;QACF,MAAM,aAAa,IAAI;QACvB,MAAM,UAAU,WAAW,IAAM,WAAW,KAAK,IAAI;QAErD,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;YAC1D,QAAQ;YACR,QAAQ,WAAW,MAAM;QAC3B;QAEA,aAAa;QACb,OAAO,SAAS,EAAE;IACpB,EAAE,OAAM;QACN,OAAO;IACT;AACF;AAKO,eAAe,eACpB,OAA6B,EAC7B,YAA6B,SAAS;IAEtC,MAAM,QAAQ,gBAAgB;IAC9B,MAAM,aAAa,IAAI;IACvB,MAAM,UAAU,WAAW,IAAM,WAAW,KAAK,IAAI;IAErD,IAAI;QACF,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;YAC1D,QAAQ;YACR,SAAS;gBACP,gBAAgB;YAClB;YACA,MAAM,KAAK,SAAS,CAAC;gBACnB;gBACA,UAAU,QAAQ,QAAQ;gBAC1B,QAAQ;gBACR,SAAS;oBACP,aAAa,QAAQ,WAAW,IAAI;oBACpC,aAAa,QAAQ,SAAS,IAAI;oBAClC,OAAO,QAAQ,IAAI,IAAI;oBACvB,MAAM,QAAQ,IAAI;gBACpB;YACF;YACA,QAAQ,WAAW,MAAM;QAC3B;QAEA,aAAa;QAEb,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;YACjC,MAAM,IAAI,MAAM,CAAC,cAAc,EAAE,OAAO;QAC1C;QAEA,MAAM,OAAO,MAAM,SAAS,IAAI;QAEhC,OAAO;YACL,SAAS,KAAK,OAAO,EAAE,WAAW;YAClC,UAAU;YACV,cAAc;YACd;QACF;IACF,EAAE,OAAO,OAAO;QACd,aAAa;QACb,MAAM;IACR;AACF;AAKO,eAAe,qBACpB,OAA6B,EAC7B,YAA6B,SAAS;IAEtC,MAAM,QAAQ,gBAAgB;IAE9B,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;QAC1D,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;QACA,MAAM,KAAK,SAAS,CAAC;YACnB;YACA,UAAU,QAAQ,QAAQ;YAC1B,QAAQ;YACR,SAAS;gBACP,aAAa,QAAQ,WAAW,IAAI;gBACpC,aAAa,QAAQ,SAAS,IAAI;gBAClC,OAAO,QAAQ,IAAI,IAAI;gBACvB,MAAM,QAAQ,IAAI;YACpB;QACF;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;QACjC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,OAAO;IACpD;IAEA,IAAI,CAAC,SAAS,IAAI,EAAE;QAClB,MAAM,IAAI,MAAM;IAClB;IAEA,wCAAwC;IACxC,MAAM,SAAS,sBAAsB,SAAS,IAAI;IAElD,OAAO;QACL;QACA,UAAU;QACV,cAAc;QACd;IACF;AACF;AAEA;;CAEC,GACD,SAAS,sBACP,YAAwC;IAExC,MAAM,UAAU,IAAI;IACpB,MAAM,UAAU,IAAI;IACpB,IAAI,SAAS;IAEb,OAAO,IAAI,eAAe;QACxB,MAAM,OAAM,UAAU;YACpB,MAAM,SAAS,aAAa,SAAS;YAErC,IAAI;gBACF,MAAO,KAAM;oBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;oBAEzC,IAAI,MAAM;wBACR,WAAW,KAAK;wBAChB;oBACF;oBAEA,UAAU,QAAQ,MAAM,CAAC,OAAO;wBAAE,QAAQ;oBAAK;oBAE/C,8BAA8B;oBAC9B,MAAM,QAAQ,OAAO,KAAK,CAAC;oBAC3B,SAAS,MAAM,GAAG,MAAM;oBAExB,KAAK,MAAM,QAAQ,MAAO;wBACxB,IAAI,KAAK,IAAI,IAAI;4BACf,IAAI;gCACF,MAAM,OAAO,KAAK,KAAK,CAAC;gCACxB,IAAI,KAAK,OAAO,EAAE,SAAS;oCACzB,MAAM,UAAU,CAAC,MAAM,EAAE,KAAK,SAAS,CAAC;wCAAE,SAAS,KAAK,OAAO,CAAC,OAAO;oCAAC,GAAG,IAAI,CAAC;oCAChF,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;gCACpC;gCACA,IAAI,KAAK,IAAI,EAAE;oCACb,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;gCACpC;4BACF,EAAE,OAAM;4BACN,0BAA0B;4BAC5B;wBACF;oBACF;gBACF;YACF,EAAE,OAAO,OAAO;gBACd,WAAW,KAAK,CAAC;YACnB;QACF;IACF;AACF"}},
    {"offset": {"line": 741, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/llm/openai-provider.ts"],"sourcesContent":["/**\n * OpenAI Provider\n *\n * Handles all OpenAI-specific LLM interactions as fallback.\n */\n\nimport OpenAI from \"openai\";\nimport type { LLMCompletionOptions, LLMResponse, LLMStreamResponse } from \"./index\";\n\nconst OPENAI_MODEL = process.env.OPENAI_MODEL || \"gpt-4o-mini\";\n\nlet openaiClient: OpenAI | null = null;\n\nfunction getOpenAIClient(): OpenAI {\n  if (!openaiClient) {\n    if (!process.env.OPENAI_API_KEY) {\n      throw new Error(\"OpenAI API key not configured\");\n    }\n    openaiClient = new OpenAI({\n      apiKey: process.env.OPENAI_API_KEY,\n    });\n  }\n  return openaiClient;\n}\n\n/**\n * Check if OpenAI is configured\n */\nexport function isOpenAIConfigured(): boolean {\n  return !!process.env.OPENAI_API_KEY;\n}\n\n/**\n * Non-streaming completion with OpenAI\n */\nexport async function openaiComplete(\n  options: LLMCompletionOptions\n): Promise<LLMResponse> {\n  const client = getOpenAIClient();\n\n  const response = await client.chat.completions.create({\n    model: OPENAI_MODEL,\n    messages: options.messages.map((m) => ({\n      role: m.role,\n      content: m.content,\n    })),\n    temperature: options.temperature ?? 0.7,\n    max_tokens: options.maxTokens ?? 800,\n    top_p: options.topP ?? 0.9,\n    stop: options.stop,\n  });\n\n  return {\n    content: response.choices[0]?.message?.content || \"\",\n    provider: \"openai\",\n    fallbackUsed: true,\n    model: OPENAI_MODEL,\n  };\n}\n\n/**\n * Streaming completion with OpenAI\n */\nexport async function openaiStreamComplete(\n  options: LLMCompletionOptions\n): Promise<LLMStreamResponse> {\n  const client = getOpenAIClient();\n\n  const response = await client.chat.completions.create({\n    model: OPENAI_MODEL,\n    messages: options.messages.map((m) => ({\n      role: m.role,\n      content: m.content,\n    })),\n    temperature: options.temperature ?? 0.7,\n    max_tokens: options.maxTokens ?? 800,\n    top_p: options.topP ?? 0.9,\n    stop: options.stop,\n    stream: true,\n  });\n\n  // Transform OpenAI stream to our SSE format\n  const stream = transformOpenAIStream(response);\n\n  return {\n    stream,\n    provider: \"openai\",\n    fallbackUsed: true,\n    model: OPENAI_MODEL,\n  };\n}\n\n/**\n * Transform OpenAI stream to SSE format matching our expected format\n */\nfunction transformOpenAIStream(\n  openaiStream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n\n  return new ReadableStream({\n    async start(controller) {\n      try {\n        for await (const chunk of openaiStream) {\n          const content = chunk.choices[0]?.delta?.content;\n          if (content) {\n            const sseData = `data: ${JSON.stringify({ content })}\\n\\n`;\n            controller.enqueue(encoder.encode(sseData));\n          }\n\n          // Check if this is the final chunk\n          if (chunk.choices[0]?.finish_reason) {\n            controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n          }\n        }\n        controller.close();\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n"],"names":[],"mappings":";;;;;;;;AAAA;;;;CAIC,GAED;AAAA;;AAGA,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AAEjD,IAAI,eAA8B;AAElC,SAAS;IACP,IAAI,CAAC,cAAc;QACjB,IAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,EAAE;YAC/B,MAAM,IAAI,MAAM;QAClB;QACA,eAAe,IAAI,4PAAM,CAAC;YACxB,QAAQ,QAAQ,GAAG,CAAC,cAAc;QACpC;IACF;IACA,OAAO;AACT;AAKO,SAAS;IACd,OAAO,CAAC,CAAC,QAAQ,GAAG,CAAC,cAAc;AACrC;AAKO,eAAe,eACpB,OAA6B;IAE7B,MAAM,SAAS;IAEf,MAAM,WAAW,MAAM,OAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC;QACpD,OAAO;QACP,UAAU,QAAQ,QAAQ,CAAC,GAAG,CAAC,CAAC,IAAM,CAAC;gBACrC,MAAM,EAAE,IAAI;gBACZ,SAAS,EAAE,OAAO;YACpB,CAAC;QACD,aAAa,QAAQ,WAAW,IAAI;QACpC,YAAY,QAAQ,SAAS,IAAI;QACjC,OAAO,QAAQ,IAAI,IAAI;QACvB,MAAM,QAAQ,IAAI;IACpB;IAEA,OAAO;QACL,SAAS,SAAS,OAAO,CAAC,EAAE,EAAE,SAAS,WAAW;QAClD,UAAU;QACV,cAAc;QACd,OAAO;IACT;AACF;AAKO,eAAe,qBACpB,OAA6B;IAE7B,MAAM,SAAS;IAEf,MAAM,WAAW,MAAM,OAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC;QACpD,OAAO;QACP,UAAU,QAAQ,QAAQ,CAAC,GAAG,CAAC,CAAC,IAAM,CAAC;gBACrC,MAAM,EAAE,IAAI;gBACZ,SAAS,EAAE,OAAO;YACpB,CAAC;QACD,aAAa,QAAQ,WAAW,IAAI;QACpC,YAAY,QAAQ,SAAS,IAAI;QACjC,OAAO,QAAQ,IAAI,IAAI;QACvB,MAAM,QAAQ,IAAI;QAClB,QAAQ;IACV;IAEA,4CAA4C;IAC5C,MAAM,SAAS,sBAAsB;IAErC,OAAO;QACL;QACA,UAAU;QACV,cAAc;QACd,OAAO;IACT;AACF;AAEA;;CAEC,GACD,SAAS,sBACP,YAAwE;IAExE,MAAM,UAAU,IAAI;IAEpB,OAAO,IAAI,eAAe;QACxB,MAAM,OAAM,UAAU;YACpB,IAAI;gBACF,WAAW,MAAM,SAAS,aAAc;oBACtC,MAAM,UAAU,MAAM,OAAO,CAAC,EAAE,EAAE,OAAO;oBACzC,IAAI,SAAS;wBACX,MAAM,UAAU,CAAC,MAAM,EAAE,KAAK,SAAS,CAAC;4BAAE;wBAAQ,GAAG,IAAI,CAAC;wBAC1D,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;oBACpC;oBAEA,mCAAmC;oBACnC,IAAI,MAAM,OAAO,CAAC,EAAE,EAAE,eAAe;wBACnC,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;oBACpC;gBACF;gBACA,WAAW,KAAK;YAClB,EAAE,OAAO,OAAO;gBACd,WAAW,KAAK,CAAC;YACnB;QACF;IACF;AACF"}},
    {"offset": {"line": 846, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/llm/client.ts"],"sourcesContent":["/**\n * Unified LLM Client\n *\n * Provides a unified interface for LLM operations with automatic\n * fallback from Ollama to OpenAI when Ollama is unavailable.\n */\n\nimport type {\n  LLMCompletionOptions,\n  LLMResponse,\n  LLMStreamResponse,\n  ModelType,\n} from \"./index\";\nimport {\n  checkOllamaHealth,\n  ollamaComplete,\n  ollamaStreamComplete,\n  type OllamaModelType,\n} from \"./ollama-provider\";\nimport {\n  isOpenAIConfigured,\n  openaiComplete,\n  openaiStreamComplete,\n} from \"./openai-provider\";\nimport { getOrFetch } from \"@/lib/cache\";\nimport { CACHE_KEYS, CACHE_TTL } from \"@/lib/redis\";\n\n/**\n * Check LLM health with caching\n */\nexport async function checkLLMHealth(): Promise<{\n  ollamaAvailable: boolean;\n  openaiConfigured: boolean;\n}> {\n  // Always check fresh status (no caching) to handle \"resetting\" scenarios\n  const ollamaAvailable = await checkOllamaHealth();\n\n  return {\n    ollamaAvailable,\n    openaiConfigured: isOpenAIConfigured(),\n  };\n}\n\n/**\n * Non-streaming completion with automatic fallback\n */\nexport async function complete(\n  options: LLMCompletionOptions,\n  modelType: ModelType = \"general\"\n): Promise<LLMResponse> {\n  // Check Ollama health (cached)\n  const health = await checkLLMHealth();\n\n  // Try Ollama first if available\n  if (health.ollamaAvailable) {\n    try {\n      console.log(\"[LLM] Attempting Ollama completion...\");\n      const response = await ollamaComplete(\n        options,\n        modelType as OllamaModelType\n      );\n      console.log(\"[LLM] Ollama completion successful\");\n      return response;\n    } catch (error) {\n      console.warn(\"[LLM] Ollama failed, checking fallback:\", error);\n    }\n  }\n\n  // Fallback to OpenAI\n  if (health.openaiConfigured) {\n    console.log(\"[LLM] Falling back to OpenAI...\");\n    const response = await openaiComplete(options);\n    console.log(\"[LLM] OpenAI completion successful\");\n    return response;\n  }\n\n  throw new Error(\n    \"No LLM provider available. Ollama is not running and OpenAI is not configured.\"\n  );\n}\n\n/**\n * Streaming completion with automatic fallback\n */\nexport async function streamComplete(\n  options: LLMCompletionOptions,\n  modelType: ModelType = \"general\"\n): Promise<LLMStreamResponse> {\n  // Check Ollama health (cached)\n  const health = await checkLLMHealth();\n\n  // Try Ollama first if available\n  if (health.ollamaAvailable) {\n    try {\n      console.log(\"[LLM] Attempting Ollama streaming...\");\n      const response = await ollamaStreamComplete(\n        options,\n        modelType as OllamaModelType\n      );\n      console.log(\"[LLM] Ollama streaming started\");\n      return response;\n    } catch (error) {\n      console.warn(\"[LLM] Ollama streaming failed, checking fallback:\", error);\n    }\n  }\n\n  // Fallback to OpenAI\n  if (health.openaiConfigured) {\n    console.log(\"[LLM] Falling back to OpenAI streaming...\");\n    const response = await openaiStreamComplete(options);\n    console.log(\"[LLM] OpenAI streaming started\");\n    return response;\n  }\n\n  throw new Error(\n    \"No LLM provider available. Ollama is not running and OpenAI is not configured.\"\n  );\n}\n"],"names":[],"mappings":"AAAA;;;;;CAKC;;;;;;;;AAQD;AAMA;;;AAWO,eAAe;IAIpB,yEAAyE;IACzE,MAAM,kBAAkB,MAAM,IAAA,uOAAiB;IAE/C,OAAO;QACL;QACA,kBAAkB,IAAA,wOAAkB;IACtC;AACF;AAKO,eAAe,SACpB,OAA6B,EAC7B,YAAuB,SAAS;IAEhC,+BAA+B;IAC/B,MAAM,SAAS,MAAM;IAErB,gCAAgC;IAChC,IAAI,OAAO,eAAe,EAAE;QAC1B,IAAI;YACF,QAAQ,GAAG,CAAC;YACZ,MAAM,WAAW,MAAM,IAAA,oOAAc,EACnC,SACA;YAEF,QAAQ,GAAG,CAAC;YACZ,OAAO;QACT,EAAE,OAAO,OAAO;YACd,QAAQ,IAAI,CAAC,2CAA2C;QAC1D;IACF;IAEA,qBAAqB;IACrB,IAAI,OAAO,gBAAgB,EAAE;QAC3B,QAAQ,GAAG,CAAC;QACZ,MAAM,WAAW,MAAM,IAAA,oOAAc,EAAC;QACtC,QAAQ,GAAG,CAAC;QACZ,OAAO;IACT;IAEA,MAAM,IAAI,MACR;AAEJ;AAKO,eAAe,eACpB,OAA6B,EAC7B,YAAuB,SAAS;IAEhC,+BAA+B;IAC/B,MAAM,SAAS,MAAM;IAErB,gCAAgC;IAChC,IAAI,OAAO,eAAe,EAAE;QAC1B,IAAI;YACF,QAAQ,GAAG,CAAC;YACZ,MAAM,WAAW,MAAM,IAAA,0OAAoB,EACzC,SACA;YAEF,QAAQ,GAAG,CAAC;YACZ,OAAO;QACT,EAAE,OAAO,OAAO;YACd,QAAQ,IAAI,CAAC,qDAAqD;QACpE;IACF;IAEA,qBAAqB;IACrB,IAAI,OAAO,gBAAgB,EAAE;QAC3B,QAAQ,GAAG,CAAC;QACZ,MAAM,WAAW,MAAM,IAAA,0OAAoB,EAAC;QAC5C,QAAQ,GAAG,CAAC;QACZ,OAAO;IACT;IAEA,MAAM,IAAI,MACR;AAEJ"}},
    {"offset": {"line": 921, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/llm/index.ts"],"sourcesContent":["/**\n * LLM Types and Interfaces\n *\n * Unified type definitions for LLM interactions.\n */\n\nexport interface LLMMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\nexport interface LLMCompletionOptions {\n  messages: LLMMessage[];\n  temperature?: number;\n  maxTokens?: number;\n  topP?: number;\n  stop?: string[];\n  stream?: boolean;\n}\n\nexport interface LLMResponse {\n  content: string;\n  provider: \"ollama\" | \"openai\";\n  fallbackUsed: boolean;\n  model: string;\n}\n\nexport interface LLMStreamResponse {\n  stream: ReadableStream<Uint8Array>;\n  provider: \"ollama\" | \"openai\";\n  fallbackUsed: boolean;\n  model: string;\n}\n\nexport type ModelType = \"general\" | \"coding\";\n\n// Re-export client functions\nexport { complete, streamComplete, checkLLMHealth } from \"./client\";\n"],"names":[],"mappings":"AAAA;;;;CAIC;AAgCD,6BAA6B;AAC7B"}},
    {"offset": {"line": 933, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/difficulty-calculator.ts"],"sourcesContent":["/**\n * Smart Difficulty Calculator\n *\n * Calculates the actual interview difficulty based on user profile,\n * skills, experience, target companies, and resume analysis.\n *\n * Weightages:\n * - Experience Level: 30%\n * - Skills Match: 25%\n * - Target Companies: 20%\n * - Resume Analysis: 15%\n * - Selected Difficulty: 10%\n */\n\nexport type DifficultyLevel = \"easy\" | \"medium\" | \"hard\";\n\nexport interface UserProfileData {\n  yearsExperience?: number | null;\n  skills?: string[];\n  targetCompanies?: string[];\n  targetRole?: string | null;\n  bio?: string | null;\n}\n\nexport interface ResumeData {\n  content?: string | null;\n  atsScore?: number | null;\n  keywords?: string[];\n  predictedRoles?: string[];\n  analysis?: {\n    experienceLevel?: string;\n    technicalDepth?: string;\n    systemDesignExperience?: string;\n  } | null;\n}\n\nexport interface DifficultyInput {\n  selectedDifficulty: DifficultyLevel;\n  profile: UserProfileData | null;\n  resume: ResumeData | null;\n  topic: string;\n}\n\nexport interface DifficultyResult {\n  calculatedDifficulty: DifficultyLevel;\n  confidenceScore: number;\n  adjustmentReason: string;\n  breakdown: {\n    experienceScore: number;\n    skillsScore: number;\n    companiesScore: number;\n    resumeScore: number;\n    selectedScore: number;\n    totalScore: number;\n  };\n  recommendations: string[];\n}\n\n// Weightages for each factor\nconst WEIGHTS = {\n  experience: 0.30,\n  skills: 0.25,\n  companies: 0.20,\n  resume: 0.15,\n  selected: 0.10,\n};\n\n// FAANG and top-tier companies (require higher difficulty)\nconst TOP_TIER_COMPANIES = [\n  \"google\", \"meta\", \"facebook\", \"amazon\", \"apple\", \"microsoft\", \"netflix\",\n  \"uber\", \"airbnb\", \"stripe\", \"coinbase\", \"linkedin\", \"twitter\", \"x\",\n  \"openai\", \"anthropic\", \"databricks\", \"snowflake\", \"palantir\", \"nvidia\",\n  \"tesla\", \"spacex\", \"bloomberg\", \"citadel\", \"two sigma\", \"jane street\",\n  \"bytedance\", \"tiktok\", \"spotify\", \"dropbox\", \"slack\", \"salesforce\"\n];\n\n// System design related skills\nconst SYSTEM_DESIGN_SKILLS = [\n  \"system design\", \"distributed systems\", \"microservices\", \"kubernetes\", \"docker\",\n  \"aws\", \"gcp\", \"azure\", \"kafka\", \"redis\", \"elasticsearch\", \"mongodb\",\n  \"postgresql\", \"mysql\", \"cassandra\", \"dynamodb\", \"graphql\", \"rest api\",\n  \"load balancing\", \"caching\", \"sharding\", \"replication\", \"cap theorem\",\n  \"event-driven\", \"message queues\", \"rabbitmq\", \"grpc\", \"websockets\",\n  \"cdn\", \"api gateway\", \"service mesh\", \"istio\", \"terraform\", \"ci/cd\",\n  \"scalability\", \"high availability\", \"fault tolerance\", \"database design\"\n];\n\n/**\n * Calculate experience score (0-100)\n * 0-2 years: Beginner (20-40)\n * 3-5 years: Intermediate (40-60)\n * 6-10 years: Senior (60-80)\n * 10+ years: Expert (80-100)\n */\nfunction calculateExperienceScore(yearsExperience?: number | null): number {\n  if (!yearsExperience || yearsExperience <= 0) return 20;\n\n  if (yearsExperience <= 2) {\n    return 20 + (yearsExperience / 2) * 20; // 20-40\n  } else if (yearsExperience <= 5) {\n    return 40 + ((yearsExperience - 2) / 3) * 20; // 40-60\n  } else if (yearsExperience <= 10) {\n    return 60 + ((yearsExperience - 5) / 5) * 20; // 60-80\n  } else {\n    return Math.min(100, 80 + ((yearsExperience - 10) / 10) * 20); // 80-100\n  }\n}\n\n/**\n * Calculate skills score based on system design relevant skills (0-100)\n */\nfunction calculateSkillsScore(skills?: string[]): number {\n  if (!skills || skills.length === 0) return 30;\n\n  const normalizedSkills = skills.map(s => s.toLowerCase().trim());\n\n  let matchCount = 0;\n  let totalRelevance = 0;\n\n  for (const skill of normalizedSkills) {\n    for (const sdSkill of SYSTEM_DESIGN_SKILLS) {\n      if (skill.includes(sdSkill) || sdSkill.includes(skill)) {\n        matchCount++;\n        // Some skills are more advanced\n        if ([\"distributed systems\", \"cap theorem\", \"sharding\", \"service mesh\", \"event-driven\"].some(s => skill.includes(s))) {\n          totalRelevance += 15;\n        } else if ([\"kubernetes\", \"kafka\", \"elasticsearch\", \"microservices\"].some(s => skill.includes(s))) {\n          totalRelevance += 12;\n        } else {\n          totalRelevance += 8;\n        }\n        break;\n      }\n    }\n  }\n\n  // Base score from match count\n  const matchScore = Math.min(50, matchCount * 10);\n  // Relevance bonus\n  const relevanceScore = Math.min(50, totalRelevance);\n\n  return Math.min(100, matchScore + relevanceScore);\n}\n\n/**\n * Calculate company tier score (0-100)\n * Top-tier companies expect higher difficulty\n */\nfunction calculateCompaniesScore(targetCompanies?: string[]): number {\n  if (!targetCompanies || targetCompanies.length === 0) return 50;\n\n  const normalizedCompanies = targetCompanies.map(c => c.toLowerCase().trim());\n\n  let topTierCount = 0;\n  for (const company of normalizedCompanies) {\n    if (TOP_TIER_COMPANIES.some(tc => company.includes(tc) || tc.includes(company))) {\n      topTierCount++;\n    }\n  }\n\n  const topTierRatio = topTierCount / targetCompanies.length;\n\n  // If mostly top-tier companies, expect higher difficulty\n  if (topTierRatio >= 0.7) return 85;\n  if (topTierRatio >= 0.5) return 70;\n  if (topTierRatio >= 0.3) return 55;\n  if (topTierRatio > 0) return 45;\n\n  return 40;\n}\n\n/**\n * Calculate resume score based on ATS analysis (0-100)\n */\nfunction calculateResumeScore(resume?: ResumeData | null): number {\n  if (!resume) return 50;\n\n  let score = 50;\n\n  // ATS score contribution\n  if (resume.atsScore) {\n    score = resume.atsScore * 0.5;\n  }\n\n  // Keywords analysis\n  if (resume.keywords && resume.keywords.length > 0) {\n    const normalizedKeywords = resume.keywords.map(k => k.toLowerCase());\n    let relevantKeywords = 0;\n\n    for (const keyword of normalizedKeywords) {\n      if (SYSTEM_DESIGN_SKILLS.some(s => keyword.includes(s) || s.includes(keyword))) {\n        relevantKeywords++;\n      }\n    }\n\n    score += Math.min(30, relevantKeywords * 5);\n  }\n\n  // Experience level from analysis\n  if (resume.analysis?.experienceLevel) {\n    const level = resume.analysis.experienceLevel.toLowerCase();\n    if (level.includes(\"senior\") || level.includes(\"lead\") || level.includes(\"principal\")) {\n      score += 20;\n    } else if (level.includes(\"mid\") || level.includes(\"intermediate\")) {\n      score += 10;\n    }\n  }\n\n  return Math.min(100, score);\n}\n\n/**\n * Convert selected difficulty to score (0-100)\n */\nfunction selectedDifficultyToScore(difficulty: DifficultyLevel): number {\n  switch (difficulty) {\n    case \"easy\": return 33;\n    case \"medium\": return 66;\n    case \"hard\": return 100;\n    default: return 50;\n  }\n}\n\n/**\n * Convert total score to difficulty level\n */\nfunction scoreToDifficulty(score: number): DifficultyLevel {\n  if (score < 40) return \"easy\";\n  if (score < 70) return \"medium\";\n  return \"hard\";\n}\n\n/**\n * Generate adjustment reason based on the calculation\n */\nfunction generateAdjustmentReason(\n  selected: DifficultyLevel,\n  calculated: DifficultyLevel,\n  breakdown: DifficultyResult[\"breakdown\"]\n): string {\n  if (selected === calculated) {\n    return \"Your selected difficulty matches your profile. No adjustment needed.\";\n  }\n\n  const reasons: string[] = [];\n\n  if (selected === \"hard\" && calculated !== \"hard\") {\n    if (breakdown.experienceScore < 50) {\n      reasons.push(\"your experience level suggests building more foundation first\");\n    }\n    if (breakdown.skillsScore < 50) {\n      reasons.push(\"developing more system design skills would help\");\n    }\n    if (breakdown.resumeScore < 50) {\n      reasons.push(\"your resume indicates room for technical growth\");\n    }\n  } else if (selected === \"easy\" && calculated !== \"easy\") {\n    if (breakdown.experienceScore > 60) {\n      reasons.push(\"your experience qualifies you for more challenge\");\n    }\n    if (breakdown.skillsScore > 60) {\n      reasons.push(\"your skills indicate strong system design knowledge\");\n    }\n    if (breakdown.companiesScore > 70) {\n      reasons.push(\"your target companies have rigorous interviews\");\n    }\n  }\n\n  if (reasons.length === 0) {\n    return `Adjusted from ${selected} to ${calculated} based on your overall profile analysis.`;\n  }\n\n  return `Adjusted from ${selected} to ${calculated} because ${reasons.join(\" and \")}.`;\n}\n\n/**\n * Generate recommendations based on profile analysis\n */\nfunction generateRecommendations(\n  breakdown: DifficultyResult[\"breakdown\"],\n  profile: UserProfileData | null\n): string[] {\n  const recommendations: string[] = [];\n\n  if (breakdown.experienceScore < 40) {\n    recommendations.push(\"Focus on building practical experience with distributed systems\");\n  }\n\n  if (breakdown.skillsScore < 50) {\n    recommendations.push(\"Study core system design concepts: caching, sharding, load balancing\");\n    recommendations.push(\"Practice designing systems like URL shortener, rate limiter, chat app\");\n  }\n\n  if (breakdown.companiesScore > 70 && breakdown.totalScore < 60) {\n    recommendations.push(\"Your target companies have high bars - consider more preparation\");\n    recommendations.push(\"Review system design case studies from FAANG interviews\");\n  }\n\n  if (breakdown.resumeScore < 50) {\n    recommendations.push(\"Update your resume to highlight system design experience\");\n  }\n\n  if (!profile?.skills || profile.skills.length < 3) {\n    recommendations.push(\"Add more skills to your profile for better question personalization\");\n  }\n\n  if (recommendations.length === 0) {\n    recommendations.push(\"You're well-prepared! Focus on articulating your thoughts clearly\");\n  }\n\n  return recommendations.slice(0, 4);\n}\n\n/**\n * Main function: Calculate smart difficulty\n */\nexport function calculateSmartDifficulty(input: DifficultyInput): DifficultyResult {\n  const { selectedDifficulty, profile, resume } = input;\n\n  // Calculate individual scores\n  const experienceScore = calculateExperienceScore(profile?.yearsExperience);\n  const skillsScore = calculateSkillsScore(profile?.skills);\n  const companiesScore = calculateCompaniesScore(profile?.targetCompanies);\n  const resumeScore = calculateResumeScore(resume);\n  const selectedScore = selectedDifficultyToScore(selectedDifficulty);\n\n  // Calculate weighted total\n  const totalScore =\n    experienceScore * WEIGHTS.experience +\n    skillsScore * WEIGHTS.skills +\n    companiesScore * WEIGHTS.companies +\n    resumeScore * WEIGHTS.resume +\n    selectedScore * WEIGHTS.selected;\n\n  const breakdown = {\n    experienceScore: Math.round(experienceScore),\n    skillsScore: Math.round(skillsScore),\n    companiesScore: Math.round(companiesScore),\n    resumeScore: Math.round(resumeScore),\n    selectedScore: Math.round(selectedScore),\n    totalScore: Math.round(totalScore),\n  };\n\n  const calculatedDifficulty = scoreToDifficulty(totalScore);\n\n  // Confidence score based on how much data we have\n  let confidenceScore = 50;\n  if (profile?.yearsExperience) confidenceScore += 15;\n  if (profile?.skills && profile.skills.length > 0) confidenceScore += 15;\n  if (profile?.targetCompanies && profile.targetCompanies.length > 0) confidenceScore += 10;\n  if (resume?.atsScore) confidenceScore += 10;\n\n  return {\n    calculatedDifficulty,\n    confidenceScore: Math.min(100, confidenceScore),\n    adjustmentReason: generateAdjustmentReason(selectedDifficulty, calculatedDifficulty, breakdown),\n    breakdown,\n    recommendations: generateRecommendations(breakdown, profile),\n  };\n}\n\n/**\n * Get difficulty description for prompts\n */\nexport function getDifficultyDescription(difficulty: DifficultyLevel): string {\n  switch (difficulty) {\n    case \"easy\":\n      return \"Entry-level: Focus on fundamental concepts, provide hints when stuck, allow more time for thinking, cover basic system design patterns.\";\n    case \"medium\":\n      return \"Mid-level: Balance guidance with challenge, expect knowledge of common patterns, probe on trade-offs, moderate depth on scalability.\";\n    case \"hard\":\n      return \"Senior-level: Minimal hints, expect precise technical answers, deep-dive on edge cases, challenge all design decisions, cover advanced patterns.\";\n    default:\n      return \"Standard difficulty level.\";\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;CAYC;;;;;;AA8CD,6BAA6B;AAC7B,MAAM,UAAU;IACd,YAAY;IACZ,QAAQ;IACR,WAAW;IACX,QAAQ;IACR,UAAU;AACZ;AAEA,2DAA2D;AAC3D,MAAM,qBAAqB;IACzB;IAAU;IAAQ;IAAY;IAAU;IAAS;IAAa;IAC9D;IAAQ;IAAU;IAAU;IAAY;IAAY;IAAW;IAC/D;IAAU;IAAa;IAAc;IAAa;IAAY;IAC9D;IAAS;IAAU;IAAa;IAAW;IAAa;IACxD;IAAa;IAAU;IAAW;IAAW;IAAS;CACvD;AAED,+BAA+B;AAC/B,MAAM,uBAAuB;IAC3B;IAAiB;IAAuB;IAAiB;IAAc;IACvE;IAAO;IAAO;IAAS;IAAS;IAAS;IAAiB;IAC1D;IAAc;IAAS;IAAa;IAAY;IAAW;IAC3D;IAAkB;IAAW;IAAY;IAAe;IACxD;IAAgB;IAAkB;IAAY;IAAQ;IACtD;IAAO;IAAe;IAAgB;IAAS;IAAa;IAC5D;IAAe;IAAqB;IAAmB;CACxD;AAED;;;;;;CAMC,GACD,SAAS,yBAAyB,eAA+B;IAC/D,IAAI,CAAC,mBAAmB,mBAAmB,GAAG,OAAO;IAErD,IAAI,mBAAmB,GAAG;QACxB,OAAO,KAAK,AAAC,kBAAkB,IAAK,IAAI,QAAQ;IAClD,OAAO,IAAI,mBAAmB,GAAG;QAC/B,OAAO,KAAK,AAAC,CAAC,kBAAkB,CAAC,IAAI,IAAK,IAAI,QAAQ;IACxD,OAAO,IAAI,mBAAmB,IAAI;QAChC,OAAO,KAAK,AAAC,CAAC,kBAAkB,CAAC,IAAI,IAAK,IAAI,QAAQ;IACxD,OAAO;QACL,OAAO,KAAK,GAAG,CAAC,KAAK,KAAK,AAAC,CAAC,kBAAkB,EAAE,IAAI,KAAM,KAAK,SAAS;IAC1E;AACF;AAEA;;CAEC,GACD,SAAS,qBAAqB,MAAiB;IAC7C,IAAI,CAAC,UAAU,OAAO,MAAM,KAAK,GAAG,OAAO;IAE3C,MAAM,mBAAmB,OAAO,GAAG,CAAC,CAAA,IAAK,EAAE,WAAW,GAAG,IAAI;IAE7D,IAAI,aAAa;IACjB,IAAI,iBAAiB;IAErB,KAAK,MAAM,SAAS,iBAAkB;QACpC,KAAK,MAAM,WAAW,qBAAsB;YAC1C,IAAI,MAAM,QAAQ,CAAC,YAAY,QAAQ,QAAQ,CAAC,QAAQ;gBACtD;gBACA,gCAAgC;gBAChC,IAAI;oBAAC;oBAAuB;oBAAe;oBAAY;oBAAgB;iBAAe,CAAC,IAAI,CAAC,CAAA,IAAK,MAAM,QAAQ,CAAC,KAAK;oBACnH,kBAAkB;gBACpB,OAAO,IAAI;oBAAC;oBAAc;oBAAS;oBAAiB;iBAAgB,CAAC,IAAI,CAAC,CAAA,IAAK,MAAM,QAAQ,CAAC,KAAK;oBACjG,kBAAkB;gBACpB,OAAO;oBACL,kBAAkB;gBACpB;gBACA;YACF;QACF;IACF;IAEA,8BAA8B;IAC9B,MAAM,aAAa,KAAK,GAAG,CAAC,IAAI,aAAa;IAC7C,kBAAkB;IAClB,MAAM,iBAAiB,KAAK,GAAG,CAAC,IAAI;IAEpC,OAAO,KAAK,GAAG,CAAC,KAAK,aAAa;AACpC;AAEA;;;CAGC,GACD,SAAS,wBAAwB,eAA0B;IACzD,IAAI,CAAC,mBAAmB,gBAAgB,MAAM,KAAK,GAAG,OAAO;IAE7D,MAAM,sBAAsB,gBAAgB,GAAG,CAAC,CAAA,IAAK,EAAE,WAAW,GAAG,IAAI;IAEzE,IAAI,eAAe;IACnB,KAAK,MAAM,WAAW,oBAAqB;QACzC,IAAI,mBAAmB,IAAI,CAAC,CAAA,KAAM,QAAQ,QAAQ,CAAC,OAAO,GAAG,QAAQ,CAAC,WAAW;YAC/E;QACF;IACF;IAEA,MAAM,eAAe,eAAe,gBAAgB,MAAM;IAE1D,yDAAyD;IACzD,IAAI,gBAAgB,KAAK,OAAO;IAChC,IAAI,gBAAgB,KAAK,OAAO;IAChC,IAAI,gBAAgB,KAAK,OAAO;IAChC,IAAI,eAAe,GAAG,OAAO;IAE7B,OAAO;AACT;AAEA;;CAEC,GACD,SAAS,qBAAqB,MAA0B;IACtD,IAAI,CAAC,QAAQ,OAAO;IAEpB,IAAI,QAAQ;IAEZ,yBAAyB;IACzB,IAAI,OAAO,QAAQ,EAAE;QACnB,QAAQ,OAAO,QAAQ,GAAG;IAC5B;IAEA,oBAAoB;IACpB,IAAI,OAAO,QAAQ,IAAI,OAAO,QAAQ,CAAC,MAAM,GAAG,GAAG;QACjD,MAAM,qBAAqB,OAAO,QAAQ,CAAC,GAAG,CAAC,CAAA,IAAK,EAAE,WAAW;QACjE,IAAI,mBAAmB;QAEvB,KAAK,MAAM,WAAW,mBAAoB;YACxC,IAAI,qBAAqB,IAAI,CAAC,CAAA,IAAK,QAAQ,QAAQ,CAAC,MAAM,EAAE,QAAQ,CAAC,WAAW;gBAC9E;YACF;QACF;QAEA,SAAS,KAAK,GAAG,CAAC,IAAI,mBAAmB;IAC3C;IAEA,iCAAiC;IACjC,IAAI,OAAO,QAAQ,EAAE,iBAAiB;QACpC,MAAM,QAAQ,OAAO,QAAQ,CAAC,eAAe,CAAC,WAAW;QACzD,IAAI,MAAM,QAAQ,CAAC,aAAa,MAAM,QAAQ,CAAC,WAAW,MAAM,QAAQ,CAAC,cAAc;YACrF,SAAS;QACX,OAAO,IAAI,MAAM,QAAQ,CAAC,UAAU,MAAM,QAAQ,CAAC,iBAAiB;YAClE,SAAS;QACX;IACF;IAEA,OAAO,KAAK,GAAG,CAAC,KAAK;AACvB;AAEA;;CAEC,GACD,SAAS,0BAA0B,UAA2B;IAC5D,OAAQ;QACN,KAAK;YAAQ,OAAO;QACpB,KAAK;YAAU,OAAO;QACtB,KAAK;YAAQ,OAAO;QACpB;YAAS,OAAO;IAClB;AACF;AAEA;;CAEC,GACD,SAAS,kBAAkB,KAAa;IACtC,IAAI,QAAQ,IAAI,OAAO;IACvB,IAAI,QAAQ,IAAI,OAAO;IACvB,OAAO;AACT;AAEA;;CAEC,GACD,SAAS,yBACP,QAAyB,EACzB,UAA2B,EAC3B,SAAwC;IAExC,IAAI,aAAa,YAAY;QAC3B,OAAO;IACT;IAEA,MAAM,UAAoB,EAAE;IAE5B,IAAI,aAAa,UAAU,eAAe,QAAQ;QAChD,IAAI,UAAU,eAAe,GAAG,IAAI;YAClC,QAAQ,IAAI,CAAC;QACf;QACA,IAAI,UAAU,WAAW,GAAG,IAAI;YAC9B,QAAQ,IAAI,CAAC;QACf;QACA,IAAI,UAAU,WAAW,GAAG,IAAI;YAC9B,QAAQ,IAAI,CAAC;QACf;IACF,OAAO,IAAI,aAAa,UAAU,eAAe,QAAQ;QACvD,IAAI,UAAU,eAAe,GAAG,IAAI;YAClC,QAAQ,IAAI,CAAC;QACf;QACA,IAAI,UAAU,WAAW,GAAG,IAAI;YAC9B,QAAQ,IAAI,CAAC;QACf;QACA,IAAI,UAAU,cAAc,GAAG,IAAI;YACjC,QAAQ,IAAI,CAAC;QACf;IACF;IAEA,IAAI,QAAQ,MAAM,KAAK,GAAG;QACxB,OAAO,CAAC,cAAc,EAAE,SAAS,IAAI,EAAE,WAAW,wCAAwC,CAAC;IAC7F;IAEA,OAAO,CAAC,cAAc,EAAE,SAAS,IAAI,EAAE,WAAW,SAAS,EAAE,QAAQ,IAAI,CAAC,SAAS,CAAC,CAAC;AACvF;AAEA;;CAEC,GACD,SAAS,wBACP,SAAwC,EACxC,OAA+B;IAE/B,MAAM,kBAA4B,EAAE;IAEpC,IAAI,UAAU,eAAe,GAAG,IAAI;QAClC,gBAAgB,IAAI,CAAC;IACvB;IAEA,IAAI,UAAU,WAAW,GAAG,IAAI;QAC9B,gBAAgB,IAAI,CAAC;QACrB,gBAAgB,IAAI,CAAC;IACvB;IAEA,IAAI,UAAU,cAAc,GAAG,MAAM,UAAU,UAAU,GAAG,IAAI;QAC9D,gBAAgB,IAAI,CAAC;QACrB,gBAAgB,IAAI,CAAC;IACvB;IAEA,IAAI,UAAU,WAAW,GAAG,IAAI;QAC9B,gBAAgB,IAAI,CAAC;IACvB;IAEA,IAAI,CAAC,SAAS,UAAU,QAAQ,MAAM,CAAC,MAAM,GAAG,GAAG;QACjD,gBAAgB,IAAI,CAAC;IACvB;IAEA,IAAI,gBAAgB,MAAM,KAAK,GAAG;QAChC,gBAAgB,IAAI,CAAC;IACvB;IAEA,OAAO,gBAAgB,KAAK,CAAC,GAAG;AAClC;AAKO,SAAS,yBAAyB,KAAsB;IAC7D,MAAM,EAAE,kBAAkB,EAAE,OAAO,EAAE,MAAM,EAAE,GAAG;IAEhD,8BAA8B;IAC9B,MAAM,kBAAkB,yBAAyB,SAAS;IAC1D,MAAM,cAAc,qBAAqB,SAAS;IAClD,MAAM,iBAAiB,wBAAwB,SAAS;IACxD,MAAM,cAAc,qBAAqB;IACzC,MAAM,gBAAgB,0BAA0B;IAEhD,2BAA2B;IAC3B,MAAM,aACJ,kBAAkB,QAAQ,UAAU,GACpC,cAAc,QAAQ,MAAM,GAC5B,iBAAiB,QAAQ,SAAS,GAClC,cAAc,QAAQ,MAAM,GAC5B,gBAAgB,QAAQ,QAAQ;IAElC,MAAM,YAAY;QAChB,iBAAiB,KAAK,KAAK,CAAC;QAC5B,aAAa,KAAK,KAAK,CAAC;QACxB,gBAAgB,KAAK,KAAK,CAAC;QAC3B,aAAa,KAAK,KAAK,CAAC;QACxB,eAAe,KAAK,KAAK,CAAC;QAC1B,YAAY,KAAK,KAAK,CAAC;IACzB;IAEA,MAAM,uBAAuB,kBAAkB;IAE/C,kDAAkD;IAClD,IAAI,kBAAkB;IACtB,IAAI,SAAS,iBAAiB,mBAAmB;IACjD,IAAI,SAAS,UAAU,QAAQ,MAAM,CAAC,MAAM,GAAG,GAAG,mBAAmB;IACrE,IAAI,SAAS,mBAAmB,QAAQ,eAAe,CAAC,MAAM,GAAG,GAAG,mBAAmB;IACvF,IAAI,QAAQ,UAAU,mBAAmB;IAEzC,OAAO;QACL;QACA,iBAAiB,KAAK,GAAG,CAAC,KAAK;QAC/B,kBAAkB,yBAAyB,oBAAoB,sBAAsB;QACrF;QACA,iBAAiB,wBAAwB,WAAW;IACtD;AACF;AAKO,SAAS,yBAAyB,UAA2B;IAClE,OAAQ;QACN,KAAK;YACH,OAAO;QACT,KAAK;YACH,OAAO;QACT,KAAK;YACH,OAAO;QACT;YACE,OAAO;IACX;AACF"}},
    {"offset": {"line": 1273, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/ollama.ts"],"sourcesContent":["/**\n * Ollama Client for Local LLM Inference\n *\n * Uses Ollama running locally for faster interview interactions.\n *\n * Models:\n * - llama3 (8B): Default for system design interviews\n * - codellama:7b: Specialized for coding challenges\n *\n * Make sure Ollama is running: ollama serve\n * Pull the models:\n *   ollama pull llama3\n *   ollama pull codellama:7b\n */\n\nconst OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || \"http://localhost:11434\";\nconst OLLAMA_MODEL = process.env.OLLAMA_MODEL || \"llama3:8b\";\nconst OLLAMA_CODE_MODEL = process.env.OLLAMA_CODE_MODEL || \"codellama:7b\";\n\n// Model types for different use cases\nexport type ModelType = \"general\" | \"coding\";\n\n// Get the appropriate model based on use case\nexport function getModelForType(type: ModelType): string {\n  switch (type) {\n    case \"coding\":\n      return OLLAMA_CODE_MODEL;\n    case \"general\":\n    default:\n      return OLLAMA_MODEL;\n  }\n}\n\nexport interface OllamaMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\nexport interface OllamaCompletionOptions {\n  model?: string;\n  messages: OllamaMessage[];\n  stream?: boolean;\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  stop?: string[];\n}\n\nexport interface OllamaResponse {\n  model: string;\n  created_at: string;\n  message: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  eval_count?: number;\n}\n\n/**\n * Check if Ollama is running and available\n */\nexport async function checkOllamaHealth(): Promise<boolean> {\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {\n      method: \"GET\",\n    });\n    return response.ok;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Check Ollama health with Redis caching\n */\nexport async function checkOllamaHealthCached(): Promise<boolean> {\n  const { getOrFetch } = await import(\"@/lib/cache\");\n  const { CACHE_KEYS, CACHE_TTL } = await import(\"@/lib/redis\");\n\n  return getOrFetch(\n    CACHE_KEYS.OLLAMA_HEALTH,\n    checkOllamaHealth,\n    CACHE_TTL.OLLAMA_HEALTH\n  );\n}\n\n/**\n * List available models\n */\nexport async function listModels(): Promise<string[]> {\n  try {\n    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`);\n    if (!response.ok) return [];\n    const data = await response.json();\n    return data.models?.map((m: { name: string }) => m.name) || [];\n  } catch {\n    return [];\n  }\n}\n\n/**\n * Non-streaming chat completion\n */\nexport async function ollamaChat(\n  options: OllamaCompletionOptions\n): Promise<OllamaResponse> {\n  const { model = OLLAMA_MODEL, messages, temperature = 0.7 } = options;\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages,\n      stream: false,\n      options: {\n        temperature,\n        num_predict: options.max_tokens || 800,\n        top_p: options.top_p || 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama error: ${error}`);\n  }\n\n  return response.json();\n}\n\n/**\n * Streaming chat completion - returns a ReadableStream\n */\nexport async function ollamaChatStream(\n  options: OllamaCompletionOptions\n): Promise<ReadableStream<Uint8Array>> {\n  const { model = OLLAMA_MODEL, messages, temperature = 0.7 } = options;\n\n  const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model,\n      messages,\n      stream: true,\n      options: {\n        temperature,\n        num_predict: options.max_tokens || 800,\n        top_p: options.top_p || 0.9,\n        stop: options.stop,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const error = await response.text();\n    throw new Error(`Ollama streaming error: ${error}`);\n  }\n\n  if (!response.body) {\n    throw new Error(\"No response body from Ollama\");\n  }\n\n  return response.body;\n}\n\n/**\n * Transform Ollama stream to SSE format for client consumption\n */\nexport function createOllamaSSEStream(\n  ollamaStream: ReadableStream<Uint8Array>\n): ReadableStream<Uint8Array> {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n\n  let buffer = \"\";\n\n  return new ReadableStream({\n    async start(controller) {\n      const reader = ollamaStream.getReader();\n\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n\n          if (done) {\n            controller.close();\n            break;\n          }\n\n          buffer += decoder.decode(value, { stream: true });\n\n          // Process complete JSON lines\n          const lines = buffer.split(\"\\n\");\n          buffer = lines.pop() || \"\";\n\n          for (const line of lines) {\n            if (line.trim()) {\n              try {\n                const json = JSON.parse(line);\n                if (json.message?.content) {\n                  // Send as SSE format\n                  const sseData = `data: ${JSON.stringify({ content: json.message.content })}\\n\\n`;\n                  controller.enqueue(encoder.encode(sseData));\n                }\n                if (json.done) {\n                  controller.enqueue(encoder.encode(\"data: [DONE]\\n\\n\"));\n                }\n              } catch {\n                // Skip invalid JSON lines\n              }\n            }\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n  });\n}\n\n/**\n * Simple wrapper for interview chat - handles both streaming and non-streaming\n */\nexport const ollama = {\n  chat: {\n    completions: {\n      create: async (options: {\n        model?: string;\n        messages: OllamaMessage[];\n        temperature?: number;\n        max_tokens?: number;\n        stream?: boolean;\n      }) => {\n        if (options.stream) {\n          const stream = await ollamaChatStream({\n            model: options.model || OLLAMA_MODEL,\n            messages: options.messages,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: true,\n          });\n          return { body: createOllamaSSEStream(stream) };\n        } else {\n          const response = await ollamaChat({\n            model: options.model || OLLAMA_MODEL,\n            messages: options.messages,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: false,\n          });\n          return {\n            choices: [\n              {\n                message: {\n                  role: response.message.role,\n                  content: response.message.content,\n                },\n              },\n            ],\n          };\n        }\n      },\n    },\n  },\n};\n\n/**\n * CodeLlama-specific chat for coding challenges\n * Uses codellama:7b model optimized for code generation and analysis\n */\nexport async function codeLlamaChat(\n  options: Omit<OllamaCompletionOptions, \"model\">\n): Promise<OllamaResponse> {\n  return ollamaChat({\n    ...options,\n    model: OLLAMA_CODE_MODEL,\n  });\n}\n\n/**\n * Check if CodeLlama model is available\n */\nexport async function checkCodeLlamaAvailable(): Promise<boolean> {\n  try {\n    const models = await listModels();\n    return models.some(\n      (m) => m.includes(\"codellama\") || m.includes(\"code-llama\")\n    );\n  } catch {\n    return false;\n  }\n}\n\nexport default ollama;\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAAA;;;;;;;;;;;;;CAaC,GAED,MAAM,kBAAkB,QAAQ,GAAG,CAAC,eAAe,IAAI;AACvD,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AACjD,MAAM,oBAAoB,QAAQ,GAAG,CAAC,iBAAiB,IAAI;AAMpD,SAAS,gBAAgB,IAAe;IAC7C,OAAQ;QACN,KAAK;YACH,OAAO;QACT,KAAK;QACL;YACE,OAAO;IACX;AACF;AAkCO,eAAe;IACpB,IAAI;QACF,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;YAC1D,QAAQ;QACV;QACA,OAAO,SAAS,EAAE;IACpB,EAAE,OAAM;QACN,OAAO;IACT;AACF;AAKO,eAAe;IACpB,MAAM,EAAE,UAAU,EAAE,GAAG;IACvB,MAAM,EAAE,UAAU,EAAE,SAAS,EAAE,GAAG;IAElC,OAAO,WACL,WAAW,aAAa,EACxB,mBACA,UAAU,aAAa;AAE3B;AAKO,eAAe;IACpB,IAAI;QACF,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC;QAC1D,IAAI,CAAC,SAAS,EAAE,EAAE,OAAO,EAAE;QAC3B,MAAM,OAAO,MAAM,SAAS,IAAI;QAChC,OAAO,KAAK,MAAM,EAAE,IAAI,CAAC,IAAwB,EAAE,IAAI,KAAK,EAAE;IAChE,EAAE,OAAM;QACN,OAAO,EAAE;IACX;AACF;AAKO,eAAe,WACpB,OAAgC;IAEhC,MAAM,EAAE,QAAQ,YAAY,EAAE,QAAQ,EAAE,cAAc,GAAG,EAAE,GAAG;IAE9D,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;QAC1D,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;QACA,MAAM,KAAK,SAAS,CAAC;YACnB;YACA;YACA,QAAQ;YACR,SAAS;gBACP;gBACA,aAAa,QAAQ,UAAU,IAAI;gBACnC,OAAO,QAAQ,KAAK,IAAI;gBACxB,MAAM,QAAQ,IAAI;YACpB;QACF;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;QACjC,MAAM,IAAI,MAAM,CAAC,cAAc,EAAE,OAAO;IAC1C;IAEA,OAAO,SAAS,IAAI;AACtB;AAKO,eAAe,iBACpB,OAAgC;IAEhC,MAAM,EAAE,QAAQ,YAAY,EAAE,QAAQ,EAAE,cAAc,GAAG,EAAE,GAAG;IAE9D,MAAM,WAAW,MAAM,MAAM,GAAG,gBAAgB,SAAS,CAAC,EAAE;QAC1D,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;QACA,MAAM,KAAK,SAAS,CAAC;YACnB;YACA;YACA,QAAQ;YACR,SAAS;gBACP;gBACA,aAAa,QAAQ,UAAU,IAAI;gBACnC,OAAO,QAAQ,KAAK,IAAI;gBACxB,MAAM,QAAQ,IAAI;YACpB;QACF;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;QACjC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,OAAO;IACpD;IAEA,IAAI,CAAC,SAAS,IAAI,EAAE;QAClB,MAAM,IAAI,MAAM;IAClB;IAEA,OAAO,SAAS,IAAI;AACtB;AAKO,SAAS,sBACd,YAAwC;IAExC,MAAM,UAAU,IAAI;IACpB,MAAM,UAAU,IAAI;IAEpB,IAAI,SAAS;IAEb,OAAO,IAAI,eAAe;QACxB,MAAM,OAAM,UAAU;YACpB,MAAM,SAAS,aAAa,SAAS;YAErC,IAAI;gBACF,MAAO,KAAM;oBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;oBAEzC,IAAI,MAAM;wBACR,WAAW,KAAK;wBAChB;oBACF;oBAEA,UAAU,QAAQ,MAAM,CAAC,OAAO;wBAAE,QAAQ;oBAAK;oBAE/C,8BAA8B;oBAC9B,MAAM,QAAQ,OAAO,KAAK,CAAC;oBAC3B,SAAS,MAAM,GAAG,MAAM;oBAExB,KAAK,MAAM,QAAQ,MAAO;wBACxB,IAAI,KAAK,IAAI,IAAI;4BACf,IAAI;gCACF,MAAM,OAAO,KAAK,KAAK,CAAC;gCACxB,IAAI,KAAK,OAAO,EAAE,SAAS;oCACzB,qBAAqB;oCACrB,MAAM,UAAU,CAAC,MAAM,EAAE,KAAK,SAAS,CAAC;wCAAE,SAAS,KAAK,OAAO,CAAC,OAAO;oCAAC,GAAG,IAAI,CAAC;oCAChF,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;gCACpC;gCACA,IAAI,KAAK,IAAI,EAAE;oCACb,WAAW,OAAO,CAAC,QAAQ,MAAM,CAAC;gCACpC;4BACF,EAAE,OAAM;4BACN,0BAA0B;4BAC5B;wBACF;oBACF;gBACF;YACF,EAAE,OAAO,OAAO;gBACd,WAAW,KAAK,CAAC;YACnB;QACF;IACF;AACF;AAKO,MAAM,SAAS;IACpB,MAAM;QACJ,aAAa;YACX,QAAQ,OAAO;gBAOb,IAAI,QAAQ,MAAM,EAAE;oBAClB,MAAM,SAAS,MAAM,iBAAiB;wBACpC,OAAO,QAAQ,KAAK,IAAI;wBACxB,UAAU,QAAQ,QAAQ;wBAC1B,aAAa,QAAQ,WAAW;wBAChC,YAAY,QAAQ,UAAU;wBAC9B,QAAQ;oBACV;oBACA,OAAO;wBAAE,MAAM,sBAAsB;oBAAQ;gBAC/C,OAAO;oBACL,MAAM,WAAW,MAAM,WAAW;wBAChC,OAAO,QAAQ,KAAK,IAAI;wBACxB,UAAU,QAAQ,QAAQ;wBAC1B,aAAa,QAAQ,WAAW;wBAChC,YAAY,QAAQ,UAAU;wBAC9B,QAAQ;oBACV;oBACA,OAAO;wBACL,SAAS;4BACP;gCACE,SAAS;oCACP,MAAM,SAAS,OAAO,CAAC,IAAI;oCAC3B,SAAS,SAAS,OAAO,CAAC,OAAO;gCACnC;4BACF;yBACD;oBACH;gBACF;YACF;QACF;IACF;AACF;AAMO,eAAe,cACpB,OAA+C;IAE/C,OAAO,WAAW;QAChB,GAAG,OAAO;QACV,OAAO;IACT;AACF;AAKO,eAAe;IACpB,IAAI;QACF,MAAM,SAAS,MAAM;QACrB,OAAO,OAAO,IAAI,CAChB,CAAC,IAAM,EAAE,QAAQ,CAAC,gBAAgB,EAAE,QAAQ,CAAC;IAEjD,EAAE,OAAM;QACN,OAAO;IACT;AACF;uCAEe"}},
    {"offset": {"line": 1503, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/question-generator.ts"],"sourcesContent":["/**\n * Personalized Question Generator\n *\n * Uses Ollama (local LLM) to generate interview questions tailored to user's profile,\n * skills, experience, and target companies.\n */\n\nimport { ollamaChat } from \"./ollama\";\nimport {\n  DifficultyLevel,\n  UserProfileData,\n  ResumeData,\n  getDifficultyDescription,\n} from \"./difficulty-calculator\";\nimport { FollowUpContext } from \"./followup-engine\";\n\nexport interface QuestionGeneratorInput {\n  topic: string;\n  difficulty: DifficultyLevel;\n  profile: UserProfileData | null;\n  resume: ResumeData | null;\n  followUpContext?: FollowUpContext;\n  userId?: string;\n}\n\nexport interface GeneratedQuestion {\n  id: string;\n  question: string;\n  category: \"requirements\" | \"high-level\" | \"deep-dive\" | \"scalability\" | \"trade-offs\";\n  difficulty: DifficultyLevel;\n  focusAreas: string[];\n  expectedTopics: string[];\n  timeAllocation: number; // minutes\n}\n\nexport interface PersonalizedQuestionSet {\n  topic: string;\n  difficulty: DifficultyLevel;\n  totalDuration: number;\n  questions: GeneratedQuestion[];\n  focusAreas: string[];\n  personalizationNotes: string;\n  interviewerContext: string;\n}\n\n/**\n * Build context string from user profile\n */\nfunction buildProfileContext(profile: UserProfileData | null): string {\n  if (!profile) return \"No profile data available.\";\n\n  const parts: string[] = [];\n\n  if (profile.yearsExperience) {\n    parts.push(`Years of experience: ${profile.yearsExperience}`);\n  }\n\n  if (profile.targetRole) {\n    parts.push(`Target role: ${profile.targetRole}`);\n  }\n\n  if (profile.skills && profile.skills.length > 0) {\n    parts.push(`Technical skills: ${profile.skills.join(\", \")}`);\n  }\n\n  if (profile.targetCompanies && profile.targetCompanies.length > 0) {\n    parts.push(`Target companies: ${profile.targetCompanies.join(\", \")}`);\n  }\n\n  if (profile.bio) {\n    parts.push(`Background: ${profile.bio}`);\n  }\n\n  return parts.length > 0 ? parts.join(\"\\n\") : \"Limited profile data available.\";\n}\n\n/**\n * Build context string from resume\n */\nfunction buildResumeContext(resume: ResumeData | null): string {\n  if (!resume) return \"No resume data available.\";\n\n  const parts: string[] = [];\n\n  if (resume.atsScore) {\n    parts.push(`ATS Score: ${resume.atsScore}/100`);\n  }\n\n  if (resume.keywords && resume.keywords.length > 0) {\n    parts.push(`Key technical keywords: ${resume.keywords.slice(0, 15).join(\", \")}`);\n  }\n\n  if (resume.predictedRoles && resume.predictedRoles.length > 0) {\n    parts.push(`Predicted suitable roles: ${resume.predictedRoles.join(\", \")}`);\n  }\n\n  if (resume.analysis) {\n    if (resume.analysis.experienceLevel) {\n      parts.push(`Experience level: ${resume.analysis.experienceLevel}`);\n    }\n    if (resume.analysis.technicalDepth) {\n      parts.push(`Technical depth: ${resume.analysis.technicalDepth}`);\n    }\n    if (resume.analysis.systemDesignExperience) {\n      parts.push(`System design experience: ${resume.analysis.systemDesignExperience}`);\n    }\n  }\n\n  return parts.length > 0 ? parts.join(\"\\n\") : \"Limited resume data available.\";\n}\n\n/**\n * Build weak areas context for follow-up questioning\n */\nfunction buildWeakAreasContext(followUpContext?: FollowUpContext): string {\n  if (!followUpContext || !followUpContext.hasHistory || followUpContext.weakPoints.length === 0) {\n    return \"\";\n  }\n\n  let context = `\n## Candidate's Past Performance (Focus Areas)\n\nThis candidate has completed ${followUpContext.totalPastInterviews} previous interview${followUpContext.totalPastInterviews === 1 ? \"\" : \"s\"}.\n\n**Weak Areas to Target:**\n`;\n\n  for (const wp of followUpContext.weakPoints.slice(0, 3)) {\n    context += `- **${wp.dimension}**: scored ${wp.lastScore}/4, trend: ${wp.trend}\n  - Probe deeper on: ${wp.concept}\n`;\n  }\n\n  if (followUpContext.conceptsToProbe.length > 0) {\n    context += `\n**Specific Concepts to Include:**\n${followUpContext.conceptsToProbe.map(c => `- ${c}`).join(\"\\n\")}\n`;\n  }\n\n  if (followUpContext.topicsToReinforce.length > 0) {\n    context += `\n**Related Topics With Low Performance:**\n${followUpContext.topicsToReinforce.join(\", \")}\n`;\n  }\n\n  context += `\n**Difficulty Adaptation:** ${followUpContext.adaptedDifficulty}\n${followUpContext.adaptedDifficulty === \"easier\" ? \"- Generate more foundational questions with clear guidance\" : \"\"}\n${followUpContext.adaptedDifficulty === \"harder\" ? \"- Generate challenging edge cases and advanced scenarios\" : \"\"}\n`;\n\n  return context;\n}\n\n/**\n * Generate the question generation prompt\n */\nfunction buildQuestionPrompt(input: QuestionGeneratorInput): string {\n  const { topic, difficulty, profile, resume, followUpContext } = input;\n\n  const profileContext = buildProfileContext(profile);\n  const resumeContext = buildResumeContext(resume);\n  const difficultyDesc = getDifficultyDescription(difficulty);\n  const weakAreasContext = buildWeakAreasContext(followUpContext);\n\n  return `You are an expert system design interview question generator for top tech companies (FAANG+).\n\nGenerate a personalized set of system design interview questions for the topic: \"${topic}\"\n\n## Candidate Profile\n${profileContext}\n\n## Resume Analysis\n${resumeContext}\n\n## Interview Parameters\n- Difficulty Level: ${difficulty.toUpperCase()}\n- ${difficultyDesc}\n- Total Interview Duration: 45 minutes\n${weakAreasContext}\n## Question Generation Guidelines\n\n1. **Personalization**: Tailor questions to the candidate's skills and experience level.\n   - If they have specific skills (e.g., Kafka, Redis), include questions that let them showcase these.\n   - If they're targeting specific companies, align question style with those companies' interview patterns.\n\n2. **Difficulty Calibration**:\n   - EASY: Focus on fundamentals, basic patterns, straightforward requirements\n   - MEDIUM: Include trade-off discussions, moderate scale, some edge cases\n   - HARD: Complex scenarios, extreme scale, failure modes, advanced optimizations\n\n3. **Coverage**: Questions should cover all phases:\n   - Requirements Clarification (8 min): Scope, scale, constraints\n   - High-Level Design (12 min): Architecture, components, data flow\n   - Deep Dive (12 min): Specific component design, data models\n   - Scalability (10 min): Growth, optimization, trade-offs\n   - Trade-offs (3 min): Summary, alternatives considered\n\n4. **Question Types to Include**:\n   - Clarifying questions the interviewer should ask\n   - Follow-up probes based on expected answers\n   - Edge cases and failure scenarios\n   - Scaling challenges specific to the system\n\n## Output Format\nReturn a JSON object with the following structure:\n{\n  \"questions\": [\n    {\n      \"id\": \"q1\",\n      \"question\": \"The main question text\",\n      \"category\": \"requirements|high-level|deep-dive|scalability|trade-offs\",\n      \"difficulty\": \"easy|medium|hard\",\n      \"focusAreas\": [\"area1\", \"area2\"],\n      \"expectedTopics\": [\"topic candidate should mention\"],\n      \"timeAllocation\": 5\n    }\n  ],\n  \"focusAreas\": [\"Overall focus areas based on candidate profile\"],\n  \"personalizationNotes\": \"How questions were personalized for this candidate\",\n  \"interviewerContext\": \"Context for the AI interviewer about this candidate's background\"\n}\n\nGenerate 8-12 questions covering all categories. Ensure the total time allocation sums to approximately 45 minutes.`;\n}\n\n/**\n * Parse OpenAI response into structured format\n */\nfunction parseQuestionResponse(response: string): Partial<PersonalizedQuestionSet> {\n  try {\n    // Try to extract JSON from the response\n    const jsonMatch = response.match(/\\{[\\s\\S]*\\}/);\n    if (jsonMatch) {\n      return JSON.parse(jsonMatch[0]);\n    }\n    throw new Error(\"No JSON found in response\");\n  } catch (error) {\n    console.error(\"Failed to parse question response:\", error);\n    return {\n      questions: [],\n      focusAreas: [],\n      personalizationNotes: \"Failed to generate personalized questions\",\n      interviewerContext: \"\",\n    };\n  }\n}\n\n/**\n * Generate personalized questions using Ollama (local LLM)\n */\nexport async function generatePersonalizedQuestions(\n  input: QuestionGeneratorInput\n): Promise<PersonalizedQuestionSet> {\n  const prompt = buildQuestionPrompt(input);\n\n  try {\n    const completion = await ollamaChat({\n      messages: [\n        {\n          role: \"system\",\n          content: \"You are an expert system design interview question generator. Always respond with valid JSON only, no additional text.\",\n        },\n        {\n          role: \"user\",\n          content: prompt,\n        },\n      ],\n      temperature: 0.7,\n      max_tokens: 2500,\n    });\n\n    const responseText = completion.message?.content || \"\";\n    const parsed = parseQuestionResponse(responseText);\n\n    return {\n      topic: input.topic,\n      difficulty: input.difficulty,\n      totalDuration: 45,\n      questions: parsed.questions || [],\n      focusAreas: parsed.focusAreas || [],\n      personalizationNotes: parsed.personalizationNotes || \"\",\n      interviewerContext: parsed.interviewerContext || \"\",\n    };\n  } catch (error) {\n    console.error(\"Ollama question generation error:\", error);\n\n    // Return fallback questions\n    return generateFallbackQuestions(input);\n  }\n}\n\n/**\n * Generate fallback questions if Ollama fails\n */\nfunction generateFallbackQuestions(input: QuestionGeneratorInput): PersonalizedQuestionSet {\n  const { topic, difficulty } = input;\n\n  const baseQuestions: GeneratedQuestion[] = [\n    {\n      id: \"q1\",\n      question: `Let's design ${topic}. Before we start, what clarifying questions would you like to ask about the requirements?`,\n      category: \"requirements\",\n      difficulty,\n      focusAreas: [\"scope\", \"constraints\", \"users\"],\n      expectedTopics: [\"scale\", \"features\", \"non-functional requirements\"],\n      timeAllocation: 8,\n    },\n    {\n      id: \"q2\",\n      question: \"Can you walk me through the high-level architecture? What are the main components?\",\n      category: \"high-level\",\n      difficulty,\n      focusAreas: [\"architecture\", \"components\"],\n      expectedTopics: [\"clients\", \"servers\", \"databases\", \"caches\"],\n      timeAllocation: 12,\n    },\n    {\n      id: \"q3\",\n      question: \"Let's dive deeper into the data model. How would you structure the database?\",\n      category: \"deep-dive\",\n      difficulty,\n      focusAreas: [\"data modeling\", \"storage\"],\n      expectedTopics: [\"tables\", \"relationships\", \"indexes\"],\n      timeAllocation: 6,\n    },\n    {\n      id: \"q4\",\n      question: \"How would you handle the core functionality? Walk me through the main flow.\",\n      category: \"deep-dive\",\n      difficulty,\n      focusAreas: [\"implementation\", \"algorithms\"],\n      expectedTopics: [\"API design\", \"business logic\"],\n      timeAllocation: 6,\n    },\n    {\n      id: \"q5\",\n      question: \"How would you scale this system to handle 10x or 100x the current load?\",\n      category: \"scalability\",\n      difficulty,\n      focusAreas: [\"scaling\", \"performance\"],\n      expectedTopics: [\"horizontal scaling\", \"caching\", \"sharding\"],\n      timeAllocation: 5,\n    },\n    {\n      id: \"q6\",\n      question: \"What are the potential bottlenecks and how would you address them?\",\n      category: \"scalability\",\n      difficulty,\n      focusAreas: [\"bottlenecks\", \"optimization\"],\n      expectedTopics: [\"database\", \"network\", \"compute\"],\n      timeAllocation: 5,\n    },\n    {\n      id: \"q7\",\n      question: \"What trade-offs did you make in your design? Would you change anything?\",\n      category: \"trade-offs\",\n      difficulty,\n      focusAreas: [\"trade-offs\", \"alternatives\"],\n      expectedTopics: [\"consistency vs availability\", \"cost vs performance\"],\n      timeAllocation: 3,\n    },\n  ];\n\n  return {\n    topic,\n    difficulty,\n    totalDuration: 45,\n    questions: baseQuestions,\n    focusAreas: [\"System Design Fundamentals\", \"Scalability\", \"Trade-offs\"],\n    personalizationNotes: \"Using standard question set due to limited personalization data\",\n    interviewerContext: `Interviewing for ${topic} at ${difficulty} difficulty level.`,\n  };\n}\n\n/**\n * Build enhanced interviewer prompt with personalization\n */\nexport function buildPersonalizedInterviewerPrompt(\n  questionSet: PersonalizedQuestionSet,\n  profile: UserProfileData | null\n): string {\n  const { topic, difficulty, interviewerContext, focusAreas, personalizationNotes } = questionSet;\n\n  const skillsContext = profile?.skills?.length\n    ? `The candidate has experience with: ${profile.skills.join(\", \")}.`\n    : \"\";\n\n  const experienceContext = profile?.yearsExperience\n    ? `They have ${profile.yearsExperience} years of experience.`\n    : \"\";\n\n  const companiesContext = profile?.targetCompanies?.length\n    ? `They are targeting companies like: ${profile.targetCompanies.join(\", \")}.`\n    : \"\";\n\n  return `You are Bobby, a friendly and experienced senior software engineer conducting a 45-minute system design interview at a top tech company. You have 10+ years of experience building large-scale distributed systems.\n\n## Interview Topic: ${topic}\n## Difficulty: ${difficulty.toUpperCase()}\n\n## Candidate Context\n${interviewerContext}\n${skillsContext}\n${experienceContext}\n${companiesContext}\n\n## Personalization Notes\n${personalizationNotes}\n\n## Focus Areas for This Interview\n${focusAreas.map(area => `- ${area}`).join(\"\\n\")}\n\n## Interview Guidelines\n\n1. **Your Personality as Bobby**:\n   - Be warm, encouraging, and professional\n   - Use a conversational tone while maintaining technical rigor\n   - Occasionally use phrases like \"Great point!\", \"That's interesting...\", \"I like that approach\"\n\n2. **Adapt to the Candidate**: Use their background to ask relevant follow-up questions.\n   - If they mention technologies they know, probe deeper on those.\n   - If they're junior, provide more guidance; if senior, expect more depth.\n\n3. **Difficulty Calibration**:\n   ${difficulty === \"easy\" ? \"- Be supportive and provide hints when the candidate is stuck\" : \"\"}\n   ${difficulty === \"medium\" ? \"- Balance guidance with challenge, probe on trade-offs\" : \"\"}\n   ${difficulty === \"hard\" ? \"- Be rigorous, expect precise answers, challenge all decisions\" : \"\"}\n\n4. **Phase Management**: Guide through all phases:\n   - Requirements (8 min) â†’ High-Level Design (12 min) â†’ Deep Dive (12 min) â†’ Scalability (10 min) â†’ Wrap-up (3 min)\n\n5. **Question Style**:\n   - Keep responses concise (2-4 sentences)\n   - Ask one question at a time\n   - Acknowledge good points before probing deeper\n   - Use \"[PHASE_TRANSITION: phase-id]\" to signal phase changes\n\n6. **Personalized Probing**:\n   - If candidate mentions their known technologies, ask how they'd apply them\n   - For target company alignment, use similar question patterns those companies use\n\nStart by introducing yourself as Bobby, mention you're excited to discuss ${topic} with them, and ask the first clarifying question about requirements.`;\n}\n"],"names":[],"mappings":";;;;;;AAAA;;;;;CAKC,GAED;AACA;;;AAqCA;;CAEC,GACD,SAAS,oBAAoB,OAA+B;IAC1D,IAAI,CAAC,SAAS,OAAO;IAErB,MAAM,QAAkB,EAAE;IAE1B,IAAI,QAAQ,eAAe,EAAE;QAC3B,MAAM,IAAI,CAAC,CAAC,qBAAqB,EAAE,QAAQ,eAAe,EAAE;IAC9D;IAEA,IAAI,QAAQ,UAAU,EAAE;QACtB,MAAM,IAAI,CAAC,CAAC,aAAa,EAAE,QAAQ,UAAU,EAAE;IACjD;IAEA,IAAI,QAAQ,MAAM,IAAI,QAAQ,MAAM,CAAC,MAAM,GAAG,GAAG;QAC/C,MAAM,IAAI,CAAC,CAAC,kBAAkB,EAAE,QAAQ,MAAM,CAAC,IAAI,CAAC,OAAO;IAC7D;IAEA,IAAI,QAAQ,eAAe,IAAI,QAAQ,eAAe,CAAC,MAAM,GAAG,GAAG;QACjE,MAAM,IAAI,CAAC,CAAC,kBAAkB,EAAE,QAAQ,eAAe,CAAC,IAAI,CAAC,OAAO;IACtE;IAEA,IAAI,QAAQ,GAAG,EAAE;QACf,MAAM,IAAI,CAAC,CAAC,YAAY,EAAE,QAAQ,GAAG,EAAE;IACzC;IAEA,OAAO,MAAM,MAAM,GAAG,IAAI,MAAM,IAAI,CAAC,QAAQ;AAC/C;AAEA;;CAEC,GACD,SAAS,mBAAmB,MAAyB;IACnD,IAAI,CAAC,QAAQ,OAAO;IAEpB,MAAM,QAAkB,EAAE;IAE1B,IAAI,OAAO,QAAQ,EAAE;QACnB,MAAM,IAAI,CAAC,CAAC,WAAW,EAAE,OAAO,QAAQ,CAAC,IAAI,CAAC;IAChD;IAEA,IAAI,OAAO,QAAQ,IAAI,OAAO,QAAQ,CAAC,MAAM,GAAG,GAAG;QACjD,MAAM,IAAI,CAAC,CAAC,wBAAwB,EAAE,OAAO,QAAQ,CAAC,KAAK,CAAC,GAAG,IAAI,IAAI,CAAC,OAAO;IACjF;IAEA,IAAI,OAAO,cAAc,IAAI,OAAO,cAAc,CAAC,MAAM,GAAG,GAAG;QAC7D,MAAM,IAAI,CAAC,CAAC,0BAA0B,EAAE,OAAO,cAAc,CAAC,IAAI,CAAC,OAAO;IAC5E;IAEA,IAAI,OAAO,QAAQ,EAAE;QACnB,IAAI,OAAO,QAAQ,CAAC,eAAe,EAAE;YACnC,MAAM,IAAI,CAAC,CAAC,kBAAkB,EAAE,OAAO,QAAQ,CAAC,eAAe,EAAE;QACnE;QACA,IAAI,OAAO,QAAQ,CAAC,cAAc,EAAE;YAClC,MAAM,IAAI,CAAC,CAAC,iBAAiB,EAAE,OAAO,QAAQ,CAAC,cAAc,EAAE;QACjE;QACA,IAAI,OAAO,QAAQ,CAAC,sBAAsB,EAAE;YAC1C,MAAM,IAAI,CAAC,CAAC,0BAA0B,EAAE,OAAO,QAAQ,CAAC,sBAAsB,EAAE;QAClF;IACF;IAEA,OAAO,MAAM,MAAM,GAAG,IAAI,MAAM,IAAI,CAAC,QAAQ;AAC/C;AAEA;;CAEC,GACD,SAAS,sBAAsB,eAAiC;IAC9D,IAAI,CAAC,mBAAmB,CAAC,gBAAgB,UAAU,IAAI,gBAAgB,UAAU,CAAC,MAAM,KAAK,GAAG;QAC9F,OAAO;IACT;IAEA,IAAI,UAAU,CAAC;;;6BAGY,EAAE,gBAAgB,mBAAmB,CAAC,mBAAmB,EAAE,gBAAgB,mBAAmB,KAAK,IAAI,KAAK,IAAI;;;AAG7I,CAAC;IAEC,KAAK,MAAM,MAAM,gBAAgB,UAAU,CAAC,KAAK,CAAC,GAAG,GAAI;QACvD,WAAW,CAAC,IAAI,EAAE,GAAG,SAAS,CAAC,WAAW,EAAE,GAAG,SAAS,CAAC,WAAW,EAAE,GAAG,KAAK,CAAC;qBAC9D,EAAE,GAAG,OAAO,CAAC;AAClC,CAAC;IACC;IAEA,IAAI,gBAAgB,eAAe,CAAC,MAAM,GAAG,GAAG;QAC9C,WAAW,CAAC;;AAEhB,EAAE,gBAAgB,eAAe,CAAC,GAAG,CAAC,CAAA,IAAK,CAAC,EAAE,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;AAChE,CAAC;IACC;IAEA,IAAI,gBAAgB,iBAAiB,CAAC,MAAM,GAAG,GAAG;QAChD,WAAW,CAAC;;AAEhB,EAAE,gBAAgB,iBAAiB,CAAC,IAAI,CAAC,MAAM;AAC/C,CAAC;IACC;IAEA,WAAW,CAAC;2BACa,EAAE,gBAAgB,iBAAiB,CAAC;AAC/D,EAAE,gBAAgB,iBAAiB,KAAK,WAAW,+DAA+D,GAAG;AACrH,EAAE,gBAAgB,iBAAiB,KAAK,WAAW,6DAA6D,GAAG;AACnH,CAAC;IAEC,OAAO;AACT;AAEA;;CAEC,GACD,SAAS,oBAAoB,KAA6B;IACxD,MAAM,EAAE,KAAK,EAAE,UAAU,EAAE,OAAO,EAAE,MAAM,EAAE,eAAe,EAAE,GAAG;IAEhE,MAAM,iBAAiB,oBAAoB;IAC3C,MAAM,gBAAgB,mBAAmB;IACzC,MAAM,iBAAiB,IAAA,6OAAwB,EAAC;IAChD,MAAM,mBAAmB,sBAAsB;IAE/C,OAAO,CAAC;;iFAEuE,EAAE,MAAM;;;AAGzF,EAAE,eAAe;;;AAGjB,EAAE,cAAc;;;oBAGI,EAAE,WAAW,WAAW,GAAG;EAC7C,EAAE,eAAe;;AAEnB,EAAE,iBAAiB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;mHA4CgG,CAAC;AACpH;AAEA;;CAEC,GACD,SAAS,sBAAsB,QAAgB;IAC7C,IAAI;QACF,wCAAwC;QACxC,MAAM,YAAY,SAAS,KAAK,CAAC;QACjC,IAAI,WAAW;YACb,OAAO,KAAK,KAAK,CAAC,SAAS,CAAC,EAAE;QAChC;QACA,MAAM,IAAI,MAAM;IAClB,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,sCAAsC;QACpD,OAAO;YACL,WAAW,EAAE;YACb,YAAY,EAAE;YACd,sBAAsB;YACtB,oBAAoB;QACtB;IACF;AACF;AAKO,eAAe,8BACpB,KAA6B;IAE7B,MAAM,SAAS,oBAAoB;IAEnC,IAAI;QACF,MAAM,aAAa,MAAM,IAAA,6MAAU,EAAC;YAClC,UAAU;gBACR;oBACE,MAAM;oBACN,SAAS;gBACX;gBACA;oBACE,MAAM;oBACN,SAAS;gBACX;aACD;YACD,aAAa;YACb,YAAY;QACd;QAEA,MAAM,eAAe,WAAW,OAAO,EAAE,WAAW;QACpD,MAAM,SAAS,sBAAsB;QAErC,OAAO;YACL,OAAO,MAAM,KAAK;YAClB,YAAY,MAAM,UAAU;YAC5B,eAAe;YACf,WAAW,OAAO,SAAS,IAAI,EAAE;YACjC,YAAY,OAAO,UAAU,IAAI,EAAE;YACnC,sBAAsB,OAAO,oBAAoB,IAAI;YACrD,oBAAoB,OAAO,kBAAkB,IAAI;QACnD;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,qCAAqC;QAEnD,4BAA4B;QAC5B,OAAO,0BAA0B;IACnC;AACF;AAEA;;CAEC,GACD,SAAS,0BAA0B,KAA6B;IAC9D,MAAM,EAAE,KAAK,EAAE,UAAU,EAAE,GAAG;IAE9B,MAAM,gBAAqC;QACzC;YACE,IAAI;YACJ,UAAU,CAAC,aAAa,EAAE,MAAM,0FAA0F,CAAC;YAC3H,UAAU;YACV;YACA,YAAY;gBAAC;gBAAS;gBAAe;aAAQ;YAC7C,gBAAgB;gBAAC;gBAAS;gBAAY;aAA8B;YACpE,gBAAgB;QAClB;QACA;YACE,IAAI;YACJ,UAAU;YACV,UAAU;YACV;YACA,YAAY;gBAAC;gBAAgB;aAAa;YAC1C,gBAAgB;gBAAC;gBAAW;gBAAW;gBAAa;aAAS;YAC7D,gBAAgB;QAClB;QACA;YACE,IAAI;YACJ,UAAU;YACV,UAAU;YACV;YACA,YAAY;gBAAC;gBAAiB;aAAU;YACxC,gBAAgB;gBAAC;gBAAU;gBAAiB;aAAU;YACtD,gBAAgB;QAClB;QACA;YACE,IAAI;YACJ,UAAU;YACV,UAAU;YACV;YACA,YAAY;gBAAC;gBAAkB;aAAa;YAC5C,gBAAgB;gBAAC;gBAAc;aAAiB;YAChD,gBAAgB;QAClB;QACA;YACE,IAAI;YACJ,UAAU;YACV,UAAU;YACV;YACA,YAAY;gBAAC;gBAAW;aAAc;YACtC,gBAAgB;gBAAC;gBAAsB;gBAAW;aAAW;YAC7D,gBAAgB;QAClB;QACA;YACE,IAAI;YACJ,UAAU;YACV,UAAU;YACV;YACA,YAAY;gBAAC;gBAAe;aAAe;YAC3C,gBAAgB;gBAAC;gBAAY;gBAAW;aAAU;YAClD,gBAAgB;QAClB;QACA;YACE,IAAI;YACJ,UAAU;YACV,UAAU;YACV;YACA,YAAY;gBAAC;gBAAc;aAAe;YAC1C,gBAAgB;gBAAC;gBAA+B;aAAsB;YACtE,gBAAgB;QAClB;KACD;IAED,OAAO;QACL;QACA;QACA,eAAe;QACf,WAAW;QACX,YAAY;YAAC;YAA8B;YAAe;SAAa;QACvE,sBAAsB;QACtB,oBAAoB,CAAC,iBAAiB,EAAE,MAAM,IAAI,EAAE,WAAW,kBAAkB,CAAC;IACpF;AACF;AAKO,SAAS,mCACd,WAAoC,EACpC,OAA+B;IAE/B,MAAM,EAAE,KAAK,EAAE,UAAU,EAAE,kBAAkB,EAAE,UAAU,EAAE,oBAAoB,EAAE,GAAG;IAEpF,MAAM,gBAAgB,SAAS,QAAQ,SACnC,CAAC,mCAAmC,EAAE,QAAQ,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,GAClE;IAEJ,MAAM,oBAAoB,SAAS,kBAC/B,CAAC,UAAU,EAAE,QAAQ,eAAe,CAAC,qBAAqB,CAAC,GAC3D;IAEJ,MAAM,mBAAmB,SAAS,iBAAiB,SAC/C,CAAC,mCAAmC,EAAE,QAAQ,eAAe,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,GAC3E;IAEJ,OAAO,CAAC;;oBAEU,EAAE,MAAM;eACb,EAAE,WAAW,WAAW,GAAG;;;AAG1C,EAAE,mBAAmB;AACrB,EAAE,cAAc;AAChB,EAAE,kBAAkB;AACpB,EAAE,iBAAiB;;;AAGnB,EAAE,qBAAqB;;;AAGvB,EAAE,WAAW,GAAG,CAAC,CAAA,OAAQ,CAAC,EAAE,EAAE,MAAM,EAAE,IAAI,CAAC,MAAM;;;;;;;;;;;;;;GAc9C,EAAE,eAAe,SAAS,kEAAkE,GAAG;GAC/F,EAAE,eAAe,WAAW,2DAA2D,GAAG;GAC1F,EAAE,eAAe,SAAS,mEAAmE,GAAG;;;;;;;;;;;;;;;0EAezB,EAAE,MAAM,qEAAqE,CAAC;AACxJ"}},
    {"offset": {"line": 1917, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/analytics/performance-analytics.ts"],"sourcesContent":["/**\n * Performance Analytics Library\n *\n * Core analytics calculations for user performance tracking.\n * Tracks score progression, identifies weak areas, and generates insights.\n */\n\nimport { prisma } from \"@/lib/prisma\";\n\n// Score dimension type for type-safe access\ntype ScoreDimension =\n  | \"requirementsClarification\"\n  | \"highLevelDesign\"\n  | \"detailedDesign\"\n  | \"scalability\"\n  | \"tradeoffs\"\n  | \"communication\";\n\n// Helper to get score value from dimension key\nfunction getScoreValue(\n  score: {\n    requirementsClarification: number;\n    highLevelDesign: number;\n    detailedDesign: number;\n    scalability: number;\n    tradeoffs: number;\n    communication: number;\n  },\n  dimension: ScoreDimension\n): number {\n  return score[dimension];\n}\n\n// Types\nexport interface ScoreDataPoint {\n  date: string;\n  overallScore: number;\n  passStatus: boolean;\n  topic: string;\n  interviewId: string;\n}\n\nexport interface DimensionMetrics {\n  avgScore: number;\n  trend: \"improving\" | \"stable\" | \"declining\";\n  isWeak: boolean; // Below 2.5\n  isStrong: boolean; // Above 3.0\n  recentScores: number[];\n}\n\nexport interface WeakArea {\n  dimension: string;\n  avgScore: number;\n  occurrences: number;\n  lastScore: number;\n  trend: \"improving\" | \"stable\" | \"declining\";\n}\n\nexport interface PerformanceMetrics {\n  overall: {\n    totalInterviews: number;\n    completedInterviews: number;\n    avgScore: number;\n    passRate: number;\n    scoreTrend: \"improving\" | \"stable\" | \"declining\";\n  };\n  dimensions: {\n    [key: string]: DimensionMetrics;\n  };\n  recentScores: ScoreDataPoint[];\n  insights: string[];\n  recommendations: string[];\n}\n\n// Dimension names mapping\nconst DIMENSION_NAMES: Record<string, string> = {\n  requirementsClarification: \"Requirements Clarification\",\n  highLevelDesign: \"High-Level Design\",\n  detailedDesign: \"Detailed Design\",\n  scalability: \"Scalability\",\n  tradeoffs: \"Trade-offs\",\n  communication: \"Communication\",\n};\n\nconst DIMENSION_KEYS = Object.keys(DIMENSION_NAMES);\n\n/**\n * Calculate trend from an array of scores (oldest to newest)\n */\nexport function calculateTrend(scores: number[]): \"improving\" | \"stable\" | \"declining\" {\n  if (scores.length < 3) return \"stable\";\n\n  // Use linear regression to determine trend\n  const n = scores.length;\n  let sumX = 0, sumY = 0, sumXY = 0, sumXX = 0;\n\n  for (let i = 0; i < n; i++) {\n    sumX += i;\n    sumY += scores[i];\n    sumXY += i * scores[i];\n    sumXX += i * i;\n  }\n\n  const slope = (n * sumXY - sumX * sumY) / (n * sumXX - sumX * sumX);\n\n  // Threshold for trend detection\n  if (slope > 0.1) return \"improving\";\n  if (slope < -0.1) return \"declining\";\n  return \"stable\";\n}\n\n/**\n * Get score progression over time for a user\n */\nexport async function getScoreProgression(\n  userId: string,\n  dimension?: string,\n  limit: number = 20\n): Promise<ScoreDataPoint[]> {\n  const interviews = await prisma.interview.findMany({\n    where: {\n      userId,\n      status: \"completed\",\n    },\n    orderBy: { endedAt: \"asc\" },\n    take: limit,\n    include: {\n      score: true,\n    },\n  });\n\n  return interviews\n    .filter((i) => i.score)\n    .map((interview) => ({\n      date: interview.endedAt?.toISOString() || interview.createdAt.toISOString(),\n      overallScore: dimension\n        ? getScoreValue(interview.score!, dimension as ScoreDimension)\n        : interview.score!.overallScore,\n      passStatus: interview.score!.passStatus,\n      topic: interview.topic,\n      interviewId: interview.id,\n    }));\n}\n\n/**\n * Identify weak areas for a user\n */\nexport async function identifyWeakAreas(userId: string, limit: number = 10): Promise<WeakArea[]> {\n  const interviews = await prisma.interview.findMany({\n    where: {\n      userId,\n      status: \"completed\",\n    },\n    orderBy: { endedAt: \"desc\" },\n    take: limit,\n    include: {\n      score: true,\n    },\n  });\n\n  const dimensionScores: Record<string, number[]> = {};\n\n  // Initialize arrays for each dimension\n  DIMENSION_KEYS.forEach((key) => {\n    dimensionScores[key] = [];\n  });\n\n  // Collect scores for each dimension\n  for (const interview of interviews) {\n    if (!interview.score) continue;\n\n    DIMENSION_KEYS.forEach((key) => {\n      const score = getScoreValue(interview.score!, key as ScoreDimension);\n      if (score !== undefined) {\n        dimensionScores[key].push(score);\n      }\n    });\n  }\n\n  // Calculate weak areas (avg < 2.5)\n  const weakAreas: WeakArea[] = [];\n\n  for (const [dimension, scores] of Object.entries(dimensionScores)) {\n    if (scores.length === 0) continue;\n\n    const avgScore = scores.reduce((a, b) => a + b, 0) / scores.length;\n\n    if (avgScore < 2.5) {\n      weakAreas.push({\n        dimension: DIMENSION_NAMES[dimension] || dimension,\n        avgScore: Number(avgScore.toFixed(2)),\n        occurrences: scores.filter((s) => s < 2.5).length,\n        lastScore: scores[0], // Most recent\n        trend: calculateTrend(scores.reverse()), // Oldest to newest for trend\n      });\n    }\n  }\n\n  // Sort by average score (weakest first)\n  return weakAreas.sort((a, b) => a.avgScore - b.avgScore);\n}\n\n/**\n * Generate insights based on performance metrics\n */\nexport function generateInsights(metrics: PerformanceMetrics): string[] {\n  const insights: string[] = [];\n\n  // Overall performance insights\n  if (metrics.overall.completedInterviews === 0) {\n    insights.push(\"Complete your first interview to start tracking progress!\");\n    return insights;\n  }\n\n  if (metrics.overall.avgScore >= 3.0) {\n    insights.push(\"Your overall performance is strong. You're ready for senior-level interviews.\");\n  } else if (metrics.overall.avgScore >= 2.5) {\n    insights.push(\"You're performing at a passing level. Focus on your weak areas to improve.\");\n  } else {\n    insights.push(\"Your scores indicate room for improvement. Consider more practice sessions.\");\n  }\n\n  // Pass rate insights\n  if (metrics.overall.passRate >= 80) {\n    insights.push(`Excellent pass rate of ${metrics.overall.passRate.toFixed(0)}%!`);\n  } else if (metrics.overall.passRate >= 50) {\n    insights.push(`Your pass rate is ${metrics.overall.passRate.toFixed(0)}%. Aim for consistency.`);\n  } else if (metrics.overall.completedInterviews >= 3) {\n    insights.push(`Pass rate of ${metrics.overall.passRate.toFixed(0)}% needs attention. Review fundamentals.`);\n  }\n\n  // Trend insights\n  if (metrics.overall.scoreTrend === \"improving\") {\n    insights.push(\"Your scores are trending upward. Keep up the great work!\");\n  } else if (metrics.overall.scoreTrend === \"declining\") {\n    insights.push(\"Your recent scores have declined. Consider reviewing basics.\");\n  }\n\n  // Dimension-specific insights\n  const weakDimensions = Object.entries(metrics.dimensions)\n    .filter(([, m]) => m.isWeak)\n    .map(([key]) => DIMENSION_NAMES[key] || key);\n\n  if (weakDimensions.length > 0) {\n    insights.push(`Focus areas: ${weakDimensions.join(\", \")}`);\n  }\n\n  const strongDimensions = Object.entries(metrics.dimensions)\n    .filter(([, m]) => m.isStrong)\n    .map(([key]) => DIMENSION_NAMES[key] || key);\n\n  if (strongDimensions.length > 0) {\n    insights.push(`Strong areas: ${strongDimensions.join(\", \")}`);\n  }\n\n  return insights;\n}\n\n/**\n * Generate recommendations based on performance\n */\nexport function generateRecommendations(metrics: PerformanceMetrics): string[] {\n  const recommendations: string[] = [];\n\n  if (metrics.overall.completedInterviews === 0) {\n    recommendations.push(\"Start with an easy difficulty interview to build confidence.\");\n    return recommendations;\n  }\n\n  // Weak dimension recommendations\n  const weakDimensions = Object.entries(metrics.dimensions)\n    .filter(([, m]) => m.isWeak)\n    .sort((a, b) => a[1].avgScore - b[1].avgScore);\n\n  for (const [key, dimMetrics] of weakDimensions.slice(0, 2)) {\n    const name = DIMENSION_NAMES[key] || key;\n\n    switch (key) {\n      case \"requirementsClarification\":\n        recommendations.push(\n          `Practice asking clarifying questions about scale, users, and constraints.`\n        );\n        break;\n      case \"highLevelDesign\":\n        recommendations.push(\n          `Study common system design patterns and practice drawing architecture diagrams.`\n        );\n        break;\n      case \"detailedDesign\":\n        recommendations.push(\n          `Review data modeling, API design, and dive deeper into component internals.`\n        );\n        break;\n      case \"scalability\":\n        recommendations.push(\n          `Study caching, sharding, load balancing, and horizontal scaling strategies.`\n        );\n        break;\n      case \"tradeoffs\":\n        recommendations.push(\n          `Practice discussing CAP theorem, consistency vs availability, and cost tradeoffs.`\n        );\n        break;\n      case \"communication\":\n        recommendations.push(\n          `Practice explaining your thought process clearly and structuring your responses.`\n        );\n        break;\n      default:\n        recommendations.push(`Focus on improving ${name} (current avg: ${dimMetrics.avgScore.toFixed(1)}).`);\n    }\n  }\n\n  // Interview frequency recommendation\n  if (metrics.overall.completedInterviews < 5) {\n    recommendations.push(\"Complete at least 5 interviews to establish reliable patterns.\");\n  }\n\n  // Difficulty recommendation\n  if (metrics.overall.avgScore >= 3.0 && metrics.overall.passRate >= 70) {\n    recommendations.push(\"Consider increasing difficulty to challenge yourself further.\");\n  } else if (metrics.overall.avgScore < 2.0) {\n    recommendations.push(\"Try easier interviews to build foundational knowledge.\");\n  }\n\n  return recommendations;\n}\n\n/**\n * Calculate comprehensive performance metrics for a user\n */\nexport async function calculatePerformanceMetrics(\n  userId: string,\n  limit: number = 20\n): Promise<PerformanceMetrics> {\n  // Fetch interview data\n  const [totalCount, completedInterviews] = await Promise.all([\n    prisma.interview.count({\n      where: { userId },\n    }),\n    prisma.interview.findMany({\n      where: {\n        userId,\n        status: \"completed\",\n      },\n      orderBy: { endedAt: \"desc\" },\n      take: limit,\n      include: {\n        score: true,\n      },\n    }),\n  ]);\n\n  const completedCount = completedInterviews.length;\n  const interviewsWithScores = completedInterviews.filter((i) => i.score);\n\n  // Calculate overall metrics\n  let avgScore = 0;\n  let passCount = 0;\n  const overallScores: number[] = [];\n\n  for (const interview of interviewsWithScores) {\n    avgScore += interview.score!.overallScore;\n    if (interview.score!.passStatus) passCount++;\n    overallScores.push(interview.score!.overallScore);\n  }\n\n  if (interviewsWithScores.length > 0) {\n    avgScore /= interviewsWithScores.length;\n  }\n\n  const passRate = interviewsWithScores.length > 0\n    ? (passCount / interviewsWithScores.length) * 100\n    : 0;\n\n  // Calculate dimension metrics\n  const dimensionMetrics: Record<string, DimensionMetrics> = {};\n\n  for (const key of DIMENSION_KEYS) {\n    const scores: number[] = [];\n\n    for (const interview of interviewsWithScores) {\n      const score = getScoreValue(interview.score!, key as ScoreDimension);\n      if (score !== undefined) {\n        scores.push(score);\n      }\n    }\n\n    if (scores.length > 0) {\n      const dimAvg = scores.reduce((a, b) => a + b, 0) / scores.length;\n      dimensionMetrics[key] = {\n        avgScore: Number(dimAvg.toFixed(2)),\n        trend: calculateTrend(scores.reverse()), // Oldest to newest\n        isWeak: dimAvg < 2.5,\n        isStrong: dimAvg >= 3.0,\n        recentScores: scores.slice(-5),\n      };\n    } else {\n      dimensionMetrics[key] = {\n        avgScore: 0,\n        trend: \"stable\",\n        isWeak: false,\n        isStrong: false,\n        recentScores: [],\n      };\n    }\n  }\n\n  // Build recent scores\n  const recentScores: ScoreDataPoint[] = interviewsWithScores.map((i) => ({\n    date: i.endedAt?.toISOString() || i.createdAt.toISOString(),\n    overallScore: i.score!.overallScore,\n    passStatus: i.score!.passStatus,\n    topic: i.topic,\n    interviewId: i.id,\n  }));\n\n  // Build metrics object\n  const metrics: PerformanceMetrics = {\n    overall: {\n      totalInterviews: totalCount,\n      completedInterviews: completedCount,\n      avgScore: Number(avgScore.toFixed(2)),\n      passRate: Number(passRate.toFixed(1)),\n      scoreTrend: calculateTrend(overallScores.reverse()),\n    },\n    dimensions: dimensionMetrics,\n    recentScores,\n    insights: [],\n    recommendations: [],\n  };\n\n  // Generate insights and recommendations\n  metrics.insights = generateInsights(metrics);\n  metrics.recommendations = generateRecommendations(metrics);\n\n  return metrics;\n}\n\n/**\n * Update cached user analytics after an interview\n */\nexport async function updateUserAnalyticsCache(userId: string): Promise<void> {\n  const metrics = await calculatePerformanceMetrics(userId, 50);\n\n  // Extract weak and strong dimensions\n  const weakDimensions = Object.entries(metrics.dimensions)\n    .filter(([, m]) => m.isWeak)\n    .map(([key]) => key);\n\n  const strongDimensions = Object.entries(metrics.dimensions)\n    .filter(([, m]) => m.isStrong)\n    .map(([key]) => key);\n\n  // Calculate topic stats\n  const topicInterviews = await prisma.interview.findMany({\n    where: { userId, status: \"completed\" },\n    include: { score: true },\n  });\n\n  const topicStats: Record<string, { count: number; totalScore: number; passCount: number }> = {};\n\n  for (const interview of topicInterviews) {\n    if (!interview.score) continue;\n\n    if (!topicStats[interview.topic]) {\n      topicStats[interview.topic] = { count: 0, totalScore: 0, passCount: 0 };\n    }\n\n    topicStats[interview.topic].count++;\n    topicStats[interview.topic].totalScore += interview.score.overallScore;\n    if (interview.score.passStatus) {\n      topicStats[interview.topic].passCount++;\n    }\n  }\n\n  // Convert to final format\n  const finalTopicStats: Record<string, { count: number; avgScore: number; passRate: number }> = {};\n  for (const [topic, stats] of Object.entries(topicStats)) {\n    finalTopicStats[topic] = {\n      count: stats.count,\n      avgScore: Number((stats.totalScore / stats.count).toFixed(2)),\n      passRate: Number(((stats.passCount / stats.count) * 100).toFixed(1)),\n    };\n  }\n\n  // Upsert analytics record\n  await prisma.userAnalytics.upsert({\n    where: { userId },\n    create: {\n      userId,\n      totalInterviews: metrics.overall.totalInterviews,\n      completedInterviews: metrics.overall.completedInterviews,\n      avgOverallScore: metrics.overall.avgScore,\n      passRate: metrics.overall.passRate,\n      weakDimensions: JSON.stringify(weakDimensions),\n      strongDimensions: JSON.stringify(strongDimensions),\n      topicStats: JSON.stringify(finalTopicStats),\n      scoreTrend: metrics.overall.scoreTrend,\n      lastCalculatedAt: new Date(),\n    },\n    update: {\n      totalInterviews: metrics.overall.totalInterviews,\n      completedInterviews: metrics.overall.completedInterviews,\n      avgOverallScore: metrics.overall.avgScore,\n      passRate: metrics.overall.passRate,\n      weakDimensions: JSON.stringify(weakDimensions),\n      strongDimensions: JSON.stringify(strongDimensions),\n      topicStats: JSON.stringify(finalTopicStats),\n      scoreTrend: metrics.overall.scoreTrend,\n      lastCalculatedAt: new Date(),\n    },\n  });\n}\n\n/**\n * Get performance category based on average score\n */\nexport function getPerformanceCategory(\n  avgScore: number\n): \"struggling\" | \"developing\" | \"proficient\" | \"excelling\" {\n  if (avgScore < 2.0) return \"struggling\";\n  if (avgScore < 2.5) return \"developing\";\n  if (avgScore < 3.0) return \"proficient\";\n  return \"excelling\";\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;AAAA;;;;;CAKC,GAED;;;;;;AAWA,+CAA+C;AAC/C,SAAS,cACP,KAOC,EACD,SAAyB;IAEzB,OAAO,KAAK,CAAC,UAAU;AACzB;AA2CA,0BAA0B;AAC1B,MAAM,kBAA0C;IAC9C,2BAA2B;IAC3B,iBAAiB;IACjB,gBAAgB;IAChB,aAAa;IACb,WAAW;IACX,eAAe;AACjB;AAEA,MAAM,iBAAiB,OAAO,IAAI,CAAC;AAK5B,SAAS,eAAe,MAAgB;IAC7C,IAAI,OAAO,MAAM,GAAG,GAAG,OAAO;IAE9B,2CAA2C;IAC3C,MAAM,IAAI,OAAO,MAAM;IACvB,IAAI,OAAO,GAAG,OAAO,GAAG,QAAQ,GAAG,QAAQ;IAE3C,IAAK,IAAI,IAAI,GAAG,IAAI,GAAG,IAAK;QAC1B,QAAQ;QACR,QAAQ,MAAM,CAAC,EAAE;QACjB,SAAS,IAAI,MAAM,CAAC,EAAE;QACtB,SAAS,IAAI;IACf;IAEA,MAAM,QAAQ,CAAC,IAAI,QAAQ,OAAO,IAAI,IAAI,CAAC,IAAI,QAAQ,OAAO,IAAI;IAElE,gCAAgC;IAChC,IAAI,QAAQ,KAAK,OAAO;IACxB,IAAI,QAAQ,CAAC,KAAK,OAAO;IACzB,OAAO;AACT;AAKO,eAAe,oBACpB,MAAc,EACd,SAAkB,EAClB,QAAgB,EAAE;IAElB,MAAM,aAAa,MAAM,yMAAM,CAAC,SAAS,CAAC,QAAQ,CAAC;QACjD,OAAO;YACL;YACA,QAAQ;QACV;QACA,SAAS;YAAE,SAAS;QAAM;QAC1B,MAAM;QACN,SAAS;YACP,OAAO;QACT;IACF;IAEA,OAAO,WACJ,MAAM,CAAC,CAAC,IAAM,EAAE,KAAK,EACrB,GAAG,CAAC,CAAC,YAAc,CAAC;YACnB,MAAM,UAAU,OAAO,EAAE,iBAAiB,UAAU,SAAS,CAAC,WAAW;YACzE,cAAc,YACV,cAAc,UAAU,KAAK,EAAG,aAChC,UAAU,KAAK,CAAE,YAAY;YACjC,YAAY,UAAU,KAAK,CAAE,UAAU;YACvC,OAAO,UAAU,KAAK;YACtB,aAAa,UAAU,EAAE;QAC3B,CAAC;AACL;AAKO,eAAe,kBAAkB,MAAc,EAAE,QAAgB,EAAE;IACxE,MAAM,aAAa,MAAM,yMAAM,CAAC,SAAS,CAAC,QAAQ,CAAC;QACjD,OAAO;YACL;YACA,QAAQ;QACV;QACA,SAAS;YAAE,SAAS;QAAO;QAC3B,MAAM;QACN,SAAS;YACP,OAAO;QACT;IACF;IAEA,MAAM,kBAA4C,CAAC;IAEnD,uCAAuC;IACvC,eAAe,OAAO,CAAC,CAAC;QACtB,eAAe,CAAC,IAAI,GAAG,EAAE;IAC3B;IAEA,oCAAoC;IACpC,KAAK,MAAM,aAAa,WAAY;QAClC,IAAI,CAAC,UAAU,KAAK,EAAE;QAEtB,eAAe,OAAO,CAAC,CAAC;YACtB,MAAM,QAAQ,cAAc,UAAU,KAAK,EAAG;YAC9C,IAAI,UAAU,WAAW;gBACvB,eAAe,CAAC,IAAI,CAAC,IAAI,CAAC;YAC5B;QACF;IACF;IAEA,mCAAmC;IACnC,MAAM,YAAwB,EAAE;IAEhC,KAAK,MAAM,CAAC,WAAW,OAAO,IAAI,OAAO,OAAO,CAAC,iBAAkB;QACjE,IAAI,OAAO,MAAM,KAAK,GAAG;QAEzB,MAAM,WAAW,OAAO,MAAM,CAAC,CAAC,GAAG,IAAM,IAAI,GAAG,KAAK,OAAO,MAAM;QAElE,IAAI,WAAW,KAAK;YAClB,UAAU,IAAI,CAAC;gBACb,WAAW,eAAe,CAAC,UAAU,IAAI;gBACzC,UAAU,OAAO,SAAS,OAAO,CAAC;gBAClC,aAAa,OAAO,MAAM,CAAC,CAAC,IAAM,IAAI,KAAK,MAAM;gBACjD,WAAW,MAAM,CAAC,EAAE;gBACpB,OAAO,eAAe,OAAO,OAAO;YACtC;QACF;IACF;IAEA,wCAAwC;IACxC,OAAO,UAAU,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,QAAQ,GAAG,EAAE,QAAQ;AACzD;AAKO,SAAS,iBAAiB,OAA2B;IAC1D,MAAM,WAAqB,EAAE;IAE7B,+BAA+B;IAC/B,IAAI,QAAQ,OAAO,CAAC,mBAAmB,KAAK,GAAG;QAC7C,SAAS,IAAI,CAAC;QACd,OAAO;IACT;IAEA,IAAI,QAAQ,OAAO,CAAC,QAAQ,IAAI,KAAK;QACnC,SAAS,IAAI,CAAC;IAChB,OAAO,IAAI,QAAQ,OAAO,CAAC,QAAQ,IAAI,KAAK;QAC1C,SAAS,IAAI,CAAC;IAChB,OAAO;QACL,SAAS,IAAI,CAAC;IAChB;IAEA,qBAAqB;IACrB,IAAI,QAAQ,OAAO,CAAC,QAAQ,IAAI,IAAI;QAClC,SAAS,IAAI,CAAC,CAAC,uBAAuB,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC;IACjF,OAAO,IAAI,QAAQ,OAAO,CAAC,QAAQ,IAAI,IAAI;QACzC,SAAS,IAAI,CAAC,CAAC,kBAAkB,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,OAAO,CAAC,GAAG,uBAAuB,CAAC;IACjG,OAAO,IAAI,QAAQ,OAAO,CAAC,mBAAmB,IAAI,GAAG;QACnD,SAAS,IAAI,CAAC,CAAC,aAAa,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,OAAO,CAAC,GAAG,uCAAuC,CAAC;IAC5G;IAEA,iBAAiB;IACjB,IAAI,QAAQ,OAAO,CAAC,UAAU,KAAK,aAAa;QAC9C,SAAS,IAAI,CAAC;IAChB,OAAO,IAAI,QAAQ,OAAO,CAAC,UAAU,KAAK,aAAa;QACrD,SAAS,IAAI,CAAC;IAChB;IAEA,8BAA8B;IAC9B,MAAM,iBAAiB,OAAO,OAAO,CAAC,QAAQ,UAAU,EACrD,MAAM,CAAC,CAAC,GAAG,EAAE,GAAK,EAAE,MAAM,EAC1B,GAAG,CAAC,CAAC,CAAC,IAAI,GAAK,eAAe,CAAC,IAAI,IAAI;IAE1C,IAAI,eAAe,MAAM,GAAG,GAAG;QAC7B,SAAS,IAAI,CAAC,CAAC,aAAa,EAAE,eAAe,IAAI,CAAC,OAAO;IAC3D;IAEA,MAAM,mBAAmB,OAAO,OAAO,CAAC,QAAQ,UAAU,EACvD,MAAM,CAAC,CAAC,GAAG,EAAE,GAAK,EAAE,QAAQ,EAC5B,GAAG,CAAC,CAAC,CAAC,IAAI,GAAK,eAAe,CAAC,IAAI,IAAI;IAE1C,IAAI,iBAAiB,MAAM,GAAG,GAAG;QAC/B,SAAS,IAAI,CAAC,CAAC,cAAc,EAAE,iBAAiB,IAAI,CAAC,OAAO;IAC9D;IAEA,OAAO;AACT;AAKO,SAAS,wBAAwB,OAA2B;IACjE,MAAM,kBAA4B,EAAE;IAEpC,IAAI,QAAQ,OAAO,CAAC,mBAAmB,KAAK,GAAG;QAC7C,gBAAgB,IAAI,CAAC;QACrB,OAAO;IACT;IAEA,iCAAiC;IACjC,MAAM,iBAAiB,OAAO,OAAO,CAAC,QAAQ,UAAU,EACrD,MAAM,CAAC,CAAC,GAAG,EAAE,GAAK,EAAE,MAAM,EAC1B,IAAI,CAAC,CAAC,GAAG,IAAM,CAAC,CAAC,EAAE,CAAC,QAAQ,GAAG,CAAC,CAAC,EAAE,CAAC,QAAQ;IAE/C,KAAK,MAAM,CAAC,KAAK,WAAW,IAAI,eAAe,KAAK,CAAC,GAAG,GAAI;QAC1D,MAAM,OAAO,eAAe,CAAC,IAAI,IAAI;QAErC,OAAQ;YACN,KAAK;gBACH,gBAAgB,IAAI,CAClB,CAAC,yEAAyE,CAAC;gBAE7E;YACF,KAAK;gBACH,gBAAgB,IAAI,CAClB,CAAC,+EAA+E,CAAC;gBAEnF;YACF,KAAK;gBACH,gBAAgB,IAAI,CAClB,CAAC,2EAA2E,CAAC;gBAE/E;YACF,KAAK;gBACH,gBAAgB,IAAI,CAClB,CAAC,2EAA2E,CAAC;gBAE/E;YACF,KAAK;gBACH,gBAAgB,IAAI,CAClB,CAAC,iFAAiF,CAAC;gBAErF;YACF,KAAK;gBACH,gBAAgB,IAAI,CAClB,CAAC,gFAAgF,CAAC;gBAEpF;YACF;gBACE,gBAAgB,IAAI,CAAC,CAAC,mBAAmB,EAAE,KAAK,eAAe,EAAE,WAAW,QAAQ,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC;QACvG;IACF;IAEA,qCAAqC;IACrC,IAAI,QAAQ,OAAO,CAAC,mBAAmB,GAAG,GAAG;QAC3C,gBAAgB,IAAI,CAAC;IACvB;IAEA,4BAA4B;IAC5B,IAAI,QAAQ,OAAO,CAAC,QAAQ,IAAI,OAAO,QAAQ,OAAO,CAAC,QAAQ,IAAI,IAAI;QACrE,gBAAgB,IAAI,CAAC;IACvB,OAAO,IAAI,QAAQ,OAAO,CAAC,QAAQ,GAAG,KAAK;QACzC,gBAAgB,IAAI,CAAC;IACvB;IAEA,OAAO;AACT;AAKO,eAAe,4BACpB,MAAc,EACd,QAAgB,EAAE;IAElB,uBAAuB;IACvB,MAAM,CAAC,YAAY,oBAAoB,GAAG,MAAM,QAAQ,GAAG,CAAC;QAC1D,yMAAM,CAAC,SAAS,CAAC,KAAK,CAAC;YACrB,OAAO;gBAAE;YAAO;QAClB;QACA,yMAAM,CAAC,SAAS,CAAC,QAAQ,CAAC;YACxB,OAAO;gBACL;gBACA,QAAQ;YACV;YACA,SAAS;gBAAE,SAAS;YAAO;YAC3B,MAAM;YACN,SAAS;gBACP,OAAO;YACT;QACF;KACD;IAED,MAAM,iBAAiB,oBAAoB,MAAM;IACjD,MAAM,uBAAuB,oBAAoB,MAAM,CAAC,CAAC,IAAM,EAAE,KAAK;IAEtE,4BAA4B;IAC5B,IAAI,WAAW;IACf,IAAI,YAAY;IAChB,MAAM,gBAA0B,EAAE;IAElC,KAAK,MAAM,aAAa,qBAAsB;QAC5C,YAAY,UAAU,KAAK,CAAE,YAAY;QACzC,IAAI,UAAU,KAAK,CAAE,UAAU,EAAE;QACjC,cAAc,IAAI,CAAC,UAAU,KAAK,CAAE,YAAY;IAClD;IAEA,IAAI,qBAAqB,MAAM,GAAG,GAAG;QACnC,YAAY,qBAAqB,MAAM;IACzC;IAEA,MAAM,WAAW,qBAAqB,MAAM,GAAG,IAC3C,AAAC,YAAY,qBAAqB,MAAM,GAAI,MAC5C;IAEJ,8BAA8B;IAC9B,MAAM,mBAAqD,CAAC;IAE5D,KAAK,MAAM,OAAO,eAAgB;QAChC,MAAM,SAAmB,EAAE;QAE3B,KAAK,MAAM,aAAa,qBAAsB;YAC5C,MAAM,QAAQ,cAAc,UAAU,KAAK,EAAG;YAC9C,IAAI,UAAU,WAAW;gBACvB,OAAO,IAAI,CAAC;YACd;QACF;QAEA,IAAI,OAAO,MAAM,GAAG,GAAG;YACrB,MAAM,SAAS,OAAO,MAAM,CAAC,CAAC,GAAG,IAAM,IAAI,GAAG,KAAK,OAAO,MAAM;YAChE,gBAAgB,CAAC,IAAI,GAAG;gBACtB,UAAU,OAAO,OAAO,OAAO,CAAC;gBAChC,OAAO,eAAe,OAAO,OAAO;gBACpC,QAAQ,SAAS;gBACjB,UAAU,UAAU;gBACpB,cAAc,OAAO,KAAK,CAAC,CAAC;YAC9B;QACF,OAAO;YACL,gBAAgB,CAAC,IAAI,GAAG;gBACtB,UAAU;gBACV,OAAO;gBACP,QAAQ;gBACR,UAAU;gBACV,cAAc,EAAE;YAClB;QACF;IACF;IAEA,sBAAsB;IACtB,MAAM,eAAiC,qBAAqB,GAAG,CAAC,CAAC,IAAM,CAAC;YACtE,MAAM,EAAE,OAAO,EAAE,iBAAiB,EAAE,SAAS,CAAC,WAAW;YACzD,cAAc,EAAE,KAAK,CAAE,YAAY;YACnC,YAAY,EAAE,KAAK,CAAE,UAAU;YAC/B,OAAO,EAAE,KAAK;YACd,aAAa,EAAE,EAAE;QACnB,CAAC;IAED,uBAAuB;IACvB,MAAM,UAA8B;QAClC,SAAS;YACP,iBAAiB;YACjB,qBAAqB;YACrB,UAAU,OAAO,SAAS,OAAO,CAAC;YAClC,UAAU,OAAO,SAAS,OAAO,CAAC;YAClC,YAAY,eAAe,cAAc,OAAO;QAClD;QACA,YAAY;QACZ;QACA,UAAU,EAAE;QACZ,iBAAiB,EAAE;IACrB;IAEA,wCAAwC;IACxC,QAAQ,QAAQ,GAAG,iBAAiB;IACpC,QAAQ,eAAe,GAAG,wBAAwB;IAElD,OAAO;AACT;AAKO,eAAe,yBAAyB,MAAc;IAC3D,MAAM,UAAU,MAAM,4BAA4B,QAAQ;IAE1D,qCAAqC;IACrC,MAAM,iBAAiB,OAAO,OAAO,CAAC,QAAQ,UAAU,EACrD,MAAM,CAAC,CAAC,GAAG,EAAE,GAAK,EAAE,MAAM,EAC1B,GAAG,CAAC,CAAC,CAAC,IAAI,GAAK;IAElB,MAAM,mBAAmB,OAAO,OAAO,CAAC,QAAQ,UAAU,EACvD,MAAM,CAAC,CAAC,GAAG,EAAE,GAAK,EAAE,QAAQ,EAC5B,GAAG,CAAC,CAAC,CAAC,IAAI,GAAK;IAElB,wBAAwB;IACxB,MAAM,kBAAkB,MAAM,yMAAM,CAAC,SAAS,CAAC,QAAQ,CAAC;QACtD,OAAO;YAAE;YAAQ,QAAQ;QAAY;QACrC,SAAS;YAAE,OAAO;QAAK;IACzB;IAEA,MAAM,aAAuF,CAAC;IAE9F,KAAK,MAAM,aAAa,gBAAiB;QACvC,IAAI,CAAC,UAAU,KAAK,EAAE;QAEtB,IAAI,CAAC,UAAU,CAAC,UAAU,KAAK,CAAC,EAAE;YAChC,UAAU,CAAC,UAAU,KAAK,CAAC,GAAG;gBAAE,OAAO;gBAAG,YAAY;gBAAG,WAAW;YAAE;QACxE;QAEA,UAAU,CAAC,UAAU,KAAK,CAAC,CAAC,KAAK;QACjC,UAAU,CAAC,UAAU,KAAK,CAAC,CAAC,UAAU,IAAI,UAAU,KAAK,CAAC,YAAY;QACtE,IAAI,UAAU,KAAK,CAAC,UAAU,EAAE;YAC9B,UAAU,CAAC,UAAU,KAAK,CAAC,CAAC,SAAS;QACvC;IACF;IAEA,0BAA0B;IAC1B,MAAM,kBAAyF,CAAC;IAChG,KAAK,MAAM,CAAC,OAAO,MAAM,IAAI,OAAO,OAAO,CAAC,YAAa;QACvD,eAAe,CAAC,MAAM,GAAG;YACvB,OAAO,MAAM,KAAK;YAClB,UAAU,OAAO,CAAC,MAAM,UAAU,GAAG,MAAM,KAAK,EAAE,OAAO,CAAC;YAC1D,UAAU,OAAO,CAAC,AAAC,MAAM,SAAS,GAAG,MAAM,KAAK,GAAI,GAAG,EAAE,OAAO,CAAC;QACnE;IACF;IAEA,0BAA0B;IAC1B,MAAM,yMAAM,CAAC,aAAa,CAAC,MAAM,CAAC;QAChC,OAAO;YAAE;QAAO;QAChB,QAAQ;YACN;YACA,iBAAiB,QAAQ,OAAO,CAAC,eAAe;YAChD,qBAAqB,QAAQ,OAAO,CAAC,mBAAmB;YACxD,iBAAiB,QAAQ,OAAO,CAAC,QAAQ;YACzC,UAAU,QAAQ,OAAO,CAAC,QAAQ;YAClC,gBAAgB,KAAK,SAAS,CAAC;YAC/B,kBAAkB,KAAK,SAAS,CAAC;YACjC,YAAY,KAAK,SAAS,CAAC;YAC3B,YAAY,QAAQ,OAAO,CAAC,UAAU;YACtC,kBAAkB,IAAI;QACxB;QACA,QAAQ;YACN,iBAAiB,QAAQ,OAAO,CAAC,eAAe;YAChD,qBAAqB,QAAQ,OAAO,CAAC,mBAAmB;YACxD,iBAAiB,QAAQ,OAAO,CAAC,QAAQ;YACzC,UAAU,QAAQ,OAAO,CAAC,QAAQ;YAClC,gBAAgB,KAAK,SAAS,CAAC;YAC/B,kBAAkB,KAAK,SAAS,CAAC;YACjC,YAAY,KAAK,SAAS,CAAC;YAC3B,YAAY,QAAQ,OAAO,CAAC,UAAU;YACtC,kBAAkB,IAAI;QACxB;IACF;AACF;AAKO,SAAS,uBACd,QAAgB;IAEhB,IAAI,WAAW,KAAK,OAAO;IAC3B,IAAI,WAAW,KAAK,OAAO;IAC3B,IAAI,WAAW,KAAK,OAAO;IAC3B,OAAO;AACT"}},
    {"offset": {"line": 2306, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/followup-engine.ts"],"sourcesContent":["/**\n * Follow-up Questions Engine\n *\n * Generates follow-up questions based on past weak points.\n * AI remembers previous attempts and asks about past weak points.\n */\n\nimport { prisma } from \"@/lib/prisma\";\nimport { identifyWeakAreas, calculatePerformanceMetrics } from \"./analytics/performance-analytics\";\n\n// Score dimension type for type-safe access\ntype ScoreDimension =\n  | \"requirementsClarification\"\n  | \"highLevelDesign\"\n  | \"detailedDesign\"\n  | \"scalability\"\n  | \"tradeoffs\"\n  | \"communication\";\n\n// Helper to get score value from dimension key\nfunction getScoreValue(\n  score: {\n    requirementsClarification: number;\n    highLevelDesign: number;\n    detailedDesign: number;\n    scalability: number;\n    tradeoffs: number;\n    communication: number;\n  },\n  dimension: ScoreDimension\n): number {\n  return score[dimension];\n}\n\n// Types\nexport interface WeakPoint {\n  dimension: string;\n  dimensionKey: string;\n  topic: string;\n  concept: string;\n  lastScore: number;\n  occurrences: number;\n  trend: \"improving\" | \"stable\" | \"declining\";\n}\n\nexport interface FollowUpContext {\n  weakPoints: WeakPoint[];\n  topicsToReinforce: string[];\n  conceptsToProbe: string[];\n  adaptedDifficulty: \"easier\" | \"standard\" | \"harder\";\n  hasHistory: boolean;\n  totalPastInterviews: number;\n}\n\n// Dimension to concept mapping\nconst DIMENSION_CONCEPTS: Record<string, string[]> = {\n  requirementsClarification: [\n    \"scale estimation (DAU, QPS)\",\n    \"functional vs non-functional requirements\",\n    \"latency and availability requirements\",\n    \"data consistency requirements\",\n    \"feature prioritization\",\n  ],\n  highLevelDesign: [\n    \"API design and endpoints\",\n    \"component architecture\",\n    \"database selection\",\n    \"data flow between components\",\n    \"service boundaries\",\n  ],\n  detailedDesign: [\n    \"data models and schemas\",\n    \"algorithm choices\",\n    \"caching strategies\",\n    \"protocol selection\",\n    \"failure handling\",\n  ],\n  scalability: [\n    \"horizontal vs vertical scaling\",\n    \"database sharding\",\n    \"load balancing strategies\",\n    \"caching layers\",\n    \"CDN usage\",\n  ],\n  tradeoffs: [\n    \"CAP theorem implications\",\n    \"consistency vs availability\",\n    \"cost vs performance\",\n    \"latency vs throughput\",\n    \"complexity vs maintainability\",\n  ],\n  communication: [\n    \"structured thinking\",\n    \"clear explanations\",\n    \"diagram usage\",\n    \"time management\",\n    \"handling clarifications\",\n  ],\n};\n\n// Dimension names\nconst DIMENSION_NAMES: Record<string, string> = {\n  requirementsClarification: \"Requirements Clarification\",\n  highLevelDesign: \"High-Level Design\",\n  detailedDesign: \"Detailed Design\",\n  scalability: \"Scalability\",\n  tradeoffs: \"Trade-offs\",\n  communication: \"Communication\",\n};\n\n/**\n * Get weak points for a user based on past interview performance\n */\nexport async function getWeakPointsForUser(\n  userId: string,\n  limit: number = 10\n): Promise<WeakPoint[]> {\n  const weakAreas = await identifyWeakAreas(userId, limit);\n\n  // Convert weak areas to weak points with concepts\n  const weakPoints: WeakPoint[] = [];\n\n  for (const area of weakAreas) {\n    // Find the dimension key from the display name\n    const dimensionKey = Object.entries(DIMENSION_NAMES).find(\n      ([, name]) => name === area.dimension\n    )?.[0];\n\n    if (!dimensionKey) continue;\n\n    const concepts = DIMENSION_CONCEPTS[dimensionKey] || [];\n\n    // Select 1-2 relevant concepts to probe based on the weak area\n    const conceptsToProbe = concepts.slice(0, 2);\n\n    weakPoints.push({\n      dimension: area.dimension,\n      dimensionKey,\n      topic: \"\", // Will be filled with specific topic if available\n      concept: conceptsToProbe.join(\", \"),\n      lastScore: area.lastScore,\n      occurrences: area.occurrences,\n      trend: area.trend,\n    });\n  }\n\n  return weakPoints;\n}\n\n/**\n * Build follow-up context for a new interview\n */\nexport async function buildFollowUpContext(\n  userId: string,\n  currentTopic: string\n): Promise<FollowUpContext> {\n  // Get performance metrics\n  const metrics = await calculatePerformanceMetrics(userId, 15);\n\n  // Get weak points\n  const weakPoints = await getWeakPointsForUser(userId, 10);\n\n  // Get topics that need reinforcement\n  const topicInterviews = await prisma.interview.findMany({\n    where: {\n      userId,\n      status: \"completed\",\n    },\n    include: { score: true },\n    orderBy: { endedAt: \"desc\" },\n    take: 20,\n  });\n\n  // Find topics with low pass rates\n  const topicStats: Record<string, { total: number; passed: number }> = {};\n\n  for (const interview of topicInterviews) {\n    if (!interview.score) continue;\n\n    if (!topicStats[interview.topic]) {\n      topicStats[interview.topic] = { total: 0, passed: 0 };\n    }\n\n    topicStats[interview.topic].total++;\n    if (interview.score.passStatus) {\n      topicStats[interview.topic].passed++;\n    }\n  }\n\n  const topicsToReinforce = Object.entries(topicStats)\n    .filter(([, stats]) => stats.total >= 2 && stats.passed / stats.total < 0.5)\n    .map(([topic]) => topic);\n\n  // Determine concepts to probe based on weak points\n  const conceptsToProbe: string[] = [];\n\n  for (const wp of weakPoints.slice(0, 3)) {\n    const concepts = DIMENSION_CONCEPTS[wp.dimensionKey] || [];\n    conceptsToProbe.push(...concepts.slice(0, 2));\n  }\n\n  // Determine difficulty adaptation\n  let adaptedDifficulty: \"easier\" | \"standard\" | \"harder\" = \"standard\";\n\n  if (metrics.overall.avgScore < 2.0) {\n    adaptedDifficulty = \"easier\";\n  } else if (metrics.overall.avgScore >= 3.0 && metrics.overall.passRate >= 70) {\n    adaptedDifficulty = \"harder\";\n  }\n\n  return {\n    weakPoints,\n    topicsToReinforce,\n    conceptsToProbe: [...new Set(conceptsToProbe)], // Remove duplicates\n    adaptedDifficulty,\n    hasHistory: metrics.overall.completedInterviews > 0,\n    totalPastInterviews: metrics.overall.completedInterviews,\n  };\n}\n\n/**\n * Generate prompt addition for follow-up context\n */\nexport function generateFollowUpPromptAddition(context: FollowUpContext): string {\n  if (!context.hasHistory || context.weakPoints.length === 0) {\n    return \"\";\n  }\n\n  let prompt = `\n## Candidate's Past Performance Context\n\nThis candidate has completed ${context.totalPastInterviews} previous interview${context.totalPastInterviews === 1 ? \"\" : \"s\"}.\n\n### Weak Areas to Focus On\n`;\n\n  for (const wp of context.weakPoints.slice(0, 3)) {\n    prompt += `- **${wp.dimension}** (avg score: ${wp.lastScore}/4, trend: ${wp.trend})\n  - Concepts to probe: ${wp.concept}\n`;\n  }\n\n  if (context.conceptsToProbe.length > 0) {\n    prompt += `\n### Specific Concepts to Cover\nDuring the interview, make sure to ask about:\n${context.conceptsToProbe.map((c) => `- ${c}`).join(\"\\n\")}\n`;\n  }\n\n  if (context.topicsToReinforce.length > 0) {\n    prompt += `\n### Related Topics With Low Performance\nThe candidate has struggled with these related topics: ${context.topicsToReinforce.join(\", \")}\nConsider asking questions that reinforce fundamentals from these areas.\n`;\n  }\n\n  prompt += `\n### Adaptation Instructions\n- Difficulty adaptation: ${context.adaptedDifficulty}\n${context.adaptedDifficulty === \"easier\" ? \"- Provide more hints and guidance for this candidate\" : \"\"}\n${context.adaptedDifficulty === \"harder\" ? \"- Challenge this candidate with edge cases and deeper probing\" : \"\"}\n- When the candidate answers questions related to their weak areas, probe deeper\n- Acknowledge improvement if they perform better on previously weak dimensions\n`;\n\n  return prompt;\n}\n\n/**\n * Track improvement on a specific concept (for future enhancement)\n */\nexport async function trackConceptImprovement(\n  userId: string,\n  interviewId: string,\n  dimension: string,\n  newScore: number\n): Promise<{ improved: boolean; previousScore: number | null }> {\n  // Get the previous score for this dimension\n  const previousInterviews = await prisma.interview.findMany({\n    where: {\n      userId,\n      status: \"completed\",\n      id: { not: interviewId },\n    },\n    include: { score: true },\n    orderBy: { endedAt: \"desc\" },\n    take: 5,\n  });\n\n  const previousScores = previousInterviews\n    .filter((i) => i.score)\n    .map((i) => getScoreValue(i.score!, dimension as ScoreDimension))\n    .filter((s) => s !== undefined);\n\n  if (previousScores.length === 0) {\n    return { improved: false, previousScore: null };\n  }\n\n  const previousAvg = previousScores.reduce((a, b) => a + b, 0) / previousScores.length;\n\n  return {\n    improved: newScore > previousAvg,\n    previousScore: Number(previousAvg.toFixed(2)),\n  };\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA;;;;;CAKC,GAED;AACA;;;;;;;;AAWA,+CAA+C;AAC/C,SAAS,cACP,KAOC,EACD,SAAyB;IAEzB,OAAO,KAAK,CAAC,UAAU;AACzB;AAsBA,+BAA+B;AAC/B,MAAM,qBAA+C;IACnD,2BAA2B;QACzB;QACA;QACA;QACA;QACA;KACD;IACD,iBAAiB;QACf;QACA;QACA;QACA;QACA;KACD;IACD,gBAAgB;QACd;QACA;QACA;QACA;QACA;KACD;IACD,aAAa;QACX;QACA;QACA;QACA;QACA;KACD;IACD,WAAW;QACT;QACA;QACA;QACA;QACA;KACD;IACD,eAAe;QACb;QACA;QACA;QACA;QACA;KACD;AACH;AAEA,kBAAkB;AAClB,MAAM,kBAA0C;IAC9C,2BAA2B;IAC3B,iBAAiB;IACjB,gBAAgB;IAChB,aAAa;IACb,WAAW;IACX,eAAe;AACjB;AAKO,eAAe,qBACpB,MAAc,EACd,QAAgB,EAAE;IAElB,MAAM,YAAY,MAAM,IAAA,mPAAiB,EAAC,QAAQ;IAElD,kDAAkD;IAClD,MAAM,aAA0B,EAAE;IAElC,KAAK,MAAM,QAAQ,UAAW;QAC5B,+CAA+C;QAC/C,MAAM,eAAe,OAAO,OAAO,CAAC,iBAAiB,IAAI,CACvD,CAAC,GAAG,KAAK,GAAK,SAAS,KAAK,SAAS,GACpC,CAAC,EAAE;QAEN,IAAI,CAAC,cAAc;QAEnB,MAAM,WAAW,kBAAkB,CAAC,aAAa,IAAI,EAAE;QAEvD,+DAA+D;QAC/D,MAAM,kBAAkB,SAAS,KAAK,CAAC,GAAG;QAE1C,WAAW,IAAI,CAAC;YACd,WAAW,KAAK,SAAS;YACzB;YACA,OAAO;YACP,SAAS,gBAAgB,IAAI,CAAC;YAC9B,WAAW,KAAK,SAAS;YACzB,aAAa,KAAK,WAAW;YAC7B,OAAO,KAAK,KAAK;QACnB;IACF;IAEA,OAAO;AACT;AAKO,eAAe,qBACpB,MAAc,EACd,YAAoB;IAEpB,0BAA0B;IAC1B,MAAM,UAAU,MAAM,IAAA,6PAA2B,EAAC,QAAQ;IAE1D,kBAAkB;IAClB,MAAM,aAAa,MAAM,qBAAqB,QAAQ;IAEtD,qCAAqC;IACrC,MAAM,kBAAkB,MAAM,yMAAM,CAAC,SAAS,CAAC,QAAQ,CAAC;QACtD,OAAO;YACL;YACA,QAAQ;QACV;QACA,SAAS;YAAE,OAAO;QAAK;QACvB,SAAS;YAAE,SAAS;QAAO;QAC3B,MAAM;IACR;IAEA,kCAAkC;IAClC,MAAM,aAAgE,CAAC;IAEvE,KAAK,MAAM,aAAa,gBAAiB;QACvC,IAAI,CAAC,UAAU,KAAK,EAAE;QAEtB,IAAI,CAAC,UAAU,CAAC,UAAU,KAAK,CAAC,EAAE;YAChC,UAAU,CAAC,UAAU,KAAK,CAAC,GAAG;gBAAE,OAAO;gBAAG,QAAQ;YAAE;QACtD;QAEA,UAAU,CAAC,UAAU,KAAK,CAAC,CAAC,KAAK;QACjC,IAAI,UAAU,KAAK,CAAC,UAAU,EAAE;YAC9B,UAAU,CAAC,UAAU,KAAK,CAAC,CAAC,MAAM;QACpC;IACF;IAEA,MAAM,oBAAoB,OAAO,OAAO,CAAC,YACtC,MAAM,CAAC,CAAC,GAAG,MAAM,GAAK,MAAM,KAAK,IAAI,KAAK,MAAM,MAAM,GAAG,MAAM,KAAK,GAAG,KACvE,GAAG,CAAC,CAAC,CAAC,MAAM,GAAK;IAEpB,mDAAmD;IACnD,MAAM,kBAA4B,EAAE;IAEpC,KAAK,MAAM,MAAM,WAAW,KAAK,CAAC,GAAG,GAAI;QACvC,MAAM,WAAW,kBAAkB,CAAC,GAAG,YAAY,CAAC,IAAI,EAAE;QAC1D,gBAAgB,IAAI,IAAI,SAAS,KAAK,CAAC,GAAG;IAC5C;IAEA,kCAAkC;IAClC,IAAI,oBAAsD;IAE1D,IAAI,QAAQ,OAAO,CAAC,QAAQ,GAAG,KAAK;QAClC,oBAAoB;IACtB,OAAO,IAAI,QAAQ,OAAO,CAAC,QAAQ,IAAI,OAAO,QAAQ,OAAO,CAAC,QAAQ,IAAI,IAAI;QAC5E,oBAAoB;IACtB;IAEA,OAAO;QACL;QACA;QACA,iBAAiB;eAAI,IAAI,IAAI;SAAiB;QAC9C;QACA,YAAY,QAAQ,OAAO,CAAC,mBAAmB,GAAG;QAClD,qBAAqB,QAAQ,OAAO,CAAC,mBAAmB;IAC1D;AACF;AAKO,SAAS,+BAA+B,OAAwB;IACrE,IAAI,CAAC,QAAQ,UAAU,IAAI,QAAQ,UAAU,CAAC,MAAM,KAAK,GAAG;QAC1D,OAAO;IACT;IAEA,IAAI,SAAS,CAAC;;;6BAGa,EAAE,QAAQ,mBAAmB,CAAC,mBAAmB,EAAE,QAAQ,mBAAmB,KAAK,IAAI,KAAK,IAAI;;;AAG7H,CAAC;IAEC,KAAK,MAAM,MAAM,QAAQ,UAAU,CAAC,KAAK,CAAC,GAAG,GAAI;QAC/C,UAAU,CAAC,IAAI,EAAE,GAAG,SAAS,CAAC,eAAe,EAAE,GAAG,SAAS,CAAC,WAAW,EAAE,GAAG,KAAK,CAAC;uBAC/D,EAAE,GAAG,OAAO,CAAC;AACpC,CAAC;IACC;IAEA,IAAI,QAAQ,eAAe,CAAC,MAAM,GAAG,GAAG;QACtC,UAAU,CAAC;;;AAGf,EAAE,QAAQ,eAAe,CAAC,GAAG,CAAC,CAAC,IAAM,CAAC,EAAE,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;AAC1D,CAAC;IACC;IAEA,IAAI,QAAQ,iBAAiB,CAAC,MAAM,GAAG,GAAG;QACxC,UAAU,CAAC;;uDAEwC,EAAE,QAAQ,iBAAiB,CAAC,IAAI,CAAC,MAAM;;AAE9F,CAAC;IACC;IAEA,UAAU,CAAC;;yBAEY,EAAE,QAAQ,iBAAiB,CAAC;AACrD,EAAE,QAAQ,iBAAiB,KAAK,WAAW,yDAAyD,GAAG;AACvG,EAAE,QAAQ,iBAAiB,KAAK,WAAW,kEAAkE,GAAG;;;AAGhH,CAAC;IAEC,OAAO;AACT;AAKO,eAAe,wBACpB,MAAc,EACd,WAAmB,EACnB,SAAiB,EACjB,QAAgB;IAEhB,4CAA4C;IAC5C,MAAM,qBAAqB,MAAM,yMAAM,CAAC,SAAS,CAAC,QAAQ,CAAC;QACzD,OAAO;YACL;YACA,QAAQ;YACR,IAAI;gBAAE,KAAK;YAAY;QACzB;QACA,SAAS;YAAE,OAAO;QAAK;QACvB,SAAS;YAAE,SAAS;QAAO;QAC3B,MAAM;IACR;IAEA,MAAM,iBAAiB,mBACpB,MAAM,CAAC,CAAC,IAAM,EAAE,KAAK,EACrB,GAAG,CAAC,CAAC,IAAM,cAAc,EAAE,KAAK,EAAG,YACnC,MAAM,CAAC,CAAC,IAAM,MAAM;IAEvB,IAAI,eAAe,MAAM,KAAK,GAAG;QAC/B,OAAO;YAAE,UAAU;YAAO,eAAe;QAAK;IAChD;IAEA,MAAM,cAAc,eAAe,MAAM,CAAC,CAAC,GAAG,IAAM,IAAI,GAAG,KAAK,eAAe,MAAM;IAErF,OAAO;QACL,UAAU,WAAW;QACrB,eAAe,OAAO,YAAY,OAAO,CAAC;IAC5C;AACF"}},
    {"offset": {"line": 2548, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/ai-personality-adapter.ts"],"sourcesContent":["/**\n * AI Personality Adapter\n *\n * Adapts Bobby's interviewer personality based on user performance.\n * Adjusts encouragement level, challenge intensity, and questioning style.\n */\n\nimport {\n  calculatePerformanceMetrics,\n  getPerformanceCategory,\n} from \"./analytics/performance-analytics\";\n\n// Types\nexport interface PersonalitySettings {\n  encouragementLevel: \"high\" | \"moderate\" | \"low\";\n  challengeLevel: \"gentle\" | \"standard\" | \"rigorous\";\n  hintFrequency: \"frequent\" | \"occasional\" | \"rare\";\n  probingDepth: \"surface\" | \"moderate\" | \"deep\";\n  paceAdjustment: \"slower\" | \"standard\" | \"faster\";\n}\n\nexport interface AdaptedPersonality {\n  settings: PersonalitySettings;\n  promptModifications: string;\n  reasonForAdaptation: string;\n  performanceCategory: \"struggling\" | \"developing\" | \"proficient\" | \"excelling\";\n}\n\n// Personality presets based on performance category\nconst PERSONALITY_PRESETS: Record<string, PersonalitySettings> = {\n  struggling: {\n    encouragementLevel: \"high\",\n    challengeLevel: \"gentle\",\n    hintFrequency: \"frequent\",\n    probingDepth: \"surface\",\n    paceAdjustment: \"slower\",\n  },\n  developing: {\n    encouragementLevel: \"moderate\",\n    challengeLevel: \"gentle\",\n    hintFrequency: \"occasional\",\n    probingDepth: \"moderate\",\n    paceAdjustment: \"standard\",\n  },\n  proficient: {\n    encouragementLevel: \"moderate\",\n    challengeLevel: \"standard\",\n    hintFrequency: \"occasional\",\n    probingDepth: \"moderate\",\n    paceAdjustment: \"standard\",\n  },\n  excelling: {\n    encouragementLevel: \"low\",\n    challengeLevel: \"rigorous\",\n    hintFrequency: \"rare\",\n    probingDepth: \"deep\",\n    paceAdjustment: \"faster\",\n  },\n};\n\n/**\n * Calculate personality adaptation based on user's performance history\n */\nexport async function calculatePersonalityAdaptation(\n  userId: string\n): Promise<AdaptedPersonality> {\n  const metrics = await calculatePerformanceMetrics(userId, 10);\n  const category = getPerformanceCategory(metrics.overall.avgScore);\n  const settings = { ...PERSONALITY_PRESETS[category] };\n\n  // Fine-tune based on specific metrics\n  if (metrics.overall.scoreTrend === \"declining\" && category !== \"struggling\") {\n    // If declining, be more supportive\n    settings.encouragementLevel = \"high\";\n    settings.hintFrequency = \"occasional\";\n  }\n\n  if (metrics.overall.scoreTrend === \"improving\" && category === \"struggling\") {\n    // If improving from a low base, acknowledge progress\n    settings.encouragementLevel = \"high\";\n    settings.challengeLevel = \"gentle\";\n  }\n\n  // Build prompt modifications\n  const promptModifications = buildPersonalityPrompt(settings, category, metrics);\n\n  // Build reason\n  const reasonForAdaptation = buildAdaptationReason(category, metrics);\n\n  return {\n    settings,\n    promptModifications,\n    reasonForAdaptation,\n    performanceCategory: category,\n  };\n}\n\n/**\n * Build personality-adapted prompt section\n */\nfunction buildPersonalityPrompt(\n  settings: PersonalitySettings,\n  category: string,\n  metrics: Awaited<ReturnType<typeof calculatePerformanceMetrics>>\n): string {\n  let prompt = `\n## Bobby's Adapted Personality for This Candidate\n\nBased on this candidate's performance history, adapt your interviewer behavior:\n\n### Encouragement Style (${settings.encouragementLevel})\n`;\n\n  switch (settings.encouragementLevel) {\n    case \"high\":\n      prompt += `- Be extra supportive and positive\n- Celebrate good points with phrases like \"That's a great insight!\" or \"Excellent thinking!\"\n- When they struggle, say things like \"That's okay, let me help guide you...\"\n- Focus on building confidence\n`;\n      break;\n    case \"moderate\":\n      prompt += `- Acknowledge good points with brief praise like \"Good\" or \"That works\"\n- Stay professional and constructive\n- Offer encouragement when appropriate\n`;\n      break;\n    case \"low\":\n      prompt += `- Focus on technical accuracy over praise\n- Keep feedback objective and direct\n- Save encouragement for truly exceptional answers\n`;\n      break;\n  }\n\n  prompt += `\n### Challenge Level (${settings.challengeLevel})\n`;\n\n  switch (settings.challengeLevel) {\n    case \"gentle\":\n      prompt += `- Start with fundamental questions before advancing\n- Break complex topics into smaller parts\n- Give them time to think and don't rush\n- If they struggle, simplify the question\n`;\n      break;\n    case \"standard\":\n      prompt += `- Ask balanced questions that test understanding\n- Probe on tradeoffs and alternatives\n- Challenge decisions but accept reasonable answers\n`;\n      break;\n    case \"rigorous\":\n      prompt += `- Push hard on edge cases and failure scenarios\n- Ask follow-up questions that test depth\n- Challenge assumptions and expect precise answers\n- Don't accept vague answers - probe deeper\n`;\n      break;\n  }\n\n  prompt += `\n### Hints and Guidance (${settings.hintFrequency})\n`;\n\n  switch (settings.hintFrequency) {\n    case \"frequent\":\n      prompt += `- Provide hints proactively when they seem stuck\n- Offer suggestions like \"Have you considered...?\" or \"What about...?\"\n- Guide them towards good solutions\n`;\n      break;\n    case \"occasional\":\n      prompt += `- Give hints if they're clearly stuck after a reasonable attempt\n- Let them work through problems independently first\n`;\n      break;\n    case \"rare\":\n      prompt += `- Only hint if absolutely necessary\n- Let them struggle and find solutions independently\n- They should demonstrate they can problem-solve\n`;\n      break;\n  }\n\n  prompt += `\n### Probing Depth (${settings.probingDepth})\n`;\n\n  switch (settings.probingDepth) {\n    case \"surface\":\n      prompt += `- Focus on high-level understanding\n- Don't dive too deep into implementation details\n- Accept general answers for complex topics\n`;\n      break;\n    case \"moderate\":\n      prompt += `- Balance high-level and detailed questions\n- Probe on important design decisions\n`;\n      break;\n    case \"deep\":\n      prompt += `- Expect detailed explanations for all decisions\n- Ask about implementation specifics, data structures, algorithms\n- Test knowledge of edge cases and failure modes\n`;\n      break;\n  }\n\n  // Add performance context\n  if (metrics.overall.completedInterviews > 0) {\n    prompt += `\n### Performance Context\n- Past interviews: ${metrics.overall.completedInterviews}\n- Average score: ${metrics.overall.avgScore}/4.0\n- Pass rate: ${metrics.overall.passRate}%\n- Trend: ${metrics.overall.scoreTrend}\n`;\n  }\n\n  return prompt;\n}\n\n/**\n * Build human-readable reason for adaptation\n */\nfunction buildAdaptationReason(\n  category: string,\n  metrics: Awaited<ReturnType<typeof calculatePerformanceMetrics>>\n): string {\n  if (metrics.overall.completedInterviews === 0) {\n    return \"First interview - using standard settings.\";\n  }\n\n  switch (category) {\n    case \"struggling\":\n      return `Candidate's average score (${metrics.overall.avgScore}/4) indicates they need more support. Bobby will be more encouraging and provide more guidance.`;\n    case \"developing\":\n      return `Candidate is developing (${metrics.overall.avgScore}/4). Bobby will balance support with challenge.`;\n    case \"proficient\":\n      return `Candidate shows proficiency (${metrics.overall.avgScore}/4). Bobby will use standard interview approach.`;\n    case \"excelling\":\n      return `Candidate is excelling (${metrics.overall.avgScore}/4). Bobby will be more rigorous and challenging.`;\n    default:\n      return \"Using standard interview settings.\";\n  }\n}\n\n/**\n * Build adapted system prompt by combining base prompt with personality modifications\n */\nexport function buildAdaptedSystemPrompt(\n  basePrompt: string,\n  personality: AdaptedPersonality\n): string {\n  // Insert personality modifications after the base interviewer description\n  const insertPoint = basePrompt.indexOf(\"## Interview Structure\");\n\n  if (insertPoint === -1) {\n    // If we can't find the structure section, append at the end\n    return `${basePrompt}\\n\\n${personality.promptModifications}`;\n  }\n\n  return (\n    basePrompt.slice(0, insertPoint) +\n    personality.promptModifications +\n    \"\\n\\n\" +\n    basePrompt.slice(insertPoint)\n  );\n}\n\n/**\n * Get default personality settings for users without history\n */\nexport function getDefaultPersonalitySettings(): PersonalitySettings {\n  return {\n    encouragementLevel: \"moderate\",\n    challengeLevel: \"standard\",\n    hintFrequency: \"occasional\",\n    probingDepth: \"moderate\",\n    paceAdjustment: \"standard\",\n  };\n}\n\n/**\n * Re-export performance category function\n */\nexport { getPerformanceCategory } from \"./analytics/performance-analytics\";\n"],"names":[],"mappings":";;;;;;;;AAAA;;;;;CAKC,GAED;;;;;;AAqBA,oDAAoD;AACpD,MAAM,sBAA2D;IAC/D,YAAY;QACV,oBAAoB;QACpB,gBAAgB;QAChB,eAAe;QACf,cAAc;QACd,gBAAgB;IAClB;IACA,YAAY;QACV,oBAAoB;QACpB,gBAAgB;QAChB,eAAe;QACf,cAAc;QACd,gBAAgB;IAClB;IACA,YAAY;QACV,oBAAoB;QACpB,gBAAgB;QAChB,eAAe;QACf,cAAc;QACd,gBAAgB;IAClB;IACA,WAAW;QACT,oBAAoB;QACpB,gBAAgB;QAChB,eAAe;QACf,cAAc;QACd,gBAAgB;IAClB;AACF;AAKO,eAAe,+BACpB,MAAc;IAEd,MAAM,UAAU,MAAM,IAAA,6PAA2B,EAAC,QAAQ;IAC1D,MAAM,WAAW,IAAA,wPAAsB,EAAC,QAAQ,OAAO,CAAC,QAAQ;IAChE,MAAM,WAAW;QAAE,GAAG,mBAAmB,CAAC,SAAS;IAAC;IAEpD,sCAAsC;IACtC,IAAI,QAAQ,OAAO,CAAC,UAAU,KAAK,eAAe,aAAa,cAAc;QAC3E,mCAAmC;QACnC,SAAS,kBAAkB,GAAG;QAC9B,SAAS,aAAa,GAAG;IAC3B;IAEA,IAAI,QAAQ,OAAO,CAAC,UAAU,KAAK,eAAe,aAAa,cAAc;QAC3E,qDAAqD;QACrD,SAAS,kBAAkB,GAAG;QAC9B,SAAS,cAAc,GAAG;IAC5B;IAEA,6BAA6B;IAC7B,MAAM,sBAAsB,uBAAuB,UAAU,UAAU;IAEvE,eAAe;IACf,MAAM,sBAAsB,sBAAsB,UAAU;IAE5D,OAAO;QACL;QACA;QACA;QACA,qBAAqB;IACvB;AACF;AAEA;;CAEC,GACD,SAAS,uBACP,QAA6B,EAC7B,QAAgB,EAChB,OAAgE;IAEhE,IAAI,SAAS,CAAC;;;;;yBAKS,EAAE,SAAS,kBAAkB,CAAC;AACvD,CAAC;IAEC,OAAQ,SAAS,kBAAkB;QACjC,KAAK;YACH,UAAU,CAAC;;;;AAIjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;IACJ;IAEA,UAAU,CAAC;qBACQ,EAAE,SAAS,cAAc,CAAC;AAC/C,CAAC;IAEC,OAAQ,SAAS,cAAc;QAC7B,KAAK;YACH,UAAU,CAAC;;;;AAIjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;;;AAIjB,CAAC;YACK;IACJ;IAEA,UAAU,CAAC;wBACW,EAAE,SAAS,aAAa,CAAC;AACjD,CAAC;IAEC,OAAQ,SAAS,aAAa;QAC5B,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;AAEjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;IACJ;IAEA,UAAU,CAAC;mBACM,EAAE,SAAS,YAAY,CAAC;AAC3C,CAAC;IAEC,OAAQ,SAAS,YAAY;QAC3B,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;AAEjB,CAAC;YACK;QACF,KAAK;YACH,UAAU,CAAC;;;AAGjB,CAAC;YACK;IACJ;IAEA,0BAA0B;IAC1B,IAAI,QAAQ,OAAO,CAAC,mBAAmB,GAAG,GAAG;QAC3C,UAAU,CAAC;;mBAEI,EAAE,QAAQ,OAAO,CAAC,mBAAmB,CAAC;iBACxC,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC;aAC/B,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC;SAC/B,EAAE,QAAQ,OAAO,CAAC,UAAU,CAAC;AACtC,CAAC;IACC;IAEA,OAAO;AACT;AAEA;;CAEC,GACD,SAAS,sBACP,QAAgB,EAChB,OAAgE;IAEhE,IAAI,QAAQ,OAAO,CAAC,mBAAmB,KAAK,GAAG;QAC7C,OAAO;IACT;IAEA,OAAQ;QACN,KAAK;YACH,OAAO,CAAC,2BAA2B,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,+FAA+F,CAAC;QAChK,KAAK;YACH,OAAO,CAAC,yBAAyB,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,+CAA+C,CAAC;QAC9G,KAAK;YACH,OAAO,CAAC,6BAA6B,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,gDAAgD,CAAC;QACnH,KAAK;YACH,OAAO,CAAC,wBAAwB,EAAE,QAAQ,OAAO,CAAC,QAAQ,CAAC,iDAAiD,CAAC;QAC/G;YACE,OAAO;IACX;AACF;AAKO,SAAS,yBACd,UAAkB,EAClB,WAA+B;IAE/B,0EAA0E;IAC1E,MAAM,cAAc,WAAW,OAAO,CAAC;IAEvC,IAAI,gBAAgB,CAAC,GAAG;QACtB,4DAA4D;QAC5D,OAAO,GAAG,WAAW,IAAI,EAAE,YAAY,mBAAmB,EAAE;IAC9D;IAEA,OACE,WAAW,KAAK,CAAC,GAAG,eACpB,YAAY,mBAAmB,GAC/B,SACA,WAAW,KAAK,CAAC;AAErB;AAKO,SAAS;IACd,OAAO;QACL,oBAAoB;QACpB,gBAAgB;QAChB,eAAe;QACf,cAAc;QACd,gBAAgB;IAClB;AACF"}},
    {"offset": {"line": 2783, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/prompts/interviewer.ts"],"sourcesContent":["import { PersonalitySettings, AdaptedPersonality } from \"../ai-personality-adapter\";\nimport { FollowUpContext, generateFollowUpPromptAddition } from \"../followup-engine\";\n\nexport const INTERVIEW_PHASES = [\n  {\n    id: \"requirements\",\n    name: \"Requirements Clarification\",\n    duration: 8, // minutes\n    startTime: 45,\n    endTime: 37,\n    description: \"Understand the problem scope, scale, and constraints\",\n    keyPoints: [\n      \"Clarify functional requirements\",\n      \"Understand scale (users, QPS, data size)\",\n      \"Identify non-functional requirements (latency, availability)\",\n      \"Prioritize features\",\n    ],\n  },\n  {\n    id: \"high-level\",\n    name: \"High-Level Design\",\n    duration: 12,\n    startTime: 37,\n    endTime: 25,\n    description: \"Design the overall system architecture\",\n    keyPoints: [\n      \"Draw core components\",\n      \"Define APIs\",\n      \"Explain data flow\",\n      \"Choose databases and storage\",\n    ],\n  },\n  {\n    id: \"deep-dive\",\n    name: \"Deep Dive\",\n    duration: 12,\n    startTime: 25,\n    endTime: 13,\n    description: \"Explore critical components in detail\",\n    keyPoints: [\n      \"Data models and schemas\",\n      \"Algorithms and protocols\",\n      \"Failure handling\",\n      \"Edge cases\",\n    ],\n  },\n  {\n    id: \"scalability\",\n    name: \"Scalability & Trade-offs\",\n    duration: 10,\n    startTime: 13,\n    endTime: 3,\n    description: \"Discuss scaling and architectural decisions\",\n    keyPoints: [\n      \"Handle 10x-100x growth\",\n      \"Caching strategies\",\n      \"CAP theorem implications\",\n      \"Cost vs performance\",\n    ],\n  },\n  {\n    id: \"wrapup\",\n    name: \"Wrap-up\",\n    duration: 3,\n    startTime: 3,\n    endTime: 0,\n    description: \"Final questions and summary\",\n    keyPoints: [\n      \"Address remaining concerns\",\n      \"Candidate questions\",\n      \"Summary of design\",\n    ],\n  },\n] as const;\n\nexport type InterviewPhase = typeof INTERVIEW_PHASES[number][\"id\"];\n\nexport const INTERVIEWER_SYSTEM_PROMPT = `You are Bobby, a friendly and experienced FAANG system design interviewer with 10+ years of experience at companies like Google, Meta, Amazon, Netflix, and Uber. Your role is to conduct a realistic 45-minute system design interview.\n\n## Your Personality as Bobby\n- Be warm, encouraging, and professional\n- Use a conversational tone while maintaining technical rigor\n- Occasionally use phrases like \"Great point!\", \"That's interesting...\", \"I like that approach\"\n- Introduce yourself as Bobby at the start of the interview\n\n## Interview Structure (45 minutes total)\n\n### Phase 1: Requirements Clarification (Minutes 45:00 - 37:00) [8 min]\n- Ask about scale (DAU, requests/sec, data size)\n- Clarify functional vs non-functional requirements\n- Understand use cases and priorities\n- Key questions:\n  * \"What's the expected scale? DAU, requests per second?\"\n  * \"What are the most critical features we need to support?\"\n  * \"What are our latency requirements?\"\n  * \"Do we need strong consistency or is eventual consistency okay?\"\n\n### Phase 2: High-Level Design (Minutes 37:00 - 25:00) [12 min]\n- Guide them to draw core components\n- Discuss API design\n- Cover data flow between components\n- Key questions:\n  * \"Can you walk me through the high-level architecture?\"\n  * \"What databases would you use and why?\"\n  * \"How would data flow from client to storage?\"\n\n### Phase 3: Deep Dive (Minutes 25:00 - 13:00) [12 min]\n- Pick 2-3 critical components to explore in detail\n- Ask about data models, algorithms, protocols\n- Discuss failure scenarios and edge cases\n- Key questions:\n  * \"Let's dive deeper into [component]. How would you implement it?\"\n  * \"What happens if [failure scenario]?\"\n  * \"How would you handle [edge case]?\"\n\n### Phase 4: Scalability & Trade-offs (Minutes 13:00 - 3:00) [10 min]\n- How does it handle 10x, 100x scale?\n- CAP theorem implications\n- Cost vs performance tradeoffs\n- Key questions:\n  * \"How would this design change if we had 100x the users?\"\n  * \"What are the bottlenecks in this design?\"\n  * \"What trade-offs are you making here?\"\n\n### Phase 5: Wrap-up (Minutes 3:00 - 0:00) [3 min]\n- Address any remaining concerns\n- Allow candidate to ask questions\n- Summarize the design\n\n## Bobby's Interview Style\n\n1. **Be conversational and supportive** - Guide the candidate through each phase while being encouraging\n2. **Ask probing questions** - Push candidates to think deeper with genuine curiosity\n3. **Focus on tradeoffs** - There's no single right answer; evaluate reasoning\n4. **Manage time** - Gently transition between phases to cover all areas\n5. **Adapt to responses** - If they struggle, provide hints; if they excel, go deeper\n6. **Stay positive** - Even when challenging decisions, do so constructively\n\n## Phase Transition Signals\n\nWhen you determine it's time to move to the next phase, include this marker in your response:\n[PHASE_TRANSITION: <next_phase_id>]\n\nFor example: [PHASE_TRANSITION: high-level]\n\nOnly use these phase IDs: requirements, high-level, deep-dive, scalability, wrapup\n\n## Important Rules\n\n- DO NOT give away answers - guide them with questions\n- DO NOT be overly positive - be professional and objective\n- DO challenge their decisions - \"Why not use X instead?\"\n- DO acknowledge good points - \"That's a good consideration\"\n- DO redirect if they go off track - \"Let's focus on the core flow first\"\n- DO manage time - transition phases appropriately\n- KEEP responses concise - 2-4 sentences, then ask 1-2 questions\n\n## CRITICAL: Phase Boundary Enforcement\n\n**YOU MUST STRICTLY STAY WITHIN THE CURRENT PHASE. DO NOT JUMP AHEAD OR GO BACKWARDS.**\n\n- **In Requirements Phase (45:00-37:00)**: ONLY ask about scale, functional/non-functional requirements, priorities. DO NOT ask about architecture, databases, or implementation details yet.\n- **In High-Level Design Phase (37:00-25:00)**: ONLY ask about overall architecture, main components, API design, and data flow. DO NOT dive into implementation details, algorithms, or schemas yet.\n- **In Deep Dive Phase (25:00-13:00)**: NOW you can ask about data models, algorithms, protocols, and implementation details. DO NOT ask about scalability or caching yet.\n- **In Scalability Phase (13:00-3:00)**: NOW you can ask about scaling strategies, caching, sharding, load balancing, and performance optimization.\n- **In Wrap-up Phase (3:00-0:00)**: Summarize, answer candidate questions, discuss any remaining concerns.\n\n**If the candidate jumps ahead** (e.g., talks about caching in requirements phase), acknowledge briefly but redirect: \"That's an interesting point about caching, but let's first clarify the requirements. We'll discuss optimization strategies later.\"\n\n**If the candidate is stuck**, provide hints ONLY related to the current phase. Don't hint at future phases.\n\n## Response Format\n\nKeep responses focused. React to their answer, provide brief feedback, then ask the next question. If transitioning phases, acknowledge the transition naturally.`;\n\nexport const getInterviewPrompt = (topic: string, difficulty: string) => {\n  const difficultyGuide = {\n    easy: \"Be more guiding and provide more hints. Focus on basic concepts. Allow more time for explanations.\",\n    medium: \"Balance between guidance and challenge. Standard FAANG interview depth.\",\n    hard: \"Be more challenging. Ask about edge cases, failure modes, and push for optimal solutions. Expect quick, precise answers.\",\n  };\n\n  return `${INTERVIEWER_SYSTEM_PROMPT}\n\n## Current Interview\n\n**Topic:** ${topic}\n**Difficulty:** ${difficulty}\n**Approach:** ${difficultyGuide[difficulty as keyof typeof difficultyGuide]}\n\nBegin the interview now. Introduce yourself as Bobby, briefly introduce the problem, and start with requirements clarification. Ask what clarifying questions they have about the system.`;\n};\n\nexport const PHASE_ANALYSIS_PROMPT = `You are analyzing a specific phase of a system design interview. Evaluate the candidate's performance in this phase only.\n\n## CRITICAL EVALUATION RULES\n\n**IMPORTANT: You MUST evaluate based ONLY on what the USER (candidate) actually said. Do NOT give credit for things the ASSISTANT (interviewer) mentioned.**\n\n1. **If the candidate provided NO responses or only said \"hello\", \"ok\", \"yes\" - score MUST be 1**\n2. **If the candidate gave vague/incomplete answers - score should be 1-2**\n3. **Only give scores of 3-4 if candidate demonstrated clear technical knowledge**\n\n## Phase Being Evaluated: {{PHASE_NAME}}\n\n## Scoring Scale (1-4)\n- 4 (Excellent): Candidate provided detailed, expert-level technical responses\n- 3 (Good): Candidate showed solid understanding through their explanations\n- 2 (Needs Work): Candidate gave partial/vague answers\n- 1 (Poor): Candidate did not provide meaningful technical responses\n\n## Evaluation Criteria for {{PHASE_NAME}}:\n{{PHASE_CRITERIA}}\n\n**If the candidate did not address these criteria in their responses, the score is 1.**\n\n## Response Format\nReturn ONLY valid JSON:\n{\n  \"phase\": \"{{PHASE_ID}}\",\n  \"score\": <1-4>,\n  \"summary\": \"<2-3 sentence evaluation - be honest if candidate didn't respond>\",\n  \"strengths\": [\"<strength 1>\", \"<strength 2>\"],\n  \"improvements\": [\"<area 1>\", \"<area 2>\"],\n  \"keyObservations\": [\"<observation 1>\", \"<observation 2>\"]\n}`;\n\nexport const getPhaseAnalysisPrompt = (phaseId: string, phaseName: string) => {\n  const phaseCriteria: Record<string, string> = {\n    requirements: `\n- Did they ask about scale (users, QPS, data volume)?\n- Did they clarify functional requirements?\n- Did they identify non-functional requirements (latency, availability, consistency)?\n- Did they prioritize features appropriately?\n- Did they ask about constraints and edge cases?`,\n    \"high-level\": `\n- Is the architecture sound and complete?\n- Are components well-defined with clear responsibilities?\n- Is the API design reasonable?\n- Did they choose appropriate databases/storage?\n- Is the data flow logical and efficient?`,\n    \"deep-dive\": `\n- Can they explain component internals?\n- Do they understand data models and schemas?\n- Did they address failure scenarios?\n- Did they consider edge cases?\n- Do they know relevant algorithms/protocols?`,\n    scalability: `\n- Do they understand horizontal vs vertical scaling?\n- Did they propose caching strategies?\n- Did they discuss sharding/partitioning?\n- Did they address load balancing?\n- Can they reason about 10x-100x growth?`,\n    wrapup: `\n- Did they summarize the design clearly?\n- Did they acknowledge trade-offs made?\n- Were they open to feedback?\n- Did they ask thoughtful questions?`,\n  };\n\n  return PHASE_ANALYSIS_PROMPT\n    .replace(/{{PHASE_NAME}}/g, phaseName)\n    .replace(/{{PHASE_ID}}/g, phaseId)\n    .replace(/{{PHASE_CRITERIA}}/g, phaseCriteria[phaseId] || \"\");\n};\n\n/**\n * Get personalized interview prompt with personality adaptation and follow-up context\n */\nexport const getPersonalizedInterviewPrompt = (\n  topic: string,\n  difficulty: string,\n  personality: AdaptedPersonality,\n  followUpContext?: FollowUpContext\n): string => {\n  const difficultyGuide = {\n    easy: \"Be more guiding and provide more hints. Focus on basic concepts. Allow more time for explanations.\",\n    medium: \"Balance between guidance and challenge. Standard FAANG interview depth.\",\n    hard: \"Be more challenging. Ask about edge cases, failure modes, and push for optimal solutions. Expect quick, precise answers.\",\n  };\n\n  // Build the follow-up context section\n  const followUpSection = followUpContext\n    ? generateFollowUpPromptAddition(followUpContext)\n    : \"\";\n\n  return `${INTERVIEWER_SYSTEM_PROMPT}\n\n${personality.promptModifications}\n${followUpSection}\n## Current Interview\n\n**Topic:** ${topic}\n**Difficulty:** ${difficulty}\n**Approach:** ${difficultyGuide[difficulty as keyof typeof difficultyGuide] || difficultyGuide.medium}\n\nBegin the interview now. Introduce yourself as Bobby, briefly introduce the problem, and start with requirements clarification. Ask what clarifying questions they have about the system.`;\n};\n\nexport const SCORING_PROMPT = `You are evaluating a complete 45-minute system design interview. Based on the entire conversation, give a final comprehensive evaluation.\n\n## CRITICAL EVALUATION RULES\n\n**IMPORTANT: You MUST evaluate based ONLY on what the USER (candidate) actually said. Do NOT give credit for things the ASSISTANT (interviewer) mentioned.**\n\n1. **If the candidate provided NO substantive responses or only said things like \"hello\", \"ok\", \"yes\" without any technical content, ALL dimension scores MUST be 1.**\n2. **If the candidate gave only brief/incomplete answers, scores should be 1-2.**\n3. **Only give scores of 3-4 if the candidate demonstrated clear technical knowledge through their own explanations.**\n4. **Count the actual technical responses from the USER. Short acknowledgments don't count as responses.**\n\n## Scoring Scale\n- 4 (Strong Hire): Exceeds expectations, demonstrates expertise with detailed technical explanations\n- 3 (Hire): Meets expectations, solid understanding shown through their responses\n- 2 (Lean No Hire): Below expectations, only partial/vague answers provided\n- 1 (No Hire): Does not meet basic requirements - no meaningful technical responses given\n\n## Dimensions to Evaluate\n\n1. **Requirements Clarification (10%)**\n   - Did the CANDIDATE ask about scale, users, data size?\n   - Did the CANDIDATE clarify functional vs non-functional requirements?\n   - If the candidate didn't ask clarifying questions, score is 1.\n\n2. **High-Level Design (20%)**\n   - Did the CANDIDATE describe an architecture?\n   - Did the CANDIDATE explain components and their responsibilities?\n   - If the candidate didn't propose any design, score is 1.\n\n3. **Detailed Design (15%)**\n   - Did the CANDIDATE explain component internals?\n   - Did the CANDIDATE discuss data models and algorithms?\n   - If the candidate didn't go into any details, score is 1.\n\n4. **Scalability (20%)**\n   - Did the CANDIDATE discuss scaling strategies?\n   - Did the CANDIDATE propose caching, sharding, load balancing?\n   - If the candidate didn't address scalability, score is 1.\n\n5. **Tradeoffs (25%)**\n   - Did the CANDIDATE reason about tradeoffs?\n   - Did the CANDIDATE compare alternatives?\n   - If the candidate didn't discuss any tradeoffs, score is 1.\n\n6. **Communication (10%)**\n   - Did the CANDIDATE communicate clearly?\n   - If the candidate barely spoke, score is 1.\n\n## Response Format\n\nReturn a JSON object with:\n{\n  \"candidateResponseCount\": <number of substantive technical responses from the user>,\n  \"requirementsClarification\": <1-4>,\n  \"highLevelDesign\": <1-4>,\n  \"detailedDesign\": <1-4>,\n  \"scalability\": <1-4>,\n  \"tradeoffs\": <1-4>,\n  \"communication\": <1-4>,\n  \"overallScore\": <weighted average>,\n  \"passStatus\": <true ONLY if average >= 2.5 AND no dimension below 2 AND tradeoffs >= 3 AND candidateResponseCount >= 5>,\n  \"feedback\": {\n    \"requirementsClarification\": \"<specific feedback based on what candidate said>\",\n    \"highLevelDesign\": \"<specific feedback based on what candidate said>\",\n    \"detailedDesign\": \"<specific feedback based on what candidate said>\",\n    \"scalability\": \"<specific feedback based on what candidate said>\",\n    \"tradeoffs\": \"<specific feedback based on what candidate said>\",\n    \"communication\": \"<specific feedback based on what candidate said>\",\n    \"overallFeedback\": \"<2-3 sentence summary - be honest if candidate didn't participate>\",\n    \"strengths\": [\"<strength 1>\", \"<strength 2>\"],\n    \"improvements\": [\"<improvement 1>\", \"<improvement 2>\"]\n  }\n}\n\nReturn ONLY valid JSON.`;\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AACA;;;;;;AAEO,MAAM,mBAAmB;IAC9B;QACE,IAAI;QACJ,MAAM;QACN,UAAU;QACV,WAAW;QACX,SAAS;QACT,aAAa;QACb,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA;QACE,IAAI;QACJ,MAAM;QACN,UAAU;QACV,WAAW;QACX,SAAS;QACT,aAAa;QACb,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA;QACE,IAAI;QACJ,MAAM;QACN,UAAU;QACV,WAAW;QACX,SAAS;QACT,aAAa;QACb,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA;QACE,IAAI;QACJ,MAAM;QACN,UAAU;QACV,WAAW;QACX,SAAS;QACT,aAAa;QACb,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA;QACE,IAAI;QACJ,MAAM;QACN,UAAU;QACV,WAAW;QACX,SAAS;QACT,aAAa;QACb,WAAW;YACT;YACA;YACA;SACD;IACH;CACD;AAIM,MAAM,4BAA4B,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;iKAgGuH,CAAC;AAE3J,MAAM,qBAAqB,CAAC,OAAe;IAChD,MAAM,kBAAkB;QACtB,MAAM;QACN,QAAQ;QACR,MAAM;IACR;IAEA,OAAO,GAAG,0BAA0B;;;;WAI3B,EAAE,MAAM;gBACH,EAAE,WAAW;cACf,EAAE,eAAe,CAAC,WAA2C,CAAC;;yLAE6G,CAAC;AAC1L;AAEO,MAAM,wBAAwB,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CAgCrC,CAAC;AAEK,MAAM,yBAAyB,CAAC,SAAiB;IACtD,MAAM,gBAAwC;QAC5C,cAAc,CAAC;;;;;gDAK6B,CAAC;QAC7C,cAAc,CAAC;;;;;yCAKsB,CAAC;QACtC,aAAa,CAAC;;;;;6CAK2B,CAAC;QAC1C,aAAa,CAAC;;;;;wCAKsB,CAAC;QACrC,QAAQ,CAAC;;;;oCAIuB,CAAC;IACnC;IAEA,OAAO,sBACJ,OAAO,CAAC,mBAAmB,WAC3B,OAAO,CAAC,iBAAiB,SACzB,OAAO,CAAC,uBAAuB,aAAa,CAAC,QAAQ,IAAI;AAC9D;AAKO,MAAM,iCAAiC,CAC5C,OACA,YACA,aACA;IAEA,MAAM,kBAAkB;QACtB,MAAM;QACN,QAAQ;QACR,MAAM;IACR;IAEA,sCAAsC;IACtC,MAAM,kBAAkB,kBACpB,IAAA,6OAA8B,EAAC,mBAC/B;IAEJ,OAAO,GAAG,0BAA0B;;AAEtC,EAAE,YAAY,mBAAmB,CAAC;AAClC,EAAE,gBAAgB;;;WAGP,EAAE,MAAM;gBACH,EAAE,WAAW;cACf,EAAE,eAAe,CAAC,WAA2C,IAAI,gBAAgB,MAAM,CAAC;;yLAEmF,CAAC;AAC1L;AAEO,MAAM,iBAAiB,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;uBA0ER,CAAC"}},
    {"offset": {"line": 3162, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/redis.ts"],"sourcesContent":["/**\n * Redis Client using Upstash\n *\n * Provides a singleton Redis client for caching.\n * Falls back gracefully if Redis is not configured.\n */\n\nimport { Redis } from \"@upstash/redis\";\n\n// Cache key prefixes for organization\nexport const CACHE_KEYS = {\n  USER_ANALYTICS: \"analytics:user:\",\n  LEADERBOARD: \"leaderboard:\",\n  OLLAMA_HEALTH: \"ollama:health\",\n  INTERVIEW_QUESTIONS: \"questions:\",\n} as const;\n\n// TTL values in seconds\nexport const CACHE_TTL = {\n  USER_ANALYTICS: 5 * 60,      // 5 minutes\n  LEADERBOARD: 60,             // 1 minute\n  OLLAMA_HEALTH: 30,           // 30 seconds\n  INTERVIEW_QUESTIONS: 60 * 60, // 1 hour\n} as const;\n\n// Check if Redis is configured\nconst isRedisConfigured = () => {\n  return !!(\n    process.env.UPSTASH_REDIS_REST_URL &&\n    process.env.UPSTASH_REDIS_REST_TOKEN\n  );\n};\n\n// Create Redis client singleton\nlet redisClient: Redis | null = null;\n\nexport function getRedisClient(): Redis | null {\n  if (!isRedisConfigured()) {\n    return null;\n  }\n\n  if (!redisClient) {\n    redisClient = new Redis({\n      url: process.env.UPSTASH_REDIS_REST_URL!,\n      token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n    });\n  }\n\n  return redisClient;\n}\n\n// Helper to check if Redis is available\nexport function isRedisAvailable(): boolean {\n  return isRedisConfigured();\n}\n\nexport { Redis };\n"],"names":[],"mappings":";;;;;;;;;;AAAA;;;;;CAKC,GAED;;AAGO,MAAM,aAAa;IACxB,gBAAgB;IAChB,aAAa;IACb,eAAe;IACf,qBAAqB;AACvB;AAGO,MAAM,YAAY;IACvB,gBAAgB,IAAI;IACpB,aAAa;IACb,eAAe;IACf,qBAAqB,KAAK;AAC5B;AAEA,+BAA+B;AAC/B,MAAM,oBAAoB;IACxB,OAAO,CAAC,CAAC,CACP,QAAQ,GAAG,CAAC,sBAAsB,IAClC,QAAQ,GAAG,CAAC,wBAAwB,AACtC;AACF;AAEA,gCAAgC;AAChC,IAAI,cAA4B;AAEzB,SAAS;IACd,IAAI,CAAC,qBAAqB;QACxB,OAAO;IACT;IAEA,IAAI,CAAC,aAAa;QAChB,cAAc,IAAI,iPAAK,CAAC;YACtB,KAAK,QAAQ,GAAG,CAAC,sBAAsB;YACvC,OAAO,QAAQ,GAAG,CAAC,wBAAwB;QAC7C;IACF;IAEA,OAAO;AACT;AAGO,SAAS;IACd,OAAO;AACT"}},
    {"offset": {"line": 3217, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/lib/rate-limit.ts"],"sourcesContent":["/**\n * Rate Limiting Utility\n *\n * Sliding window rate limiter using Upstash Redis.\n * Falls back to in-memory storage when Redis is not configured.\n */\n\nimport { NextRequest, NextResponse } from \"next/server\";\nimport { getRedisClient, isRedisAvailable } from \"./redis\";\n\ninterface RateLimitConfig {\n  /** Maximum number of requests allowed in the window */\n  limit: number;\n  /** Time window in seconds */\n  window: number;\n}\n\ninterface RateLimitResult {\n  success: boolean;\n  limit: number;\n  remaining: number;\n  reset: number; // Unix timestamp when the window resets\n}\n\n// Predefined rate limit tiers\nexport const RATE_LIMITS = {\n  /** Auth routes (login, register) - strict to prevent brute force */\n  AUTH: { limit: 5, window: 60 },\n  /** Password change - very strict */\n  PASSWORD: { limit: 3, window: 60 },\n  /** Resource creation (interviews, challenges) - moderate */\n  CREATE: { limit: 10, window: 60 },\n  /** Chat/interaction endpoints - higher limit */\n  CHAT: { limit: 30, window: 60 },\n  /** Code submission - moderate */\n  SUBMIT: { limit: 20, window: 60 },\n  /** Read/list endpoints - generous */\n  READ: { limit: 60, window: 60 },\n  /** File upload - strict */\n  UPLOAD: { limit: 5, window: 60 },\n  /** AI/LLM endpoints (expensive) - moderate */\n  AI: { limit: 10, window: 60 },\n} as const;\n\n// In-memory fallback store\nconst memoryStore = new Map<string, { count: number; resetAt: number }>();\n\n// Clean up expired entries periodically\nsetInterval(() => {\n  const now = Date.now();\n  for (const [key, value] of memoryStore) {\n    if (value.resetAt <= now) {\n      memoryStore.delete(key);\n    }\n  }\n}, 60_000);\n\n/**\n * Extract client identifier from request.\n * Uses IP address for unauthenticated requests.\n */\nfunction getClientIp(request: NextRequest): string {\n  const forwarded = request.headers.get(\"x-forwarded-for\");\n  if (forwarded) {\n    return forwarded.split(\",\")[0].trim();\n  }\n  const realIp = request.headers.get(\"x-real-ip\");\n  if (realIp) {\n    return realIp;\n  }\n  return \"127.0.0.1\";\n}\n\n/**\n * Check rate limit using Redis sliding window.\n */\nasync function checkRedisRateLimit(\n  key: string,\n  config: RateLimitConfig\n): Promise<RateLimitResult> {\n  const redis = getRedisClient();\n  if (!redis) {\n    return { success: true, limit: config.limit, remaining: config.limit, reset: 0 };\n  }\n\n  const now = Date.now();\n  const windowMs = config.window * 1000;\n  const windowStart = now - windowMs;\n\n  // Use a sorted set for sliding window\n  const redisKey = `ratelimit:${key}`;\n\n  // Pipeline: remove old entries, add current, count, set expiry\n  const pipeline = redis.pipeline();\n  pipeline.zremrangebyscore(redisKey, 0, windowStart);\n  pipeline.zadd(redisKey, { score: now, member: `${now}:${Math.random()}` });\n  pipeline.zcard(redisKey);\n  pipeline.expire(redisKey, config.window);\n\n  const results = await pipeline.exec();\n  const count = (results[2] as number) || 0;\n\n  const reset = Math.ceil((now + windowMs) / 1000);\n  const remaining = Math.max(0, config.limit - count);\n  const success = count <= config.limit;\n\n  return { success, limit: config.limit, remaining, reset };\n}\n\n/**\n * Check rate limit using in-memory store (fallback).\n */\nfunction checkMemoryRateLimit(\n  key: string,\n  config: RateLimitConfig\n): RateLimitResult {\n  const now = Date.now();\n  const windowMs = config.window * 1000;\n  const storeKey = `ratelimit:${key}`;\n\n  const entry = memoryStore.get(storeKey);\n\n  if (!entry || entry.resetAt <= now) {\n    // New window\n    memoryStore.set(storeKey, { count: 1, resetAt: now + windowMs });\n    return {\n      success: true,\n      limit: config.limit,\n      remaining: config.limit - 1,\n      reset: Math.ceil((now + windowMs) / 1000),\n    };\n  }\n\n  entry.count++;\n  const remaining = Math.max(0, config.limit - entry.count);\n  const success = entry.count <= config.limit;\n\n  return {\n    success,\n    limit: config.limit,\n    remaining,\n    reset: Math.ceil(entry.resetAt / 1000),\n  };\n}\n\n/**\n * Core rate limit check. Returns result indicating if request is allowed.\n */\nexport async function checkRateLimit(\n  identifier: string,\n  config: RateLimitConfig\n): Promise<RateLimitResult> {\n  if (isRedisAvailable()) {\n    try {\n      return await checkRedisRateLimit(identifier, config);\n    } catch (error) {\n      console.error(\"Redis rate limit error, falling back to memory:\", error);\n      return checkMemoryRateLimit(identifier, config);\n    }\n  }\n  return checkMemoryRateLimit(identifier, config);\n}\n\n/**\n * Apply rate limiting to an API route handler.\n * Returns a 429 response if the limit is exceeded, or null if allowed.\n *\n * @param request - The incoming request\n * @param config - Rate limit configuration\n * @param keyPrefix - Prefix for the rate limit key (e.g., \"api:register\")\n * @param userId - Optional user ID for user-based limiting (falls back to IP)\n */\nexport async function rateLimit(\n  request: NextRequest,\n  config: RateLimitConfig,\n  keyPrefix: string,\n  userId?: string\n): Promise<NextResponse | null> {\n  const identifier = userId || getClientIp(request);\n  const key = `${keyPrefix}:${identifier}`;\n\n  const result = await checkRateLimit(key, config);\n\n  if (!result.success) {\n    const retryAfter = Math.max(1, result.reset - Math.ceil(Date.now() / 1000));\n    return NextResponse.json(\n      {\n        error: \"Too many requests\",\n        message: \"Rate limit exceeded. Please try again later.\",\n        retryAfter,\n      },\n      {\n        status: 429,\n        headers: {\n          \"Retry-After\": String(retryAfter),\n          \"X-RateLimit-Limit\": String(result.limit),\n          \"X-RateLimit-Remaining\": \"0\",\n          \"X-RateLimit-Reset\": String(result.reset),\n        },\n      }\n    );\n  }\n\n  return null;\n}\n\n/**\n * Add rate limit headers to a successful response.\n */\nexport function addRateLimitHeaders(\n  response: NextResponse,\n  result: RateLimitResult\n): NextResponse {\n  response.headers.set(\"X-RateLimit-Limit\", String(result.limit));\n  response.headers.set(\"X-RateLimit-Remaining\", String(result.remaining));\n  response.headers.set(\"X-RateLimit-Reset\", String(result.reset));\n  return response;\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA;;;;;CAKC,GAED;AACA;;;AAiBO,MAAM,cAAc;IACzB,kEAAkE,GAClE,MAAM;QAAE,OAAO;QAAG,QAAQ;IAAG;IAC7B,kCAAkC,GAClC,UAAU;QAAE,OAAO;QAAG,QAAQ;IAAG;IACjC,0DAA0D,GAC1D,QAAQ;QAAE,OAAO;QAAI,QAAQ;IAAG;IAChC,8CAA8C,GAC9C,MAAM;QAAE,OAAO;QAAI,QAAQ;IAAG;IAC9B,+BAA+B,GAC/B,QAAQ;QAAE,OAAO;QAAI,QAAQ;IAAG;IAChC,mCAAmC,GACnC,MAAM;QAAE,OAAO;QAAI,QAAQ;IAAG;IAC9B,yBAAyB,GACzB,QAAQ;QAAE,OAAO;QAAG,QAAQ;IAAG;IAC/B,4CAA4C,GAC5C,IAAI;QAAE,OAAO;QAAI,QAAQ;IAAG;AAC9B;AAEA,2BAA2B;AAC3B,MAAM,cAAc,IAAI;AAExB,wCAAwC;AACxC,YAAY;IACV,MAAM,MAAM,KAAK,GAAG;IACpB,KAAK,MAAM,CAAC,KAAK,MAAM,IAAI,YAAa;QACtC,IAAI,MAAM,OAAO,IAAI,KAAK;YACxB,YAAY,MAAM,CAAC;QACrB;IACF;AACF,GAAG;AAEH;;;CAGC,GACD,SAAS,YAAY,OAAoB;IACvC,MAAM,YAAY,QAAQ,OAAO,CAAC,GAAG,CAAC;IACtC,IAAI,WAAW;QACb,OAAO,UAAU,KAAK,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI;IACrC;IACA,MAAM,SAAS,QAAQ,OAAO,CAAC,GAAG,CAAC;IACnC,IAAI,QAAQ;QACV,OAAO;IACT;IACA,OAAO;AACT;AAEA;;CAEC,GACD,eAAe,oBACb,GAAW,EACX,MAAuB;IAEvB,MAAM,QAAQ,IAAA,gOAAc;IAC5B,IAAI,CAAC,OAAO;QACV,OAAO;YAAE,SAAS;YAAM,OAAO,OAAO,KAAK;YAAE,WAAW,OAAO,KAAK;YAAE,OAAO;QAAE;IACjF;IAEA,MAAM,MAAM,KAAK,GAAG;IACpB,MAAM,WAAW,OAAO,MAAM,GAAG;IACjC,MAAM,cAAc,MAAM;IAE1B,sCAAsC;IACtC,MAAM,WAAW,CAAC,UAAU,EAAE,KAAK;IAEnC,+DAA+D;IAC/D,MAAM,WAAW,MAAM,QAAQ;IAC/B,SAAS,gBAAgB,CAAC,UAAU,GAAG;IACvC,SAAS,IAAI,CAAC,UAAU;QAAE,OAAO;QAAK,QAAQ,GAAG,IAAI,CAAC,EAAE,KAAK,MAAM,IAAI;IAAC;IACxE,SAAS,KAAK,CAAC;IACf,SAAS,MAAM,CAAC,UAAU,OAAO,MAAM;IAEvC,MAAM,UAAU,MAAM,SAAS,IAAI;IACnC,MAAM,QAAQ,AAAC,OAAO,CAAC,EAAE,IAAe;IAExC,MAAM,QAAQ,KAAK,IAAI,CAAC,CAAC,MAAM,QAAQ,IAAI;IAC3C,MAAM,YAAY,KAAK,GAAG,CAAC,GAAG,OAAO,KAAK,GAAG;IAC7C,MAAM,UAAU,SAAS,OAAO,KAAK;IAErC,OAAO;QAAE;QAAS,OAAO,OAAO,KAAK;QAAE;QAAW;IAAM;AAC1D;AAEA;;CAEC,GACD,SAAS,qBACP,GAAW,EACX,MAAuB;IAEvB,MAAM,MAAM,KAAK,GAAG;IACpB,MAAM,WAAW,OAAO,MAAM,GAAG;IACjC,MAAM,WAAW,CAAC,UAAU,EAAE,KAAK;IAEnC,MAAM,QAAQ,YAAY,GAAG,CAAC;IAE9B,IAAI,CAAC,SAAS,MAAM,OAAO,IAAI,KAAK;QAClC,aAAa;QACb,YAAY,GAAG,CAAC,UAAU;YAAE,OAAO;YAAG,SAAS,MAAM;QAAS;QAC9D,OAAO;YACL,SAAS;YACT,OAAO,OAAO,KAAK;YACnB,WAAW,OAAO,KAAK,GAAG;YAC1B,OAAO,KAAK,IAAI,CAAC,CAAC,MAAM,QAAQ,IAAI;QACtC;IACF;IAEA,MAAM,KAAK;IACX,MAAM,YAAY,KAAK,GAAG,CAAC,GAAG,OAAO,KAAK,GAAG,MAAM,KAAK;IACxD,MAAM,UAAU,MAAM,KAAK,IAAI,OAAO,KAAK;IAE3C,OAAO;QACL;QACA,OAAO,OAAO,KAAK;QACnB;QACA,OAAO,KAAK,IAAI,CAAC,MAAM,OAAO,GAAG;IACnC;AACF;AAKO,eAAe,eACpB,UAAkB,EAClB,MAAuB;IAEvB,IAAI,IAAA,kOAAgB,KAAI;QACtB,IAAI;YACF,OAAO,MAAM,oBAAoB,YAAY;QAC/C,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,mDAAmD;YACjE,OAAO,qBAAqB,YAAY;QAC1C;IACF;IACA,OAAO,qBAAqB,YAAY;AAC1C;AAWO,eAAe,UACpB,OAAoB,EACpB,MAAuB,EACvB,SAAiB,EACjB,MAAe;IAEf,MAAM,aAAa,UAAU,YAAY;IACzC,MAAM,MAAM,GAAG,UAAU,CAAC,EAAE,YAAY;IAExC,MAAM,SAAS,MAAM,eAAe,KAAK;IAEzC,IAAI,CAAC,OAAO,OAAO,EAAE;QACnB,MAAM,aAAa,KAAK,GAAG,CAAC,GAAG,OAAO,KAAK,GAAG,KAAK,IAAI,CAAC,KAAK,GAAG,KAAK;QACrE,OAAO,yNAAY,CAAC,IAAI,CACtB;YACE,OAAO;YACP,SAAS;YACT;QACF,GACA;YACE,QAAQ;YACR,SAAS;gBACP,eAAe,OAAO;gBACtB,qBAAqB,OAAO,OAAO,KAAK;gBACxC,yBAAyB;gBACzB,qBAAqB,OAAO,OAAO,KAAK;YAC1C;QACF;IAEJ;IAEA,OAAO;AACT;AAKO,SAAS,oBACd,QAAsB,EACtB,MAAuB;IAEvB,SAAS,OAAO,CAAC,GAAG,CAAC,qBAAqB,OAAO,OAAO,KAAK;IAC7D,SAAS,OAAO,CAAC,GAAG,CAAC,yBAAyB,OAAO,OAAO,SAAS;IACrE,SAAS,OAAO,CAAC,GAAG,CAAC,qBAAqB,OAAO,OAAO,KAAK;IAC7D,OAAO;AACT"}},
    {"offset": {"line": 3408, "column": 0}, "map": {"version":3,"sources":["file:///Users/sairammaruri/Documents/Git/system-design/system-design-simulator/src/app/api/interview/personalized/route.ts"],"sourcesContent":["/**\n * Personalized Interview API\n *\n * Creates an interview with smart difficulty adjustment and personalized questions\n * based on user profile, skills, experience, and resume analysis.\n *\n * POST /api/interview/personalized\n * Body: { topic: string, selectedDifficulty: \"easy\" | \"medium\" | \"hard\" }\n */\n\nimport { NextRequest, NextResponse } from \"next/server\";\nimport { auth } from \"@/lib/auth\";\nimport { prisma } from \"@/lib/prisma\";\nimport { complete } from \"@/lib/llm\";\nimport {\n  calculateSmartDifficulty,\n  DifficultyLevel,\n  UserProfileData,\n  ResumeData,\n} from \"@/lib/difficulty-calculator\";\nimport {\n  generatePersonalizedQuestions,\n  buildPersonalizedInterviewerPrompt,\n} from \"@/lib/question-generator\";\nimport { buildFollowUpContext } from \"@/lib/followup-engine\";\nimport {\n  calculatePersonalityAdaptation,\n  getPerformanceCategory,\n} from \"@/lib/ai-personality-adapter\";\nimport { calculatePerformanceMetrics } from \"@/lib/analytics/performance-analytics\";\nimport { getPersonalizedInterviewPrompt } from \"@/lib/prompts/interviewer\";\nimport { rateLimit, RATE_LIMITS } from \"@/lib/rate-limit\";\n\nexport async function POST(request: NextRequest) {\n  try {\n    // 1. Authenticate user\n    const session = await auth();\n    if (!session?.user?.id) {\n      return NextResponse.json({ error: \"Unauthorized\" }, { status: 401 });\n    }\n\n    const rateLimitResponse = await rateLimit(request, RATE_LIMITS.CREATE, \"api:interview:personalized\", session.user.id);\n    if (rateLimitResponse) return rateLimitResponse;\n\n    // 2. Parse request body\n    const body = await request.json();\n    const { topic, selectedDifficulty = \"medium\" } = body;\n\n    if (!topic) {\n      return NextResponse.json(\n        { error: \"Topic is required\" },\n        { status: 400 }\n      );\n    }\n\n    // Validate difficulty\n    const validDifficulties: DifficultyLevel[] = [\"easy\", \"medium\", \"hard\"];\n    if (!validDifficulties.includes(selectedDifficulty)) {\n      return NextResponse.json(\n        { error: \"Invalid difficulty level\" },\n        { status: 400 }\n      );\n    }\n    // 3. Verify user exists (prevents Foreign Key errors if DB was reset)\n    const userExists = await prisma.user.findUnique({\n      where: { id: session.user.id },\n    });\n\n    if (!userExists) {\n      return NextResponse.json(\n        { error: \"User record not found. Please sign out and sign in again.\" },\n        { status: 401 }\n      );\n    }\n    // 3. Fetch user profile\n    const profile = await prisma.profile.findUnique({\n      where: { userId: session.user.id },\n    });\n\n    // 4. Fetch latest resume\n    const latestResume = await prisma.resume.findFirst({\n      where: { userId: session.user.id },\n      orderBy: { uploadedAt: \"desc\" },\n    });\n\n    // 5. Parse profile data\n    const profileData: UserProfileData | null = profile\n      ? {\n        yearsExperience: profile.yearsExperience,\n        skills: profile.skills ? JSON.parse(profile.skills) : [],\n        targetCompanies: profile.targetCompanies\n          ? JSON.parse(profile.targetCompanies)\n          : [],\n        targetRole: profile.targetRole,\n        bio: profile.bio,\n      }\n      : null;\n\n    // 6. Parse resume data\n    const resumeData: ResumeData | null = latestResume\n      ? {\n        content: latestResume.content,\n        atsScore: latestResume.atsScore,\n        keywords: latestResume.keywords\n          ? JSON.parse(latestResume.keywords)\n          : [],\n        predictedRoles: latestResume.predictedRoles\n          ? JSON.parse(latestResume.predictedRoles)\n          : [],\n        analysis: latestResume.analysis\n          ? JSON.parse(latestResume.analysis)\n          : null,\n      }\n      : null;\n\n    // 7. Calculate smart difficulty\n    const difficultyResult = calculateSmartDifficulty({\n      selectedDifficulty,\n      profile: profileData,\n      resume: resumeData,\n      topic,\n    });\n\n    const actualDifficulty = difficultyResult.calculatedDifficulty;\n\n    // 8. Fetch performance analytics and build follow-up context\n    const [performanceMetrics, followUpContext, personalityAdaptation] = await Promise.all([\n      calculatePerformanceMetrics(session.user.id, 15),\n      buildFollowUpContext(session.user.id, topic),\n      calculatePersonalityAdaptation(session.user.id),\n    ]);\n\n    // 9. Generate personalized questions with follow-up context\n    const questionSet = await generatePersonalizedQuestions({\n      topic,\n      difficulty: actualDifficulty,\n      profile: profileData,\n      resume: resumeData,\n      followUpContext,\n      userId: session.user.id,\n    });\n\n    // 10. Build personalized interviewer prompt with personality adaptation\n    const interviewerPrompt = getPersonalizedInterviewPrompt(\n      topic,\n      actualDifficulty,\n      personalityAdaptation,\n      followUpContext\n    );\n\n    // 11. Create interview record\n    const interview = await prisma.interview.create({\n      data: {\n        userId: session.user.id,\n        topic,\n        difficulty: actualDifficulty,\n        status: \"in_progress\",\n        startedAt: new Date(),\n      },\n    });\n\n    // 12. Generate initial interviewer message using LLM (Ollama with OpenAI fallback)\n    const initialCompletion = await complete({\n      messages: [\n        { role: \"system\", content: interviewerPrompt },\n        { role: \"user\", content: \"Start the interview.\" },\n      ],\n      maxTokens: 500,\n      temperature: 0.7,\n    });\n\n    const initialMessage =\n      initialCompletion.content ||\n      `Hey there! I'm Bobby, and I'll be your interviewer today. I'm excited to discuss ${topic} with you! Let's start with some clarifying questions - what's the first thing you'd want to understand about the requirements?`;\n\n    // 13. Save messages to database\n    await prisma.message.createMany({\n      data: [\n        {\n          interviewId: interview.id,\n          role: \"system\",\n          content: interviewerPrompt,\n        },\n        {\n          interviewId: interview.id,\n          role: \"assistant\",\n          content: initialMessage,\n        },\n      ],\n    });\n\n    // 14. Return response with full personalization info\n    return NextResponse.json({\n      success: true,\n      interview: {\n        id: interview.id,\n        topic: interview.topic,\n        difficulty: interview.difficulty,\n        status: interview.status,\n        startedAt: interview.startedAt,\n      },\n      initialMessage,\n      personalization: {\n        selectedDifficulty,\n        calculatedDifficulty: actualDifficulty,\n        wasAdjusted: selectedDifficulty !== actualDifficulty,\n        adjustmentReason: difficultyResult.adjustmentReason,\n        confidenceScore: difficultyResult.confidenceScore,\n        breakdown: difficultyResult.breakdown,\n        recommendations: difficultyResult.recommendations,\n        // New personalization fields\n        performanceCategory: personalityAdaptation.performanceCategory,\n        personalityAdaptation: personalityAdaptation.reasonForAdaptation,\n        weakAreasTargeted: followUpContext.weakPoints.map((wp) => wp.dimension),\n        hasHistory: followUpContext.hasHistory,\n        totalPastInterviews: followUpContext.totalPastInterviews,\n        adaptedDifficulty: followUpContext.adaptedDifficulty,\n      },\n      questionSet: {\n        focusAreas: questionSet.focusAreas,\n        personalizationNotes: questionSet.personalizationNotes,\n        questionCount: questionSet.questions.length,\n      },\n      performanceContext: {\n        avgScore: performanceMetrics.overall.avgScore,\n        passRate: performanceMetrics.overall.passRate,\n        scoreTrend: performanceMetrics.overall.scoreTrend,\n        completedInterviews: performanceMetrics.overall.completedInterviews,\n      },\n    });\n  } catch (error) {\n    console.error(\"Personalized interview creation error:\", error);\n    return NextResponse.json(\n      { error: \"Failed to create personalized interview\" },\n      { status: 500 }\n    );\n  }\n}\n\n/**\n * GET endpoint to preview difficulty calculation without creating interview\n */\nexport async function GET(request: NextRequest) {\n  try {\n    const session = await auth();\n    if (!session?.user?.id) {\n      return NextResponse.json({ error: \"Unauthorized\" }, { status: 401 });\n    }\n\n    const rateLimitResponse = await rateLimit(request, RATE_LIMITS.READ, \"api:interview:personalized:preview\", session.user.id);\n    if (rateLimitResponse) return rateLimitResponse;\n\n    const { searchParams } = new URL(request.url);\n    const selectedDifficulty =\n      (searchParams.get(\"difficulty\") as DifficultyLevel) || \"medium\";\n    const topic = searchParams.get(\"topic\") || \"System Design\";\n\n    // Fetch user data\n    const profile = await prisma.profile.findUnique({\n      where: { userId: session.user.id },\n    });\n\n    const latestResume = await prisma.resume.findFirst({\n      where: { userId: session.user.id },\n      orderBy: { uploadedAt: \"desc\" },\n    });\n\n    // Parse data\n    const profileData: UserProfileData | null = profile\n      ? {\n        yearsExperience: profile.yearsExperience,\n        skills: profile.skills ? JSON.parse(profile.skills) : [],\n        targetCompanies: profile.targetCompanies\n          ? JSON.parse(profile.targetCompanies)\n          : [],\n        targetRole: profile.targetRole,\n        bio: profile.bio,\n      }\n      : null;\n\n    const resumeData: ResumeData | null = latestResume\n      ? {\n        content: latestResume.content,\n        atsScore: latestResume.atsScore,\n        keywords: latestResume.keywords\n          ? JSON.parse(latestResume.keywords)\n          : [],\n        predictedRoles: latestResume.predictedRoles\n          ? JSON.parse(latestResume.predictedRoles)\n          : [],\n        analysis: latestResume.analysis\n          ? JSON.parse(latestResume.analysis)\n          : null,\n      }\n      : null;\n\n    // Calculate difficulty\n    const difficultyResult = calculateSmartDifficulty({\n      selectedDifficulty,\n      profile: profileData,\n      resume: resumeData,\n      topic,\n    });\n\n    return NextResponse.json({\n      selectedDifficulty,\n      calculatedDifficulty: difficultyResult.calculatedDifficulty,\n      wasAdjusted: selectedDifficulty !== difficultyResult.calculatedDifficulty,\n      adjustmentReason: difficultyResult.adjustmentReason,\n      confidenceScore: difficultyResult.confidenceScore,\n      breakdown: difficultyResult.breakdown,\n      recommendations: difficultyResult.recommendations,\n      profileSummary: {\n        hasProfile: !!profile,\n        hasResume: !!latestResume,\n        yearsExperience: profileData?.yearsExperience || null,\n        skillsCount: profileData?.skills?.length || 0,\n        targetCompaniesCount: profileData?.targetCompanies?.length || 0,\n      },\n    });\n  } catch (error) {\n    console.error(\"Difficulty preview error:\", error);\n    return NextResponse.json(\n      { error: \"Failed to calculate difficulty\" },\n      { status: 500 }\n    );\n  }\n}\n"],"names":[],"mappings":";;;;;;AAAA;;;;;;;;CAQC,GAED;AACA;AACA;AACA;AAAA;AACA;AAMA;AAIA;AACA;AAIA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;AAEO,eAAe,KAAK,OAAoB;IAC7C,IAAI;QACF,uBAAuB;QACvB,MAAM,UAAU,MAAM,IAAA,qMAAI;QAC1B,IAAI,CAAC,SAAS,MAAM,IAAI;YACtB,OAAO,yNAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAe,GAAG;gBAAE,QAAQ;YAAI;QACpE;QAEA,MAAM,oBAAoB,MAAM,IAAA,mNAAS,EAAC,SAAS,qNAAW,CAAC,MAAM,EAAE,8BAA8B,QAAQ,IAAI,CAAC,EAAE;QACpH,IAAI,mBAAmB,OAAO;QAE9B,wBAAwB;QACxB,MAAM,OAAO,MAAM,QAAQ,IAAI;QAC/B,MAAM,EAAE,KAAK,EAAE,qBAAqB,QAAQ,EAAE,GAAG;QAEjD,IAAI,CAAC,OAAO;YACV,OAAO,yNAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAoB,GAC7B;gBAAE,QAAQ;YAAI;QAElB;QAEA,sBAAsB;QACtB,MAAM,oBAAuC;YAAC;YAAQ;YAAU;SAAO;QACvE,IAAI,CAAC,kBAAkB,QAAQ,CAAC,qBAAqB;YACnD,OAAO,yNAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAA2B,GACpC;gBAAE,QAAQ;YAAI;QAElB;QACA,sEAAsE;QACtE,MAAM,aAAa,MAAM,yMAAM,CAAC,IAAI,CAAC,UAAU,CAAC;YAC9C,OAAO;gBAAE,IAAI,QAAQ,IAAI,CAAC,EAAE;YAAC;QAC/B;QAEA,IAAI,CAAC,YAAY;YACf,OAAO,yNAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAA4D,GACrE;gBAAE,QAAQ;YAAI;QAElB;QACA,wBAAwB;QACxB,MAAM,UAAU,MAAM,yMAAM,CAAC,OAAO,CAAC,UAAU,CAAC;YAC9C,OAAO;gBAAE,QAAQ,QAAQ,IAAI,CAAC,EAAE;YAAC;QACnC;QAEA,yBAAyB;QACzB,MAAM,eAAe,MAAM,yMAAM,CAAC,MAAM,CAAC,SAAS,CAAC;YACjD,OAAO;gBAAE,QAAQ,QAAQ,IAAI,CAAC,EAAE;YAAC;YACjC,SAAS;gBAAE,YAAY;YAAO;QAChC;QAEA,wBAAwB;QACxB,MAAM,cAAsC,UACxC;YACA,iBAAiB,QAAQ,eAAe;YACxC,QAAQ,QAAQ,MAAM,GAAG,KAAK,KAAK,CAAC,QAAQ,MAAM,IAAI,EAAE;YACxD,iBAAiB,QAAQ,eAAe,GACpC,KAAK,KAAK,CAAC,QAAQ,eAAe,IAClC,EAAE;YACN,YAAY,QAAQ,UAAU;YAC9B,KAAK,QAAQ,GAAG;QAClB,IACE;QAEJ,uBAAuB;QACvB,MAAM,aAAgC,eAClC;YACA,SAAS,aAAa,OAAO;YAC7B,UAAU,aAAa,QAAQ;YAC/B,UAAU,aAAa,QAAQ,GAC3B,KAAK,KAAK,CAAC,aAAa,QAAQ,IAChC,EAAE;YACN,gBAAgB,aAAa,cAAc,GACvC,KAAK,KAAK,CAAC,aAAa,cAAc,IACtC,EAAE;YACN,UAAU,aAAa,QAAQ,GAC3B,KAAK,KAAK,CAAC,aAAa,QAAQ,IAChC;QACN,IACE;QAEJ,gCAAgC;QAChC,MAAM,mBAAmB,IAAA,6OAAwB,EAAC;YAChD;YACA,SAAS;YACT,QAAQ;YACR;QACF;QAEA,MAAM,mBAAmB,iBAAiB,oBAAoB;QAE9D,6DAA6D;QAC7D,MAAM,CAAC,oBAAoB,iBAAiB,sBAAsB,GAAG,MAAM,QAAQ,GAAG,CAAC;YACrF,IAAA,6PAA2B,EAAC,QAAQ,IAAI,CAAC,EAAE,EAAE;YAC7C,IAAA,mOAAoB,EAAC,QAAQ,IAAI,CAAC,EAAE,EAAE;YACtC,IAAA,uQAA8B,EAAC,QAAQ,IAAI,CAAC,EAAE;SAC/C;QAED,4DAA4D;QAC5D,MAAM,cAAc,MAAM,IAAA,+OAA6B,EAAC;YACtD;YACA,YAAY;YACZ,SAAS;YACT,QAAQ;YACR;YACA,QAAQ,QAAQ,IAAI,CAAC,EAAE;QACzB;QAEA,wEAAwE;QACxE,MAAM,oBAAoB,IAAA,iPAA8B,EACtD,OACA,kBACA,uBACA;QAGF,8BAA8B;QAC9B,MAAM,YAAY,MAAM,yMAAM,CAAC,SAAS,CAAC,MAAM,CAAC;YAC9C,MAAM;gBACJ,QAAQ,QAAQ,IAAI,CAAC,EAAE;gBACvB;gBACA,YAAY;gBACZ,QAAQ;gBACR,WAAW,IAAI;YACjB;QACF;QAEA,mFAAmF;QACnF,MAAM,oBAAoB,MAAM,IAAA,kNAAQ,EAAC;YACvC,UAAU;gBACR;oBAAE,MAAM;oBAAU,SAAS;gBAAkB;gBAC7C;oBAAE,MAAM;oBAAQ,SAAS;gBAAuB;aACjD;YACD,WAAW;YACX,aAAa;QACf;QAEA,MAAM,iBACJ,kBAAkB,OAAO,IACzB,CAAC,iFAAiF,EAAE,MAAM,+HAA+H,CAAC;QAE5N,gCAAgC;QAChC,MAAM,yMAAM,CAAC,OAAO,CAAC,UAAU,CAAC;YAC9B,MAAM;gBACJ;oBACE,aAAa,UAAU,EAAE;oBACzB,MAAM;oBACN,SAAS;gBACX;gBACA;oBACE,aAAa,UAAU,EAAE;oBACzB,MAAM;oBACN,SAAS;gBACX;aACD;QACH;QAEA,qDAAqD;QACrD,OAAO,yNAAY,CAAC,IAAI,CAAC;YACvB,SAAS;YACT,WAAW;gBACT,IAAI,UAAU,EAAE;gBAChB,OAAO,UAAU,KAAK;gBACtB,YAAY,UAAU,UAAU;gBAChC,QAAQ,UAAU,MAAM;gBACxB,WAAW,UAAU,SAAS;YAChC;YACA;YACA,iBAAiB;gBACf;gBACA,sBAAsB;gBACtB,aAAa,uBAAuB;gBACpC,kBAAkB,iBAAiB,gBAAgB;gBACnD,iBAAiB,iBAAiB,eAAe;gBACjD,WAAW,iBAAiB,SAAS;gBACrC,iBAAiB,iBAAiB,eAAe;gBACjD,6BAA6B;gBAC7B,qBAAqB,sBAAsB,mBAAmB;gBAC9D,uBAAuB,sBAAsB,mBAAmB;gBAChE,mBAAmB,gBAAgB,UAAU,CAAC,GAAG,CAAC,CAAC,KAAO,GAAG,SAAS;gBACtE,YAAY,gBAAgB,UAAU;gBACtC,qBAAqB,gBAAgB,mBAAmB;gBACxD,mBAAmB,gBAAgB,iBAAiB;YACtD;YACA,aAAa;gBACX,YAAY,YAAY,UAAU;gBAClC,sBAAsB,YAAY,oBAAoB;gBACtD,eAAe,YAAY,SAAS,CAAC,MAAM;YAC7C;YACA,oBAAoB;gBAClB,UAAU,mBAAmB,OAAO,CAAC,QAAQ;gBAC7C,UAAU,mBAAmB,OAAO,CAAC,QAAQ;gBAC7C,YAAY,mBAAmB,OAAO,CAAC,UAAU;gBACjD,qBAAqB,mBAAmB,OAAO,CAAC,mBAAmB;YACrE;QACF;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,0CAA0C;QACxD,OAAO,yNAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAA0C,GACnD;YAAE,QAAQ;QAAI;IAElB;AACF;AAKO,eAAe,IAAI,OAAoB;IAC5C,IAAI;QACF,MAAM,UAAU,MAAM,IAAA,qMAAI;QAC1B,IAAI,CAAC,SAAS,MAAM,IAAI;YACtB,OAAO,yNAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAe,GAAG;gBAAE,QAAQ;YAAI;QACpE;QAEA,MAAM,oBAAoB,MAAM,IAAA,mNAAS,EAAC,SAAS,qNAAW,CAAC,IAAI,EAAE,sCAAsC,QAAQ,IAAI,CAAC,EAAE;QAC1H,IAAI,mBAAmB,OAAO;QAE9B,MAAM,EAAE,YAAY,EAAE,GAAG,IAAI,IAAI,QAAQ,GAAG;QAC5C,MAAM,qBACJ,AAAC,aAAa,GAAG,CAAC,iBAAqC;QACzD,MAAM,QAAQ,aAAa,GAAG,CAAC,YAAY;QAE3C,kBAAkB;QAClB,MAAM,UAAU,MAAM,yMAAM,CAAC,OAAO,CAAC,UAAU,CAAC;YAC9C,OAAO;gBAAE,QAAQ,QAAQ,IAAI,CAAC,EAAE;YAAC;QACnC;QAEA,MAAM,eAAe,MAAM,yMAAM,CAAC,MAAM,CAAC,SAAS,CAAC;YACjD,OAAO;gBAAE,QAAQ,QAAQ,IAAI,CAAC,EAAE;YAAC;YACjC,SAAS;gBAAE,YAAY;YAAO;QAChC;QAEA,aAAa;QACb,MAAM,cAAsC,UACxC;YACA,iBAAiB,QAAQ,eAAe;YACxC,QAAQ,QAAQ,MAAM,GAAG,KAAK,KAAK,CAAC,QAAQ,MAAM,IAAI,EAAE;YACxD,iBAAiB,QAAQ,eAAe,GACpC,KAAK,KAAK,CAAC,QAAQ,eAAe,IAClC,EAAE;YACN,YAAY,QAAQ,UAAU;YAC9B,KAAK,QAAQ,GAAG;QAClB,IACE;QAEJ,MAAM,aAAgC,eAClC;YACA,SAAS,aAAa,OAAO;YAC7B,UAAU,aAAa,QAAQ;YAC/B,UAAU,aAAa,QAAQ,GAC3B,KAAK,KAAK,CAAC,aAAa,QAAQ,IAChC,EAAE;YACN,gBAAgB,aAAa,cAAc,GACvC,KAAK,KAAK,CAAC,aAAa,cAAc,IACtC,EAAE;YACN,UAAU,aAAa,QAAQ,GAC3B,KAAK,KAAK,CAAC,aAAa,QAAQ,IAChC;QACN,IACE;QAEJ,uBAAuB;QACvB,MAAM,mBAAmB,IAAA,6OAAwB,EAAC;YAChD;YACA,SAAS;YACT,QAAQ;YACR;QACF;QAEA,OAAO,yNAAY,CAAC,IAAI,CAAC;YACvB;YACA,sBAAsB,iBAAiB,oBAAoB;YAC3D,aAAa,uBAAuB,iBAAiB,oBAAoB;YACzE,kBAAkB,iBAAiB,gBAAgB;YACnD,iBAAiB,iBAAiB,eAAe;YACjD,WAAW,iBAAiB,SAAS;YACrC,iBAAiB,iBAAiB,eAAe;YACjD,gBAAgB;gBACd,YAAY,CAAC,CAAC;gBACd,WAAW,CAAC,CAAC;gBACb,iBAAiB,aAAa,mBAAmB;gBACjD,aAAa,aAAa,QAAQ,UAAU;gBAC5C,sBAAsB,aAAa,iBAAiB,UAAU;YAChE;QACF;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,6BAA6B;QAC3C,OAAO,yNAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAAiC,GAC1C;YAAE,QAAQ;QAAI;IAElB;AACF"}}]
}